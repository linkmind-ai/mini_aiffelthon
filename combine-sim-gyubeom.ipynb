{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11259510,"sourceType":"datasetVersion","datasetId":7037079}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain-experimental","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:06:13.576185Z","iopub.execute_input":"2025-04-04T06:06:13.576385Z","iopub.status.idle":"2025-04-04T06:06:24.268445Z","shell.execute_reply.started":"2025-04-04T06:06:13.576363Z","shell.execute_reply":"2025-04-04T06:06:24.267635Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-experimental\n  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\nCollecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental)\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-core<0.4.0,>=0.3.28 (from langchain-experimental)\n  Downloading langchain_core-0.3.50-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.21 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.26.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.12.2)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.0a2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain<1.0.0,>=0.3.21->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.29.0)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.2.2)\nDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.50-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.4/423.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.22-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, httpx-sse, async-timeout, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain-community, langchain-experimental\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3 httpx-sse-0.4.0 langchain-0.3.22 langchain-community-0.3.20 langchain-core-0.3.50 langchain-experimental-0.3.4 langchain-text-splitters-0.3.7 pydantic-settings-2.8.1 python-dotenv-1.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# ... (라이브러리 임포트 - 이전과 동일) ...\nimport os, glob, json, hashlib, torch\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nimport pandas as pd\n\n# --- 1. 설정 (Configuration) ---\n# ... (INPUT_DIR, OUTPUT_DIR, MODEL_NAME, BATCH_SIZE 등 설정 - 이전과 동일) ...\nINPUT_DIR = '/kaggle/input/predata/'\nOUTPUT_DIR = '/kaggle/working/output'\nOUTPUT_EMBEDDING_FILE = os.path.join(OUTPUT_DIR, 'chunk_embeddings.jsonl')\nMODEL_NAME = 'jhgan/ko-sbert-sts'\nBATCH_SIZE = 32\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"입력 디렉토리: {INPUT_DIR}\")\nprint(f\"출력 디렉토리: {OUTPUT_DIR}\")\nprint(f\"출력 파일: {OUTPUT_EMBEDDING_FILE}\")\n\n# --- 2. 모델 및 환경 설정 ---\n# ... (GPU 설정, 모델 로딩, Splitter 생성 - 이전과 동일, 오류 처리 포함) ...\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"사용 장치: {device}\")\nembedding_model = None\ntext_splitter = None\ntry:\n    print(f\"'{MODEL_NAME}' 임베딩 모델 로딩 시작...\")\n    embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n    print(\"임베딩 모델 로딩 완료.\")\n    langchain_embeddings = HuggingFaceEmbeddings(\n        model_name=MODEL_NAME, model_kwargs={'device': device}, encode_kwargs={'normalize_embeddings': False}\n    )\n    print(\"LangChain Embeddings 래퍼 생성 완료.\")\n    text_splitter = SemanticChunker(langchain_embeddings, breakpoint_threshold_type=\"percentile\")\n    print(\"SemanticChunker 생성 완료.\")\nexcept Exception as e:\n    print(f\"모델 또는 Splitter 초기화 중 오류 발생: {e}\")\n\n# --- 3. 이어하기 기능: 기존 데이터 로드 (ID와 해시 포함) --- ### 수정됨 ###\n\nexisting_data_dict = {} # 기존 데이터를 저장할 딕셔너리 {chunk_unique_id: data_dict}\nif os.path.exists(OUTPUT_EMBEDDING_FILE):\n    print(f\"기존 출력 파일 '{OUTPUT_EMBEDDING_FILE}'을 읽어옵니다...\")\n    try:\n        with open(OUTPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for line in f_in:\n                try:\n                    data = json.loads(line)\n                    note_id = data.get('note_id')\n                    chunk_id = data.get('chunk_id')\n                    # text_hash와 vector가 있는지 확인 (이전 버전 파일 호환성)\n                    text_hash = data.get('text_hash')\n                    vector = data.get('vector')\n                    if note_id is not None and chunk_id is not None and text_hash and vector:\n                        chunk_unique_id = f\"{note_id}_{chunk_id}\"\n                        existing_data_dict[chunk_unique_id] = data # 전체 데이터 저장\n                    # else:\n                    #     print(f\"경고: 기존 데이터에 필요한 필드(text_hash, vector 등)가 누락되었습니다. 해당 라인 건너뛰기: {line.strip()}\")\n                except json.JSONDecodeError:\n                    print(f\"경고: 출력 파일의 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n        print(f\"총 {len(existing_data_dict)}개의 기존 청크 데이터를 로드했습니다.\")\n    except Exception as e:\n        print(f\"경고: 기존 출력 파일을 읽는 중 오류 발생: {e}. 처음부터 다시 처리합니다.\")\n        existing_data_dict = {} # 오류 시 초기화\n\n# --- 4. 데이터 처리 및 임베딩 대상 선정 --- ### 수정됨 ###\n\nif embedding_model and text_splitter:\n    print(\"\\n--- 청크 분할 및 변경 사항 확인 시작 ---\")\n\n    chunks_to_embed = [] # 새로 임베딩해야 할 청크 데이터 리스트\n    processed_this_run = {} # 현재 실행에서 유효한 데이터 저장 (기존+신규+업데이트)\n\n    input_files = glob.glob(os.path.join(INPUT_DIR, '*.json'))\n    print(f\"총 {len(input_files)}개의 JSON 파일을 찾았습니다.\")\n\n    for filepath in tqdm(input_files, desc=\"파일 읽기 및 청킹/비교 중\"):\n        display_filepath = os.path.basename(filepath)\n        try:\n            with open(filepath, 'r', encoding='utf-8-sig') as f_in: content = json.load(f_in)\n\n            if 'data' in content and isinstance(content['data'], list):\n                for doc_data in content['data']:\n                    note_id = doc_data.get('source_id'); corpus = doc_data.get('corpus')\n                    doc_title = doc_data.get('title', '')\n                    if not note_id or not corpus or not isinstance(corpus, str): continue\n\n                    current_chunk_id_counter = 0\n                    try:\n                        chunks = text_splitter.split_text(corpus)\n                    except Exception as split_err:\n                        print(f\"오류: {display_filepath} (ID: {note_id}) 청킹 중 오류: {split_err}\")\n                        continue\n\n                    for chunk_text in chunks:\n                        if not chunk_text.strip(): continue\n                        chunk_id = current_chunk_id_counter\n                        chunk_unique_id = f\"{note_id}_{chunk_id}\"\n                        current_hash = hashlib.sha256(chunk_text.strip().encode('utf-8')).hexdigest()\n\n                        # *** 이어하기 + 변경 감지 로직 ***\n                        process_this_chunk = False\n                        previous_data = existing_data_dict.get(chunk_unique_id)\n\n                        if previous_data is None: # 신규 청크\n                            process_this_chunk = True\n                        elif previous_data.get('text_hash') != current_hash: # 내용 변경됨\n                            # print(f\"정보: 내용 변경 감지, 재처리: {chunk_unique_id}\") # 로그 (선택적)\n                            process_this_chunk = True\n\n                        # 현재 실행에서 유효한 데이터 구성\n                        current_chunk_data = {\n                            \"note_id\": note_id, \"chunk_id\": chunk_id,\n                            \"doc_title\": doc_title, \"filepath\": filepath,\n                            \"text\": chunk_text.strip(), \"text_hash\": current_hash\n                            # 'vector'는 나중에 추가됨\n                        }\n\n                        if process_this_chunk:\n                            chunks_to_embed.append(current_chunk_data) # 임베딩 대상 리스트에 추가\n                        else:\n                            # 변경 없는 경우, 기존 벡터 사용\n                            current_chunk_data['vector'] = previous_data.get('vector')\n\n                        # 현재 실행의 최종 데이터 딕셔너리에 저장/업데이트\n                        processed_this_run[chunk_unique_id] = current_chunk_data\n                        current_chunk_id_counter += 1\n\n            else: print(f\"경고: 파일 {display_filepath} 건너뛰기 ('data' 리스트 없음).\")\n        except json.JSONDecodeError as json_err: print(f\"오류: 파일 {display_filepath} JSON 구조 오류: {json_err}\")\n        except Exception as file_err: print(f\"파일 {display_filepath} 처리 중 오류: {file_err}\")\n\n    # --- 5. 배치 임베딩 (새 대상만) ---\n    newly_processed_count = len(chunks_to_embed)\n    print(f\"\\n총 {newly_processed_count}개의 신규/변경된 청크에 대해 임베딩을 시작합니다.\")\n\n    if newly_processed_count > 0:\n        for i in tqdm(range(0, newly_processed_count, BATCH_SIZE), desc=\"임베딩 중\"):\n            batch_data_to_embed = chunks_to_embed[i : i + BATCH_SIZE]\n            batch_texts = [item['text'] for item in batch_data_to_embed]\n            if not batch_texts: continue\n\n            try:\n                batch_embeddings = embedding_model.encode(\n                    batch_texts, convert_to_numpy=True, show_progress_bar=False, batch_size=len(batch_texts)\n                )\n                batch_embeddings_list = batch_embeddings.tolist()\n\n                # 계산된 벡터를 processed_this_run 딕셔너리에 업데이트\n                for j, embedding_vector in enumerate(batch_embeddings_list):\n                    # 임베딩 대상 리스트의 원본 딕셔너리를 직접 수정하면 안됨\n                    # processed_this_run 에서 해당 청크를 찾아 업데이트\n                    original_chunk_data = batch_data_to_embed[j]\n                    chunk_unique_id = f\"{original_chunk_data['note_id']}_{original_chunk_data['chunk_id']}\"\n                    if chunk_unique_id in processed_this_run:\n                         processed_this_run[chunk_unique_id]['vector'] = embedding_vector\n                    # else: # 이론상 이 경우는 없어야 함\n                    #     print(f\"경고: 임베딩된 청크 {chunk_unique_id}를 processed_this_run에서 찾을 수 없습니다.\")\n\n            except Exception as embed_err:\n                print(f\"오류: 배치 {i // BATCH_SIZE} 임베딩 중 오류: {embed_err}\")\n    else:\n        print(\"새로 처리할 청크가 없습니다.\")\n\n    # --- 6. 최종 결과 파일 저장 (전체 덮어쓰기) --- ### 수정됨 ###\n    print(f\"\\n--- 최종 결과 파일 저장 시작 ({len(processed_this_run)}개 청크) ---\")\n    # 출력 파일을 쓰기 모드('w')로 열어 전체 내용을 새로 씀\n    try:\n        with open(OUTPUT_EMBEDDING_FILE, 'w', encoding='utf-8') as f_out:\n            # 정렬 기준 설정 (note_id, chunk_id 순서) - 선택 사항이지만 일관성에 좋음\n            sorted_chunk_ids = sorted(processed_this_run.keys(), key=lambda x: (x.split('_')[0], int(x.split('_')[1])))\n\n            for chunk_unique_id in tqdm(sorted_chunk_ids, desc=\"최종 파일 저장 중\"):\n                final_chunk_data = processed_this_run[chunk_unique_id]\n                # 벡터 데이터가 있는지 최종 확인 (임베딩 오류 등으로 누락될 수 있음)\n                if 'vector' in final_chunk_data and final_chunk_data['vector']:\n                    json_string = json.dumps(final_chunk_data, ensure_ascii=False)\n                    f_out.write(json_string + '\\n')\n                else:\n                    print(f\"경고: 청크 {chunk_unique_id}의 벡터 데이터가 없어 최종 파일에 저장하지 않습니다.\")\n        print(f\"최종 결과가 '{OUTPUT_EMBEDDING_FILE}'에 저장되었습니다.\")\n    except Exception as write_err:\n        print(f\"오류: 최종 결과 파일 '{OUTPUT_EMBEDDING_FILE}' 저장 중 오류 발생: {write_err}\")\n\n\n    print(\"\\n--- 임베딩 생성 및 저장 완료 ---\")\n    print(\"-\" * 40)\n\nelse:\n    print(\"\\n오류: 모델 또는 텍스트 스플리터가 제대로 초기화되지 않아 임베딩 프로세스를 시작할 수 없습니다.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:06:24.270041Z","iopub.execute_input":"2025-04-04T06:06:24.270370Z","iopub.status.idle":"2025-04-04T06:07:02.197930Z","shell.execute_reply.started":"2025-04-04T06:06:24.270322Z","shell.execute_reply":"2025-04-04T06:07:02.196956Z"}},"outputs":[{"name":"stdout","text":"입력 디렉토리: /kaggle/input/predata/\n출력 디렉토리: /kaggle/working/output\n출력 파일: /kaggle/working/output/chunk_embeddings.jsonl\n사용 장치: cuda\n'jhgan/ko-sbert-sts' 임베딩 모델 로딩 시작...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518cfce7130348f3934bfe14907a335e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a7c71e6515440cac1ac6ff792f1900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ecc57c68bee4a339b3f5fc9721593e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b91f653d8a4c1da400361dd084ff29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93a78a6ebc834c26b886b5c3e815933d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c38e910bfb4608ad03c351087a771c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3391d907d65a4085bc2a7f3178a74958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7035eabd3fa7464dba46bcbbd19c3878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e230d614dfe44d8eb8e36e8cd2e396c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3bb10da117f46068fe35c1d5f45fabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e75c21b5f2e40288a8102963fd84c99"}},"metadata":{}},{"name":"stdout","text":"임베딩 모델 로딩 완료.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-5b2e683e7b5f>:32: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  langchain_embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":"LangChain Embeddings 래퍼 생성 완료.\nSemanticChunker 생성 완료.\n기존 출력 파일 '/kaggle/working/output/chunk_embeddings.jsonl'을 읽어옵니다...\n총 130개의 기존 청크 데이터를 로드했습니다.\n\n--- 청크 분할 및 변경 사항 확인 시작 ---\n총 10개의 JSON 파일을 찾았습니다.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"파일 읽기 및 청킹/비교 중:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c7ccde3f334211a3dfa6041b02526b"}},"metadata":{}},{"name":"stdout","text":"\n총 0개의 신규/변경된 청크에 대해 임베딩을 시작합니다.\n새로 처리할 청크가 없습니다.\n\n--- 최종 결과 파일 저장 시작 (130개 청크) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"최종 파일 저장 중:   0%|          | 0/130 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408f9d23d11a40c19552ee8279874992"}},"metadata":{}},{"name":"stdout","text":"최종 결과가 '/kaggle/working/output/chunk_embeddings.jsonl'에 저장되었습니다.\n\n--- 임베딩 생성 및 저장 완료 ---\n----------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\nimport networkx as nx # 그래프 기반 그룹핑을 위해 사용 (설치 필요 시 !pip install networkx)\nfrom tqdm.notebook import tqdm\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_EMBEDDING_FILE = '/kaggle/working/output/chunk_embeddings.jsonl' # 임베딩 데이터 파일\nOUTPUT_GROUP_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl' # 그룹핑 결과 파일\nSIMILARITY_THRESHOLD = 0.7  # 유사도 임계값 (실험적으로 조절 필요)\nMIN_GROUP_SIZE = 2          # 그룹으로 간주할 최소 청크 수 (예: 2개 이상 유사해야 그룹)\n\nprint(f\"입력 임베딩 파일: {INPUT_EMBEDDING_FILE}\")\nprint(f\"출력 그룹 파일: {OUTPUT_GROUP_FILE}\")\nprint(f\"유사도 임계값: {SIMILARITY_THRESHOLD}\")\nprint(f\"최소 그룹 크기: {MIN_GROUP_SIZE}\")\n\n# --- 2. 데이터 로드 ---\n\nall_chunk_data = [] # 청크 데이터 (메타데이터 + 벡터) 저장 리스트\nchunk_embeddings = [] # 벡터만 따로 저장할 리스트 (유사도 계산용)\nchunk_lookup = {}     # 인덱스 -> 청크 메타데이터 매핑용 딕셔너리\n\nprint(f\"\\n--- 임베딩 데이터 로딩 시작 ---\")\nif not os.path.exists(INPUT_EMBEDDING_FILE):\n    print(f\"오류: 임베딩 파일 '{INPUT_EMBEDDING_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(INPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for i, line in enumerate(f_in):\n                try:\n                    data = json.loads(line)\n                    # 필수 데이터 확인\n                    if 'note_id' in data and 'chunk_id' in data and 'vector' in data and 'text' in data:\n                        vector = np.array(data['vector'], dtype=np.float32) # numpy 배열로 변환\n                        # 벡터 차원 일관성 확인 (선택적)\n                        if i > 0 and vector.shape[0] != chunk_embeddings[0].shape[0]:\n                             print(f\"경고: {i}번째 청크 벡터 차원({vector.shape[0]})이 이전 벡터 차원({chunk_embeddings[0].shape[0]})과 다릅니다. 건너<0xEB><0x9B><0x84>니다.\")\n                             continue\n\n                        all_chunk_data.append(data) # 전체 데이터 저장\n                        chunk_embeddings.append(vector) # 벡터만 따로 저장\n                        chunk_lookup[i] = data # 인덱스로 메타데이터 조회 가능하도록\n                    else:\n                        print(f\"경고: {i}번째 라인에 필수 필드가 누락되어 건너<0xEB><0x9B><0x84>니다.\")\n                except json.JSONDecodeError:\n                    print(f\"경고: 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n                except Exception as parse_err:\n                     print(f\"경고: 데이터 처리 중 오류 발생 ({i}번째 라인): {parse_err}\")\n\n        if not all_chunk_data or not chunk_embeddings:\n             print(\"오류: 유효한 임베딩 데이터를 로드하지 못했습니다.\")\n             chunk_embeddings = None # 이후 처리 방지\n        else:\n            chunk_embeddings = np.array(chunk_embeddings) # 최종적으로 numpy 배열로 변환\n            print(f\"총 {len(all_chunk_data)}개의 청크 데이터 및 임베딩 로드 완료.\")\n            print(f\"임베딩 배열 형태: {chunk_embeddings.shape}\")\n\n    except Exception as e:\n        print(f\"임베딩 파일 로딩 중 오류 발생: {e}\")\n        chunk_embeddings = None\n\n# --- 3. 유사도 계산 및 그룹핑 ---\n\nif chunk_embeddings is not None and len(chunk_embeddings) >= MIN_GROUP_SIZE:\n    print(f\"\\n--- 유사도 계산 및 그룹핑 시작 (임계값: {SIMILARITY_THRESHOLD}) ---\")\n\n    # 코사인 유사도 계산 (모든 쌍)\n    print(\"코사인 유사도 행렬 계산 중...\")\n    # 벡터 정규화 (L2 norm) - cosine 유사도는 내적(dot product)으로 계산 가능\n    # SentenceTransformer 모델이 이미 정규화된 벡터를 반환할 수도 있지만, 안전하게 다시 정규화\n    # from sklearn.preprocessing import normalize\n    # embeddings_normalized = normalize(chunk_embeddings, axis=1, norm='l2')\n    # similarity_matrix = np.dot(embeddings_normalized, embeddings_normalized.T)\n    # 또는 sklearn 함수 직접 사용 (내부적으로 정규화 처리 가능성 있음)\n    similarity_matrix = cosine_similarity(chunk_embeddings)\n    print(\"유사도 행렬 계산 완료.\")\n\n    # 임계값 이상인 쌍들을 엣지로 하는 그래프 생성\n    print(\"유사도 기반 그래프 생성 중...\")\n    graph = nx.Graph()\n    num_chunks = len(all_chunk_data)\n    # 자기 자신과의 유사도는 제외하고, 임계값 이상인 엣지만 추가\n    for i in tqdm(range(num_chunks), desc=\"엣지 추가 중\"):\n        graph.add_node(i) # 모든 청크를 노드로 추가\n        for j in range(i + 1, num_chunks): # 중복 계산 피하기 위해 i+1 부터 시작\n            if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD:\n                graph.add_edge(i, j, weight=similarity_matrix[i, j]) # 엣지 추가 (weight는 유사도)\n\n    # 연결된 컴포넌트(그룹) 찾기\n    print(\"연결된 컴포넌트(그룹) 찾는 중...\")\n    connected_components = list(nx.connected_components(graph))\n\n    # 최소 크기 기준을 만족하는 그룹만 필터링\n    valid_groups = [group for group in connected_components if len(group) >= MIN_GROUP_SIZE]\n    print(f\"총 {len(valid_groups)}개의 유효한 그룹 (크기 >= {MIN_GROUP_SIZE})을 찾았습니다.\")\n\n    # --- 4. 그룹 정보 및 액션 대상 생성/저장 ---\n    print(f\"\\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\")\n    grouped_results = []\n    group_id_counter = 0\n\n    for group_indices in tqdm(valid_groups, desc=\"그룹 정보 생성 중\"):\n        group_data = {\n            \"group_id\": group_id_counter,\n            \"similarity_threshold\": SIMILARITY_THRESHOLD,\n            \"member_chunks\": [],\n            \"synthesis_input_texts\": [],\n            \"backlink_candidate_notes\": set() # 중복 제거 위해 Set 사용\n        }\n\n        # 그룹 멤버 정보 추가\n        member_texts = []\n        note_ids_in_group = set()\n        for index in group_indices:\n            chunk_info = chunk_lookup.get(index)\n            if chunk_info:\n                # 그룹 멤버 정보 구성 (필요한 메타데이터 추가)\n                member_chunk_info = {\n                    \"note_id\": chunk_info.get(\"note_id\"),\n                    \"chunk_id\": chunk_info.get(\"chunk_id\"),\n                    \"doc_title\": chunk_info.get(\"doc_title\", \"\"),\n                    \"filepath\": chunk_info.get(\"filepath\", \"\"),\n                    \"text_preview\": chunk_info.get(\"text\", \"\")[:100] + \"...\" # 텍스트 미리보기\n                    # 필요시 그룹 내 다른 멤버와의 평균/최대 유사도 등 추가 가능\n                }\n                group_data[\"member_chunks\"].append(member_chunk_info)\n                member_texts.append(chunk_info.get(\"text\", \"\")) # 통합용 텍스트\n                note_ids_in_group.add(chunk_info.get(\"note_id\")) # 백링크용 노트 ID\n\n        group_data[\"synthesis_input_texts\"] = member_texts\n        # Set을 리스트로 변환하여 저장\n        group_data[\"backlink_candidate_notes\"] = sorted(list(note_ids_in_group))\n\n        grouped_results.append(group_data)\n        group_id_counter += 1\n\n    # 최종 결과를 JSON Lines 파일로 저장\n    try:\n        with open(OUTPUT_GROUP_FILE, 'w', encoding='utf-8') as f_out:\n            for group_result in grouped_results:\n                json_string = json.dumps(group_result, ensure_ascii=False)\n                f_out.write(json_string + '\\n')\n        print(f\"그룹핑 결과가 '{OUTPUT_GROUP_FILE}'에 저장되었습니다.\")\n    except Exception as write_err:\n        print(f\"오류: 그룹핑 결과 파일 저장 중 오류 발생: {write_err}\")\n\n    print(\"\\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\")\n    print(\"-\" * 40)\n\nelif len(chunk_embeddings) < MIN_GROUP_SIZE:\n     print(f\"\\n오류: 로드된 청크 수가 최소 그룹 크기({MIN_GROUP_SIZE})보다 작아 그룹핑을 수행할 수 없습니다.\")\nelse:\n    print(\"\\n오류: 임베딩 데이터가 로드되지 않아 그룹핑 프로세스를 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:07:02.199442Z","iopub.execute_input":"2025-04-04T06:07:02.199694Z","iopub.status.idle":"2025-04-04T06:07:02.495176Z","shell.execute_reply.started":"2025-04-04T06:07:02.199673Z","shell.execute_reply":"2025-04-04T06:07:02.494153Z"}},"outputs":[{"name":"stdout","text":"입력 임베딩 파일: /kaggle/working/output/chunk_embeddings.jsonl\n출력 그룹 파일: /kaggle/working/output/grouped_chunks_info.jsonl\n유사도 임계값: 0.7\n최소 그룹 크기: 2\n\n--- 임베딩 데이터 로딩 시작 ---\n총 130개의 청크 데이터 및 임베딩 로드 완료.\n임베딩 배열 형태: (130, 768)\n\n--- 유사도 계산 및 그룹핑 시작 (임계값: 0.7) ---\n코사인 유사도 행렬 계산 중...\n유사도 행렬 계산 완료.\n유사도 기반 그래프 생성 중...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"엣지 추가 중:   0%|          | 0/130 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95ba8fa8a18485e97070d6e6fce75ae"}},"metadata":{}},{"name":"stdout","text":"연결된 컴포넌트(그룹) 찾는 중...\n총 15개의 유효한 그룹 (크기 >= 2)을 찾았습니다.\n\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"그룹 정보 생성 중:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78e50d20bf240538a26266849880baf"}},"metadata":{}},{"name":"stdout","text":"그룹핑 결과가 '/kaggle/working/output/grouped_chunks_info.jsonl'에 저장되었습니다.\n\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\n----------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport json\nimport os\n\n# 그룹핑 결과 파일 경로\nGROUP_INFO_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl'\n\nprint(f\"--- 그룹핑 결과 확인 ({GROUP_INFO_FILE}) ---\")\n\nif not os.path.exists(GROUP_INFO_FILE):\n    print(f\"오류: 그룹핑 결과 파일 '{GROUP_INFO_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(GROUP_INFO_FILE, 'r', encoding='utf-8') as f_in:\n            group_found = False\n            for line in f_in:\n                group_found = True\n                try:\n                    group_data = json.loads(line)\n                    group_id = group_data.get(\"group_id\", \"N/A\")\n                    threshold = group_data.get(\"similarity_threshold\", \"N/A\")\n                    member_chunks = group_data.get(\"member_chunks\", [])\n                    backlink_notes = group_data.get(\"backlink_candidate_notes\", [])\n\n                    print(f\"\\n===== 그룹 ID: {group_id} (임계값: {threshold}) =====\")\n                    print(f\"포함된 청크 수: {len(member_chunks)}\")\n                    print(f\"백링크 후보 노트 ID: {backlink_notes}\")\n\n                    print(\"\\n--- 포함된 청크 목록 (미리보기) ---\")\n                    if not member_chunks:\n                        print(\"  (포함된 청크 정보 없음)\")\n                    else:\n                        for i, chunk_info in enumerate(member_chunks):\n                            note_id = chunk_info.get(\"note_id\", \"?\")\n                            chunk_id = chunk_info.get(\"chunk_id\", \"?\")\n                            title = chunk_info.get(\"doc_title\", \"제목 없음\")\n                            preview = chunk_info.get(\"text_preview\", \"내용 없음\")\n                            print(f\"  {i+1}. [노트:{note_id} / 청크:{chunk_id}] (제목: {title})\")\n                            print(f\"     내용: {preview}\")\n\n                    # 통합용 텍스트는 너무 길 수 있으니 필요한 경우 별도 확인\n                    # synthesis_texts = group_data.get(\"synthesis_input_texts\", [])\n                    # print(\"\\n--- 통합 대상 텍스트 목록 ---\")\n                    # for i, text in enumerate(synthesis_texts):\n                    #     print(f\"  {i+1}. {text[:150]}...\") # 일부만 출력\n\n                    print(\"=\" * (len(str(group_id)) + 20)) # 구분선\n\n                except json.JSONDecodeError:\n                    print(f\"\\n오류: 잘못된 JSON 라인 발견 - {line.strip()}\")\n                except Exception as parse_err:\n                    print(f\"\\n오류: 그룹 데이터 처리 중 오류 발생 - {parse_err}\")\n\n            if not group_found:\n                print(\"결과 파일은 존재하지만, 유효한 그룹 정보를 찾지 못했습니다.\")\n\n    except Exception as e:\n        print(f\"그룹핑 결과 파일 읽기 중 오류 발생: {e}\")\n\nprint(\"\\n--- 그룹핑 결과 확인 완료 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:07:02.496246Z","iopub.execute_input":"2025-04-04T06:07:02.497286Z","iopub.status.idle":"2025-04-04T06:07:02.534841Z","shell.execute_reply.started":"2025-04-04T06:07:02.497252Z","shell.execute_reply":"2025-04-04T06:07:02.534182Z"}},"outputs":[{"name":"stdout","text":"--- 그룹핑 결과 확인 (/kaggle/working/output/grouped_chunks_info.jsonl) ---\n\n===== 그룹 ID: 0 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000105']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:1] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 연구배경 및 목적\n1. 연구의 배경민원행정서비스에 대한 만족도 조사는 한국행정연구원에서 1996년 조사모델과 방법을 개발한 이후 현재까지 지속적으로 수행하고 있는 중앙정부 및 지방...\n  2. [노트:S0000105 / 청크:3] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 11. 30(35일)\n▷ 공간적 범위\n- 횡성군 종합민원실\n▷ 내용적 범위\n- 서론(연구 개요로서 연구의 배경과 목적 및 범위 및 방법)\n- 민원 만족도 조사(민원서비스 시설, 민...\n=====================\n\n===== 그룹 ID: 1 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000105']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:8] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 6. 분산분석(ANOVA; analysis of variance)결과\n가. 연령별 분산분석\n1)연령별 민원시설관련 분산분석응답자들의 연령대별 평균차이를 비교하여 통계적으로 유의한 ...\n  2. [노트:S0000105 / 청크:9] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 나. 민원분야에 따른 분산분석\n1)민원시설관련 만족도 분산분석응답자들이 제공받은 민원서비스 분야별 접근성에 대한 만족도는 평균 72.70점으로, 주차장과 관련한 만족도는 61.85...\n  3. [노트:S0000105 / 청크:10] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 종합 평가\n1. 민원행정 시설에 대한 만족도\n▷ 민원행정 시설에 대한 종합만족지수는 70.33점으로 상반기에 비해 4.32점이 높아졌음. ▷ 접근성에 대한 만족도는 71.14점으로...\n=====================\n\n===== 그룹 ID: 2 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000286']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000286 / 청크:7] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 인증기관에서 발급한다. 3.8 인코딩(Encoding)\n기계적 처리를 위해 고안된 구문 또는 신호를 특정한 부호들의 나열로 그 형태를 바꾸는 것\n[정보통신용어사전, KS X ISO...\n  2. [노트:S0000286 / 청크:0] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 전자기록물 장기보존패키지 - 제2부: 디렉토리로 구조화된 방식(NEO3)\n1. 적용범위\n이 표준은 디지털객체에 대한 논리적 또는 물리적 캡슐화 규격을 규정하는 동시에 진본성, 무결...\n  3. [노트:S0000286 / 청크:10] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 영구기록물관리기관 등은 디렉토리로 구조화된 방식을 채택하는 경우라 하더라도 기관의 환경 등에 따라 NEO3가 아닌 다른 캡슐화 방식을 정의하고 적용할 수 있다. 5. 디렉토리로 구...\n=====================\n\n===== 그룹 ID: 3 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000337']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000337 / 청크:11] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 〈표3. 2 _ 2 4 ＞ 는 구직 희망자 재취업 교육훈련프로그램 필요여부를 살펴보았다. 분석결과 •필 요하다\" 가 3 0. 5 %로 가장 높은 비율로 나타났고 •매우필요하다• 3...\n  2. [노트:S0000337 / 청크:12] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 서비스직” 1 0. 3 %, ••임시직. 단순노무직'1 0. 3 % 순으로 나타 났다. 〈표3.2-26〉는 구직 형식을 살펴보았다. 분석결과 *자영업'아 43.1%로 가장 높은 비...\n=====================\n\n===== 그룹 ID: 4 (임계값: 0.7) =====\n포함된 청크 수: 4\n백링크 후보 노트 ID: ['S0000473']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000473 / 청크:0] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 『최저임금 적용효과에 관한 실태조사』품질개선 컨설팅 최종결과보고서 \n요약\n「최저임금 적용효과에 관한 실태조사」는 최저임금이 기업경영과 고용에 미치는 효과, 사용자와 근로자가 실제 ...\n  2. [노트:S0000473 / 청크:1] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 최저임금 등 임금분포와 조사대상\n○ 최저임금과 비교되는 시급 임금분포와 최저임금의 중위임금에 대비한 상대적 수준 추이는'최저임금 적용효과에 관한 실태조사'의 조사대상 선정기준이 현...\n  3. [노트:S0000473 / 청크:3] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 조사대상 업종 및 사업체 규모 범위 확대\n〇 현행 「최저임금 적용효과에 관한 실태조사」의 모집단은 저임금 근로자의고용 비중이 높은 업종(C, G, H, I, L, N, P, Q, ...\n  4. [노트:S0000473 / 청크:4] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: ○ 조사의 용이성\n- 조사 표본이 수월하게 확보될 수 있도록 설정되어야 함\n⇒ 각 기준들이 서로 상충될 수도 있음: 예를 들어, 국제적으로 통용되는 저임금근로자의 개념은 중위임금의...\n=====================\n\n===== 그룹 ID: 5 (임계값: 0.7) =====\n포함된 청크 수: 6\n백링크 후보 노트 ID: ['S0000474']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000474 / 청크:0] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 『여성농업인실태조사』품질개선 컨설팅 최종결과보고서\nFinal Report on Quality Improvement Consulting for 『Survey on the Korean...\n  2. [노트:S0000474 / 청크:3] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 그리고 다문화가정여성 또한 실제 가구수 비율(1.2%)보다 많은 250가구(12.5%)로 할당을 함\n- 다문화가구와 귀농여성의 실제 가구수 비율보다 많이 할당한 이유는\n귀농여성농업...\n  3. [노트:S0000474 / 청크:6] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: □ 조사표 개선 방향 도출\n○ 조사대상 재설정\n- 농업을 주업으로 하면서, 농업외 소득활동이 있는 겸업 여성농업인을 포함하기 위해 여성농업인의 정의를 재설정할 필요가 있다. - 조...\n  4. [노트:S0000474 / 청크:8] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 유사통계 개요\n여성농업인실태조사와 관련있는 15종의 승인통계(주로 조사통계)들을 검토하였다. 크게 여성관련 조사 6종, 농업관련 조사 4종, 귀농관련 조사 2종, 다문화 관련 조사...\n  5. [노트:S0000474 / 청크:9] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 그 외 조사의 경우 부록3을 참고하도록 한다. 제 2 절 유사통계 검토결과\n1. 조사대상 남성 포함\n여성농업인 실태를 파악하기 위해 현재의 조사대상인 여성 뿐 아니라 남성역시 조사...\n  6. [노트:S0000474 / 청크:10] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 따라서 여성관리자패널조사는 제외한 나머지 유사통계 및 농림축산식품부의 귀농귀촌실태조사의 예산을 비교 검토하였다. - 2018 여성농업인 실태 조사 : 일반여성농업인 1,500가구,...\n=====================\n\n===== 그룹 ID: 6 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000474']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000474 / 청크:4] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 1. 표본설계 기본현황\n- 일반여성의 경우, 1500명의 표본이 모집단인 조사시점 우리나라 동지역을 제외한 읍/면 지역에 거주하는 만 18세 이상의 농가 여성을 대표할 수있도록 시...\n  2. [노트:S0000474 / 청크:14] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 표본 감소로 인한 제약에 따라일부 지역들을 제외하여 전체 모집단의 지역별 특성을 충분히 반영하지 못하였으며, 일반여성농업인의 표본비율이 전체 모집단의 단 0.16% 수준 밖에미치지...\n=====================\n\n===== 그룹 ID: 7 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000474']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000474 / 청크:12] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 현재 여성농업인실태조사는 지역별 통계를 공표하지 않고있는데, 이러한 층화기준을 사용함으로써 지역별 공표 및 공표수준별 추정이가능할 것이다. 제 4 절 표본크기 결정 및 표본배분\n1...\n  2. [노트:S0000474 / 청크:13] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 제 6 절 가중치 및 모수 추정\n1. 가중치 조정\n표본설계는 표본조사의 결과가 모집단을 잘 대표할 수 있도록 설계하는 데에 궁극적인 목적이 있으나, 잘 계획된 표본설계가 실제 표본...\n=====================\n\n===== 그룹 ID: 8 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000565']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000565 / 청크:6] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: )\n◯ 유형별 평가결과를 보면 7개 유형 모두 사업평가 보다 회계평가 점수가 높았음. ◯ 1유형 사회통합의 종합평가 점수가 88.45점으로 가장 높았고, 5유형 평화증진 및 국가안...\n  2. [노트:S0000565 / 청크:9] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: (유형별 평균의 평균이 아님. )\n◯ 유형별 회계평가 결과를 항목 별로 살펴보면 자부담 집행 적법성(99.25점/100점 환산점수), 정산자료 구비(95.83점, 100점 환산점수...\n=====================\n\n===== 그룹 ID: 9 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000565']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000565 / 청크:10] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: · 사업평가 결과가 가장 우수한 단체는 1유형의 다년도 단체 A(92.61점)이었으며, 가장 미흡한 단체는 7유형의 다년도 단체 I(64.64점)이었음. · 10개 다년도 사업 중...\n  2. [노트:S0000565 / 청크:11] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: ◯ 컨소시엄 사업을 진행한 단체는 1개(4유형 1개)이며, 종합평가 점수가 전체사업 평균 대비 매우 낮은 58.40점을 기록하였음. 사업평가 점수는 62.67점으로 전체사업 평균 ...\n=====================\n\n===== 그룹 ID: 10 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000865']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:6] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 강릉아산병원 \n7. 1차 의료기관 15곳: 전국 분포 의원급 등 2차 병원 5개소와 1차기관 15개소 대상 감시망 구성. (2)감시망 운영\n1)각 협력병원에 초회 내원하는 호흡기질...\n  2. [노트:S0000865 / 청크:0] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 보고서 요약문\n본 사업은 전국 지역사회를 기반으로 하는 협력 병의원과 질병관리본부를 연계하여, 호흡기 감염증 병원체의 실험실 감시망을 구축하고 운영함으로서 국내 주요 호흡기 감염증...\n  3. [노트:S0000865 / 청크:17] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: pneumoniae 균주는 각각 1이 검출되었다. 호흡기감염증 감시 체계를 통하여 각 병원체의 지역별, 연령별, 월별 양성률을 파악하고원인 병원체의 발생 추이, 환자 임상 정보 및...\n======================\n\n===== 그룹 ID: 11 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000865']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:12] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, 3. 인천의료원 4. 평촌성심병원 5....\n  2. [노트:S0000865 / 청크:5] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, \n3. 인천의료원 \n4. 평촌성심병원 \n5 춘천 성심병원 \n6....\n======================\n\n===== 그룹 ID: 12 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000865', 'S0000957']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:11] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 점검회의\n(1)일시 : 2017년 1월 10일 14:00 ~ 16:00\n(2)장소 : 반포동스마트웍센터(3)참석자\n(4)내용\n- 대표성 확보 : region, age, season...\n  2. [노트:S0000865 / 청크:13] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 춘천 성심병원 6. 원주기독병원 강릉아산병원 \n(2)지역사회폐렴 감시망 참여병원 관리\n- 해당 참여병원에 대하여 지속적인 교육을 실시하고 있으며, 교육내용은 사업목적, 검체 채취방...\n  3. [노트:S0000957 / 청크:9] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 4. 참여기관 현장 방문 조사 및 건의 사항 청취\n1)현장 방문을 위한 조사지 구성\n2)현장 방문 필요 병원 연구진 회의를 통해 선정 \n- 6개 병원\n3)현장 방문 조사 내용\n▸방...\n======================\n\n===== 그룹 ID: 13 (임계값: 0.7) =====\n포함된 청크 수: 7\n백링크 후보 노트 ID: ['S0000954']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000954 / 청크:0] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 편집순서 6 : 총괄용역과제의 연구결과\n학술연구개발용역과제 연구결과\n1.1 목표\n가. 연구 주제: 줄기세포 연구 활용 촉진을 위한 사람 전분화능줄기세포의 심근세포 분화 표준 프로토...\n  2. [노트:S0000954 / 청크:1] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 나. 인간 전분화능줄기세포에서 심근세포로 분화하는 방법은 배아의 심장 발생 과정과 환경을 모티브로 하는 기법들이 개발되었으며 대표적인 3가지 방법으로 1)배발생 단계의 환경과 유사...\n  3. [노트:S0000954 / 청크:2] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 공배양 기법은 발생학적 관점에서 접근하여, 심장 발생 과정에 중요한 내배엽성세포를 이용한 심근세포의 분화 유도 방법으로, 인간전분화능줄기세포와 내배엽성세포(END2)의 공배양을 통...\n  4. [노트:S0000954 / 청크:7] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 발주부서 연구원에게 CMC3 심근세포 분화 SOP 기술이전 \n가. CMC3 세포주를 이용한 심근세포 분화 기술 개발관련 SOP 확립: SOP 제공 전, 2017.10.18. 발주부...\n  5. [노트:S0000954 / 청크:8] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 2017.10.26. CMC3 세포주를 이용한 인간전분화능줄기세포 유래 심근세포 분화기술 SOP 문서를 제공함. 다....\n  6. [노트:S0000954 / 청크:11] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 마. 교육 및 기술이전 내용: SOP와 동일한 방법으로 CMC3 세포주를 이용하여 미분화배양, 계대배양, 심근세포 분화, 분화된 심근세포(응수축 심근세포)확인, 심근세포 정제 관련...\n  7. [노트:S0000954 / 청크:14] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 세포치료제 개발을 위한 심근세포 프로토콜 개발 전략\n가. 안전성 확보를 위한 프로토콜 개발: Xeno- free 배양을 위한 matrigel 대체 vitronectin, lamin...\n======================\n\n===== 그룹 ID: 14 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000957']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000957 / 청크:2] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 중환자실에서 유치도뇨관 삽입률을 줄이기 어렵다는 것이 주요원인이었다. 이와 관련하여 프로그램을 보완하는 것이 필요해 보인다. ◦ 국내에서는 유치도뇨관 관련 요로감염 예방 프로그램은...\n  2. [노트:S0000957 / 청크:3] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 유치도뇨관 삽입의 대체 방법 강구\n3. 소변량 측정을 위한 방광 스캐너 사용\n4. 폐쇄된 요관 시스템 사용\n5. 정기적인 유치도뇨관 교체\n6. 무증상 세균뇨에 대한 선별\n7. 손위...\n  3. [노트:S0000957 / 청크:1] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 예방프로그램의 주요 목표를 표 1에 정리하였다. 매일 유치도뇨관 장착 여부와 필요성을 확인하고, 유치도뇨관보다는 다른 대체방법을 강구하고, 유치도뇨관을 삽입할 때 무균적으로 시행하...\n======================\n\n--- 그룹핑 결과 확인 완료 ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_EMBEDDING_FILE = '/kaggle/working/output/chunk_embeddings.jsonl' # 임베딩 데이터 파일\n\nprint(f\"입력 임베딩 파일: {INPUT_EMBEDDING_FILE}\")\n\n# --- 2. 데이터 로드 및 노트별 그룹핑 ---\n\nnotes_data = defaultdict(list) # {note_id: [vector1, vector2, ...]} 형태\nnote_chunk_counts = defaultdict(int) # 노트별 청크 수 저장\n\nprint(f\"\\n--- 임베딩 데이터 로드 및 노트별 그룹핑 시작 ---\")\nif not os.path.exists(INPUT_EMBEDDING_FILE):\n    print(f\"오류: 임베딩 파일 '{INPUT_EMBEDDING_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        line_count = 0\n        with open(INPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for line in f_in:\n                line_count += 1\n                try:\n                    data = json.loads(line)\n                    note_id = data.get('note_id')\n                    vector = data.get('vector')\n                    if note_id and vector:\n                        # 벡터를 float32 numpy 배열로 변환하여 추가\n                        notes_data[note_id].append(np.array(vector, dtype=np.float32))\n                        note_chunk_counts[note_id] += 1\n                except json.JSONDecodeError:\n                    print(f\"경고: 잘못된 JSON 라인 건너뛰기 ({line_count}번째 라인)\")\n                except Exception as parse_err:\n                     print(f\"경고: 데이터 처리 중 오류 발생 ({line_count}번째 라인): {parse_err}\")\n\n        print(f\"총 {len(notes_data)}개의 노트에서 {sum(note_chunk_counts.values())}개의 청크 임베딩 로드 완료.\")\n\n    except Exception as e:\n        print(f\"임베딩 파일 로딩 중 오류 발생: {e}\")\n        notes_data = None # 이후 처리 방지\n\n# --- 3. 문서 내 유사도 계산 및 통계 분석 ---\n\nif notes_data:\n    print(\"\\n--- 각 문서 내 청크 간 유사도 분포 분석 시작 ---\")\n\n    results_per_note = [] # 각 노트별 통계 저장 리스트\n    all_similarities = [] # 전체 유사도 값 저장 리스트\n\n    # 각 노트를 순회하며 분석\n    for note_id, vectors in tqdm(notes_data.items(), desc=\"문서별 유사도 분석 중\"):\n        num_chunks = len(vectors)\n\n        # 청크가 2개 이상인 경우에만 유사도 계산 가능\n        if num_chunks >= 2:\n            # 벡터 리스트를 numpy 배열로 변환\n            embeddings_array = np.array(vectors)\n\n            # 코사인 유사도 계산\n            try:\n                # 벡터가 이미 정규화되었다고 가정하고 내적 사용 or cosine_similarity 직접 사용\n                # sim_matrix = np.dot(embeddings_array, embeddings_array.T)\n                sim_matrix = cosine_similarity(embeddings_array)\n\n                # 대각선(자기 자신과의 유사도) 및 하삼각행렬 제외하고 유사도 값 추출\n                # np.triu_indices: 상삼각행렬의 인덱스를 반환 (k=1은 대각선 제외)\n                indices = np.triu_indices(num_chunks, k=1)\n                unique_similarities = sim_matrix[indices]\n\n                # 전체 유사도 리스트에 추가\n                all_similarities.extend(unique_similarities)\n\n                # 기술 통계량 계산 (numpy 사용)\n                if len(unique_similarities) > 0:\n                    stats = {\n                        'note_id': note_id,\n                        'num_chunks': num_chunks,\n                        'num_pairs': len(unique_similarities),\n                        'mean': np.mean(unique_similarities),\n                        'std': np.std(unique_similarities),\n                        'min': np.min(unique_similarities),\n                        '25% (Q1)': np.percentile(unique_similarities, 25),\n                        '50% (Median)': np.median(unique_similarities),\n                        '75% (Q3)': np.percentile(unique_similarities, 75),\n                        'max': np.max(unique_similarities)\n                    }\n                    results_per_note.append(stats)\n                else: # 비교할 쌍이 없는 경우 (이론상 num_chunks >= 2 이므로 발생 안 함)\n                     results_per_note.append({'note_id': note_id, 'num_chunks': num_chunks, 'num_pairs': 0})\n\n            except Exception as calc_err:\n                print(f\"오류: 노트 '{note_id}' 유사도 계산 중 오류 발생: {calc_err}\")\n        else:\n             # 청크가 1개인 노트는 비교 불가\n             results_per_note.append({'note_id': note_id, 'num_chunks': num_chunks, 'num_pairs': 0})\n\n    # --- 4. 결과 출력 ---\n    print(\"\\n--- 문서별 유사도 통계 결과 ---\")\n    if results_per_note:\n        df_results = pd.DataFrame(results_per_note)\n        # 보기 좋게 출력 (note_id 기준 정렬)\n        print(df_results.sort_values(by='note_id').to_string())\n    else:\n        print(\"분석할 노트가 없습니다.\")\n\n    print(\"\\n--- 전체 문서 내 청크 간 유사도 통계 (고유 쌍 기준) ---\")\n    if all_similarities:\n        all_similarities_array = np.array(all_similarities)\n        overall_stats = {\n            'Total Pairs': len(all_similarities_array),\n            'Overall Mean': np.mean(all_similarities_array),\n            'Overall Std': np.std(all_similarities_array),\n            'Overall Min': np.min(all_similarities_array),\n            'Overall 25% (Q1)': np.percentile(all_similarities_array, 25),\n            'Overall 50% (Median)': np.median(all_similarities_array),\n            'Overall 75% (Q3)': np.percentile(all_similarities_array, 75),\n            'Overall Max': np.max(all_similarities_array)\n        }\n        # 보기 좋게 출력\n        for key, value in overall_stats.items():\n             # 소수점 4자리까지 표시\n             if isinstance(value, (float, np.float32, np.float64)):\n                 print(f\"{key}: {value:.4f}\")\n             else:\n                 print(f\"{key}: {value}\")\n\n        # 히스토그램 시각화 (선택 사항, matplotlib 필요: !pip install matplotlib)\n        # import matplotlib.pyplot as plt\n        # plt.figure(figsize=(10, 6))\n        # plt.hist(all_similarities_array, bins=50, color='skyblue', edgecolor='black')\n        # plt.title('Distribution of Cosine Similarities between Chunks within Documents')\n        # plt.xlabel('Cosine Similarity')\n        # plt.ylabel('Frequency')\n        # plt.grid(axis='y', alpha=0.75)\n        # plt.show()\n\n    else:\n        print(\"계산된 유사도 값이 없습니다.\")\n\n    print(\"\\n--- 분석 완료 ---\")\n    print(\"위 통계 결과(특히 전체 중앙값, Q3 등)를 참고하여 그룹핑 임계값 후보를 정할 수 있습니다.\")\n    print(\"-\" * 40)\n\nelse:\n    print(\"\\n오류: 임베딩 데이터가 로드되지 않아 분석을 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:21:17.634053Z","iopub.execute_input":"2025-04-05T16:21:17.634319Z","iopub.status.idle":"2025-04-05T16:21:19.589342Z","shell.execute_reply.started":"2025-04-05T16:21:17.634297Z","shell.execute_reply":"2025-04-05T16:21:19.588355Z"}},"outputs":[{"name":"stdout","text":"입력 임베딩 파일: /kaggle/working/output/chunk_embeddings.jsonl\n\n--- 임베딩 데이터 로드 및 노트별 그룹핑 시작 ---\n총 10개의 노트에서 130개의 청크 임베딩 로드 완료.\n\n--- 각 문서 내 청크 간 유사도 분포 분석 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"문서별 유사도 분석 중:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6181daa41cab4de2a82805757d02be52"}},"metadata":{}},{"name":"stdout","text":"\n--- 문서별 유사도 통계 결과 ---\n    note_id  num_chunks  num_pairs      mean       std       min  25% (Q1)  50% (Median)  75% (Q3)       max\n0  S0000105          12         66  0.402825  0.222825 -0.080198  0.263595      0.464515  0.574849  0.789231\n1  S0000286          12         66  0.506660  0.118531  0.251630  0.429345      0.526255  0.600050  0.784638\n2  S0000337          24        276  0.398637  0.125208  0.093367  0.311305      0.405393  0.488760  0.731864\n3  S0000473           5         10  0.606027  0.107232  0.455347  0.497419      0.621908  0.694066  0.764980\n4  S0000474          15        105  0.501833  0.209834 -0.051862  0.496606      0.568077  0.619850  0.755033\n5  S0000565          12         66  0.458255  0.174801  0.024859  0.426608      0.511979  0.567507  0.741638\n6  S0000803           5         10  0.372503  0.165366  0.185897  0.216926      0.300597  0.560831  0.599471\n7  S0000865          18        153  0.382753  0.179589  0.012226  0.261097      0.404519  0.501824  0.886461\n8  S0000954          15        105  0.436240  0.197642  0.001856  0.268906      0.476777  0.593371  0.761264\n9  S0000957          12         66  0.410563  0.188077 -0.002264  0.289720      0.451702  0.545454  0.728094\n\n--- 전체 문서 내 청크 간 유사도 통계 (고유 쌍 기준) ---\nTotal Pairs: 923\nOverall Mean: 0.4271\nOverall Std: 0.1771\nOverall Min: -0.0802\nOverall 25% (Q1): 0.3117\nOverall 50% (Median): 0.4529\nOverall 75% (Q3): 0.5577\nOverall Max: 0.8865\n\n--- 분석 완료 ---\n위 통계 결과(특히 전체 중앙값, Q3 등)를 참고하여 그룹핑 임계값 후보를 정할 수 있습니다.\n----------------------------------------\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\nimport networkx as nx # 그래프 기반 그룹핑을 위해 사용\nfrom tqdm.notebook import tqdm\nimport pandas as pd # 결과 확인용 (선택 사항)\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_EMBEDDING_FILE = '/kaggle/working/output/chunk_embeddings.jsonl' # 임베딩 데이터 파일\nOUTPUT_GROUP_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl' # 그룹핑 결과 파일\nSIMILARITY_THRESHOLD = 0.8  # 유사도 임계값 ### 0.8로 수정됨 ###\nMIN_GROUP_SIZE = 2          # 그룹으로 간주할 최소 청크 수\n\nprint(f\"입력 임베딩 파일: {INPUT_EMBEDDING_FILE}\")\nprint(f\"출력 그룹 파일: {OUTPUT_GROUP_FILE}\")\nprint(f\"유사도 임계값: {SIMILARITY_THRESHOLD}\")\nprint(f\"최소 그룹 크기: {MIN_GROUP_SIZE}\")\n\n# --- 2. 데이터 로드 ---\n\nall_chunk_data = [] # 청크 데이터 (메타데이터 + 벡터) 저장 리스트\nchunk_embeddings = [] # 벡터만 따로 저장할 리스트 (유사도 계산용)\nchunk_lookup = {}     # 인덱스 -> 청크 메타데이터 매핑용 딕셔너리\n\nprint(f\"\\n--- 임베딩 데이터 로딩 시작 ---\")\nif not os.path.exists(INPUT_EMBEDDING_FILE):\n    print(f\"오류: 임베딩 파일 '{INPUT_EMBEDDING_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(INPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for i, line in enumerate(f_in):\n                try:\n                    data = json.loads(line)\n                    if 'note_id' in data and 'chunk_id' in data and 'vector' in data and 'text' in data:\n                        vector = np.array(data['vector'], dtype=np.float32)\n                        if i > 0 and vector.shape[0] != chunk_embeddings[0].shape[0]:\n                             print(f\"경고: {i}번째 청크 벡터 차원 불일치. 건너<0xEB><0x9B><0x84>니다.\")\n                             continue\n                        all_chunk_data.append(data)\n                        chunk_embeddings.append(vector)\n                        chunk_lookup[i] = data\n                    else: print(f\"경고: {i}번째 라인 필수 필드 누락. 건너<0xEB><0x9B><0x84>니다.\")\n                except json.JSONDecodeError: print(f\"경고: 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n                except Exception as parse_err: print(f\"경고: 데이터 처리 중 오류 ({i}번째 라인): {parse_err}\")\n\n        if not all_chunk_data or not chunk_embeddings:\n             print(\"오류: 유효한 임베딩 데이터를 로드하지 못했습니다.\")\n             chunk_embeddings = None\n        else:\n            chunk_embeddings = np.array(chunk_embeddings)\n            print(f\"총 {len(all_chunk_data)}개의 청크 데이터 및 임베딩 로드 완료.\")\n            print(f\"임베딩 배열 형태: {chunk_embeddings.shape}\")\n\n    except Exception as e:\n        print(f\"임베딩 파일 로딩 중 오류 발생: {e}\")\n        chunk_embeddings = None\n\n# --- 3. 유사도 계산 및 그룹핑 ---\n\nif chunk_embeddings is not None and len(chunk_embeddings) >= MIN_GROUP_SIZE:\n    print(f\"\\n--- 유사도 계산 및 그룹핑 시작 (임계값: {SIMILARITY_THRESHOLD}) ---\")\n\n    # 코사인 유사도 계산 (모든 쌍)\n    print(\"코사인 유사도 행렬 계산 중...\")\n    try:\n        similarity_matrix = cosine_similarity(chunk_embeddings)\n        print(\"유사도 행렬 계산 완료.\")\n    except Exception as sim_err:\n        print(f\"오류: 유사도 행렬 계산 중 오류 발생: {sim_err}\")\n        similarity_matrix = None\n\n    if similarity_matrix is not None:\n        # 임계값 이상인 쌍들을 엣지로 하는 그래프 생성\n        print(\"유사도 기반 그래프 생성 중...\")\n        graph = nx.Graph()\n        num_chunks = len(all_chunk_data)\n        for i in tqdm(range(num_chunks), desc=\"엣지 추가 중\"):\n            graph.add_node(i)\n            for j in range(i + 1, num_chunks):\n                # 부동 소수점 비교 시 작은 오차 고려 (선택적)\n                # if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD - 1e-9:\n                if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD:\n                    graph.add_edge(i, j, weight=similarity_matrix[i, j])\n\n        # 연결된 컴포넌트(그룹) 찾기\n        print(\"연결된 컴포넌트(그룹) 찾는 중...\")\n        connected_components = list(nx.connected_components(graph))\n\n        # 최소 크기 기준을 만족하는 그룹만 필터링\n        valid_groups = [group for group in connected_components if len(group) >= MIN_GROUP_SIZE]\n        print(f\"총 {len(valid_groups)}개의 유효한 그룹 (크기 >= {MIN_GROUP_SIZE})을 찾았습니다.\")\n\n        # --- 4. 그룹 정보 및 액션 대상 생성/저장 ---\n        print(f\"\\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\")\n        grouped_results = []\n        group_id_counter = 0\n\n        for group_indices in tqdm(valid_groups, desc=\"그룹 정보 생성 중\"):\n            # 그룹 인덱스들을 set으로 변환 (조회 속도 향상)\n            group_indices_set = set(group_indices)\n\n            group_data = {\n                \"group_id\": group_id_counter,\n                \"similarity_threshold\": SIMILARITY_THRESHOLD,\n                \"member_chunks\": [],\n                \"synthesis_input_texts\": [],\n                \"backlink_candidate_notes\": set()\n            }\n\n            member_texts = []\n            note_ids_in_group = set()\n            # 그룹 멤버 정보 추가\n            for index in group_indices_set:\n                chunk_info = chunk_lookup.get(index)\n                if chunk_info:\n                    member_chunk_info = {\n                        \"note_id\": chunk_info.get(\"note_id\"),\n                        \"chunk_id\": chunk_info.get(\"chunk_id\"),\n                        \"doc_title\": chunk_info.get(\"doc_title\", \"\"),\n                        \"filepath\": chunk_info.get(\"filepath\", \"\"),\n                        \"text_preview\": chunk_info.get(\"text\", \"\")[:100] + \"...\"\n                        # 그룹 내 다른 멤버와의 유사도 정보 추가 (선택적, 계산량 증가)\n                        # \"similarities_within_group\": {}\n                    }\n                    # # 그룹 내 다른 멤버와의 유사도 계산 (선택적)\n                    # for other_index in group_indices_set:\n                    #     if index != other_index:\n                    #         sim = similarity_matrix[index, other_index]\n                    #         member_chunk_info[\"similarities_within_group\"][f\"chunk_{other_index}\"] = round(sim, 4)\n\n                    group_data[\"member_chunks\"].append(member_chunk_info)\n                    member_texts.append(chunk_info.get(\"text\", \"\"))\n                    note_ids_in_group.add(chunk_info.get(\"note_id\"))\n\n            group_data[\"synthesis_input_texts\"] = member_texts\n            group_data[\"backlink_candidate_notes\"] = sorted(list(note_ids_in_group))\n\n            # member_chunks 리스트를 chunk_id 기준으로 정렬 (선택적)\n            group_data[\"member_chunks\"].sort(key=lambda x: (x[\"note_id\"], x[\"chunk_id\"]))\n\n            grouped_results.append(group_data)\n            group_id_counter += 1\n\n        # 최종 결과를 JSON Lines 파일로 저장 (덮어쓰기 'w')\n        try:\n            with open(OUTPUT_GROUP_FILE, 'w', encoding='utf-8') as f_out:\n                for group_result in grouped_results:\n                    json_string = json.dumps(group_result, ensure_ascii=False)\n                    f_out.write(json_string + '\\n')\n            print(f\"그룹핑 결과가 '{OUTPUT_GROUP_FILE}'에 저장되었습니다.\")\n        except Exception as write_err:\n            print(f\"오류: 그룹핑 결과 파일 저장 중 오류 발생: {write_err}\")\n\n        print(\"\\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\")\n        print(\"-\" * 40)\n\n    else:\n         print(\"오류: 유사도 행렬 계산에 실패하여 그룹핑을 진행할 수 없습니다.\")\n\nelif len(chunk_embeddings) < MIN_GROUP_SIZE:\n     print(f\"\\n오류: 로드된 청크 수가 최소 그룹 크기({MIN_GROUP_SIZE})보다 작아 그룹핑을 수행할 수 없습니다.\")\nelse:\n    print(\"\\n오류: 임베딩 데이터가 로드되지 않아 그룹핑 프로세스를 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:50:13.118610Z","iopub.execute_input":"2025-04-05T16:50:13.118948Z","iopub.status.idle":"2025-04-05T16:50:13.234469Z","shell.execute_reply.started":"2025-04-05T16:50:13.118925Z","shell.execute_reply":"2025-04-05T16:50:13.233516Z"}},"outputs":[{"name":"stdout","text":"입력 임베딩 파일: /kaggle/working/output/chunk_embeddings.jsonl\n출력 그룹 파일: /kaggle/working/output/grouped_chunks_info.jsonl\n유사도 임계값: 0.8\n최소 그룹 크기: 2\n\n--- 임베딩 데이터 로딩 시작 ---\n총 130개의 청크 데이터 및 임베딩 로드 완료.\n임베딩 배열 형태: (130, 768)\n\n--- 유사도 계산 및 그룹핑 시작 (임계값: 0.8) ---\n코사인 유사도 행렬 계산 중...\n유사도 행렬 계산 완료.\n유사도 기반 그래프 생성 중...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"엣지 추가 중:   0%|          | 0/130 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ce921b86d54d04a215c7354a85c768"}},"metadata":{}},{"name":"stdout","text":"연결된 컴포넌트(그룹) 찾는 중...\n총 1개의 유효한 그룹 (크기 >= 2)을 찾았습니다.\n\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"그룹 정보 생성 중:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45923ccace1d4dc2b91d6b31bff239db"}},"metadata":{}},{"name":"stdout","text":"그룹핑 결과가 '/kaggle/working/output/grouped_chunks_info.jsonl'에 저장되었습니다.\n\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\n----------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport json\nimport os\n\n# 그룹핑 결과 파일 경로\nGROUP_INFO_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl'\n\nprint(f\"--- 그룹핑 결과 확인 ({GROUP_INFO_FILE}) ---\")\n\nif not os.path.exists(GROUP_INFO_FILE):\n    print(f\"오류: 그룹핑 결과 파일 '{GROUP_INFO_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(GROUP_INFO_FILE, 'r', encoding='utf-8') as f_in:\n            group_found = False\n            for line in f_in:\n                group_found = True\n                try:\n                    group_data = json.loads(line)\n                    group_id = group_data.get(\"group_id\", \"N/A\")\n                    threshold = group_data.get(\"similarity_threshold\", \"N/A\")\n                    member_chunks = group_data.get(\"member_chunks\", [])\n                    backlink_notes = group_data.get(\"backlink_candidate_notes\", [])\n\n                    print(f\"\\n===== 그룹 ID: {group_id} (임계값: {threshold}) =====\")\n                    print(f\"포함된 청크 수: {len(member_chunks)}\")\n                    print(f\"백링크 후보 노트 ID: {backlink_notes}\")\n\n                    print(\"\\n--- 포함된 청크 목록 (미리보기) ---\")\n                    if not member_chunks:\n                        print(\"  (포함된 청크 정보 없음)\")\n                    else:\n                        for i, chunk_info in enumerate(member_chunks):\n                            note_id = chunk_info.get(\"note_id\", \"?\")\n                            chunk_id = chunk_info.get(\"chunk_id\", \"?\")\n                            title = chunk_info.get(\"doc_title\", \"제목 없음\")\n                            preview = chunk_info.get(\"text_preview\", \"내용 없음\")\n                            print(f\"  {i+1}. [노트:{note_id} / 청크:{chunk_id}] (제목: {title})\")\n                            print(f\"     내용: {preview}\")\n\n                    # 통합용 텍스트는 너무 길 수 있으니 필요한 경우 별도 확인\n                    # synthesis_texts = group_data.get(\"synthesis_input_texts\", [])\n                    # print(\"\\n--- 통합 대상 텍스트 목록 ---\")\n                    # for i, text in enumerate(synthesis_texts):\n                    #     print(f\"  {i+1}. {text[:150]}...\") # 일부만 출력\n\n                    print(\"=\" * (len(str(group_id)) + 20)) # 구분선\n\n                except json.JSONDecodeError:\n                    print(f\"\\n오류: 잘못된 JSON 라인 발견 - {line.strip()}\")\n                except Exception as parse_err:\n                    print(f\"\\n오류: 그룹 데이터 처리 중 오류 발생 - {parse_err}\")\n\n            if not group_found:\n                print(\"결과 파일은 존재하지만, 유효한 그룹 정보를 찾지 못했습니다.\")\n\n    except Exception as e:\n        print(f\"그룹핑 결과 파일 읽기 중 오류 발생: {e}\")\n\nprint(\"\\n--- 그룹핑 결과 확인 완료 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:50:13.306471Z","iopub.execute_input":"2025-04-05T16:50:13.306731Z","iopub.status.idle":"2025-04-05T16:50:13.316938Z","shell.execute_reply.started":"2025-04-05T16:50:13.306711Z","shell.execute_reply":"2025-04-05T16:50:13.316186Z"}},"outputs":[{"name":"stdout","text":"--- 그룹핑 결과 확인 (/kaggle/working/output/grouped_chunks_info.jsonl) ---\n\n===== 그룹 ID: 0 (임계값: 0.8) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000865']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:5] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, \n3. 인천의료원 \n4. 평촌성심병원 \n5 춘천 성심병원 \n6....\n  2. [노트:S0000865 / 청크:12] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, 3. 인천의료원 4. 평촌성심병원 5....\n=====================\n\n--- 그룹핑 결과 확인 완료 ---\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}