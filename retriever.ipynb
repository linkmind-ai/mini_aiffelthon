{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ea8ba4-1e16-4b3b-b22e-e2e5c813965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 10ê°œì˜ JSON ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "===============================\n",
      "ğŸ” ì²­í‚¹ ì„¤ì •: 500+50\n",
      "===============================\n",
      "\n",
      "â–¶ï¸ [Top 1] ìœ ì‚¬ë„: 0.6761\n",
      "ë¡œ ì¥ê¸°ë°œì „ì „ëµ ìˆ˜ë¦½\n",
      "â–· ê³ ê°ë§Œì¡±ë„ ì œê³ ë¥¼ ìœ„í•œ ê¸°ì´ˆì¡°ì‚¬ì‹¤ì‹œì˜ í•„ìš”ì„± ë¶€ê°\n",
      "â–· ëˆˆë†’ì´ ë¯¼ì›í–‰ì •ì˜ ì‹¤í˜„\n",
      "2. ì—°êµ¬ì˜ ëª©ì \n",
      "íš¡ì„±êµ° ì¢…í•©ë¯¼ì›ì‹¤ì—ì„œ ì œê³µí•˜ëŠ” í–‰ì •ì„œë¹„ìŠ¤ì˜ ê³ ê°ì¸ ì´ìš©ìë“¤ë¡œ í•˜ì—¬ê¸ˆ ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ì˜ ì „ë‹¬ì²´ê³„ì™€ ê·¸ ì„œë¹„ìŠ¤ ê³¼ì •ì—ì„œ ê³µë¬´ì›ë“¤ì˜ í–‰íƒœì™€ ë¯¼ì› ì œë„, ì‹œì„¤ ë“±ì— ëŒ€í•˜ì—¬ ì²´ê°ì •ë„ë¥¼ ì¡°ì‚¬ë¥¼ ëª©ì ìœ¼ë¡œ í•˜ê³  ìˆë‹¤. \n",
      "íš¡ì„±êµ° ë¯¼ì›í–‰ì • ì„œë¹„ìŠ¤ ê³ ê°ë§Œì¡±ë„ ì¡°ì‚¬ëŠ” êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì˜ì˜ë¥¼ ë‚´í¬í•˜ê³  ìˆë‹¤. \n",
      "â–· ê³ ê°ì¤‘ì‹¬ì˜ ì´ˆì¼ë¥˜ ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ì˜ ì‹¤í˜„\n",
      "â–· ê³ ê°ë§Œì¡±ë„ ì œê³  ë° ê³ ê°ê°ë™ ì§€í–¥\n",
      "â–· ë¶ˆí•©ë¦¬í•œ ì œë„ê°œì„ ê³¼ ë°œì „ë°©í–¥ ì œì‹œ\n",
      "â–· ë§ì¶¤í˜• ë¯¼ì›í–‰ì • ì„œë¹„ìŠ¤ ì‹¤í˜„\n",
      "â–· ì „ìë¯¼ì› í–‰ì •êµ¬í˜„ì„ ìœ„í•œ ê¸°ì´ˆì¡°ì‚¬\n",
      "â–· í–‰ì •ì—…ë¬´ ë¶„ì•¼ë³„ ë§Œì¡±ë„ ì œê³ \n",
      "â–· í–‰ì •ì„œë¹„ìŠ¤ì˜ ì·¨ì•½ì ê³¼ ê°œì„ ë°©í–¥ì„ ë„ì¶œ\n",
      "â–· ê³¼ê±°ì˜ ê³ ê°ë§Œì¡±ë„ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ê³ ê°ë§Œì¡±ë„ ìˆ˜ì¤€ì˜ ë³€í™”ì •ë„ë¥¼ ë¶„ì„ê³ ê°ë§Œì¡±ë„ ì¡°ì‚¬ëŠ” ê¶ê·¹ì ìœ¼ë¡œ í‰ê°€ëŒ€ìƒì´ ë˜ëŠ” ì¢…í•©ë¯¼ì›ì‹¤ë¡œ í•˜ì—¬ê¸ˆ ê³ ê°ì¤‘ì‹¬ì˜ í–‰ì •ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•˜ë©°, í–‰ì •ì„œë¹„ìŠ¤ì˜ ì§ˆì„ í–¥ìƒì‹œí‚´ìœ¼ë¡œì¨ íš¡ì„±êµ°ì˜ ê²½ìŸë ¥ ì œê³  ...\n",
      "\n",
      "\n",
      "â–¶ï¸ [Top 2] ìœ ì‚¬ë„: 0.6384\n",
      "ì œ1ì¥ ì—°êµ¬ê°œìš”\n",
      "I. ì—°êµ¬ë°°ê²½ ë° ëª©ì \n",
      "1. ì—°êµ¬ì˜ ë°°ê²½ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ì— ëŒ€í•œ ë§Œì¡±ë„ ì¡°ì‚¬ëŠ” í•œêµ­í–‰ì •ì—°êµ¬ì›ì—ì„œ 1996ë…„ ì¡°ì‚¬ëª¨ë¸ê³¼ ë°©ë²•ì„ ê°œë°œí•œ ì´í›„ í˜„ì¬ê¹Œì§€ ì§€ì†ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³  ìˆëŠ” ì¤‘ì•™ì •ë¶€ ë° ì§€ë°©ì •ë¶€ì˜ ê³¼ì œì´ë‹¤. \n",
      "íš¡ì„±êµ° ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ ë§Œì¡±ë„ì¡°ì‚¬ëŠ” íš¡ì„±êµ° ì¢…í•©ë¯¼ì›ì‹¤ì—ì„œ ì œê³µí•˜ëŠ” ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•œ ì§€ì—­ì£¼ë¯¼ê³¼ ì´ìš©ê°ì„ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬ í–‰ì •ì„œë¹„ìŠ¤ì˜ ì „ë‹¬ì²´ê³„ì™€ ê·¸ ì„œë¹„ìŠ¤ ê³¼ì •ì—ì„œì˜ ê³µë¬´ì›ë“¤ì˜ í–‰íƒœì™€ ë¯¼ì›ì œë„ ë“±ì— ëŒ€í•´ ì²´ê°í•˜ëŠ” ë§Œì¡±ë„ë¥¼ ì¡°ì‚¬í•˜ê³ ì ì‹¤ì‹œë˜ì—ˆë‹¤. \n",
      "íŠ¹íˆ ìƒë°˜ê¸°ì¡°ì‚¬ê²°ê³¼ë¥¼ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬ í•˜ë°˜ê¸° ê³ ê° ë§Œì¡±ë„ì˜ ìˆ˜ì¤€ì„ í‰ê°€í•˜ê³  ë°œì „ì§€í–¥ì ì¸ ê³ ê°ì¤‘ì‹¬ì˜ í–‰ì •ì„œë¹„ìŠ¤ ì²´ì œë¥¼ êµ¬ì¶•í•˜ëŠ”ë° ìˆì–´ ìë£Œë¡œ í™œìš©í•¨ìœ¼ë¡œì¨ íš¡ì„±êµ°ì´ ì œê³µí•˜ëŠ” ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ì˜ ì§ˆì  í–¥ìƒì— ê¸°ì—¬í•˜ê³ ì í•œë‹¤. \n",
      "â–· ì¹œì ˆí•˜ê³  ì¾Œì í•œ ë¯¼ì›ì‹¤\n",
      "â–· ê³ ê°ì¤‘ì‹¬ì˜ ì´ˆì¼ë¥˜ ë¯¼ì› í–‰ì •ê¸°ê´€ìœ¼ë¡œì„œì˜ ìœ„ìƒì •ë¦½\n",
      "â–· ì´ìš©ê³ ê°ì— ëŒ€í•œ ê³¼í•™ì  ë¶„ì„ì„ í† ëŒ€ë¡œ ì¥ê¸°ë°œì „ì „ëµ ìˆ˜ë¦½\n",
      "â–· ê³ ê°ë§Œì¡±ë„ ì œê³ ë¥¼ ìœ„í•œ ê¸°ì´ˆì¡°ì‚¬ì‹¤ì‹œì˜ í•„ìš”ì„± ë¶€ê°\n",
      "â–· ëˆˆë†’ì´ ë¯¼ì› ...\n",
      "\n",
      "\n",
      "â–¶ï¸ [Top 3] ìœ ì‚¬ë„: 0.5579\n",
      "ê¸ˆ ê³ ê°ì¤‘ì‹¬ì˜ í–‰ì •ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•˜ë©°, í–‰ì •ì„œë¹„ìŠ¤ì˜ ì§ˆì„ í–¥ìƒì‹œí‚´ìœ¼ë¡œì¨ íš¡ì„±êµ°ì˜ ê²½ìŸë ¥ ì œê³ ë¥¼ ê¶ê·¹ì ì¸ ëª©í‘œë¡œ í•˜ê³  ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. \n",
      "II. ì—°êµ¬ë²”ìœ„ ë° ë°©ë²•\n",
      "1. ì—°êµ¬ë²”ìœ„\n",
      "â–· ì‹œê°„ì  ë²”ìœ„\n",
      "- 2006. 10. 27 ~ 2006. 11. 30(35ì¼)\n",
      "â–· ê³µê°„ì  ë²”ìœ„\n",
      "- íš¡ì„±êµ° ì¢…í•©ë¯¼ì›ì‹¤\n",
      "â–· ë‚´ìš©ì  ë²”ìœ„\n",
      "- ì„œë¡ (ì—°êµ¬ ê°œìš”ë¡œì„œ ì—°êµ¬ì˜ ë°°ê²½ê³¼ ëª©ì  ë° ë²”ìœ„ ë° ë°©ë²•)\n",
      "- ë¯¼ì› ë§Œì¡±ë„ ì¡°ì‚¬(ë¯¼ì›ì„œë¹„ìŠ¤ ì‹œì„¤, ë¯¼ì›ì—…ë¬´ ë‹´ë‹¹ê³µë¬´ì›ì˜ ì¹œì ˆë„, ë¯¼ì›ì²˜ë¦¬ë‚´ìš© ì¤‘ì‹¬)\n",
      "- ì„¤ë¬¸ê²°ê³¼ ë¶„ì„\n",
      "- ìƒë°˜ê¸° ì¡°ì‚¬ê²°ê³¼ì™€ì˜ ë¹„êµë¶„ì„\n",
      "- ì¢…í•©í‰ê°€ ë° ë°œì „ë°©ì•ˆ\n",
      "2. ì—°êµ¬ë°©ë²•\n",
      "â–· ë¬¸í—Œì—°êµ¬ ë° ìë£Œë¶„ì„(literature study & data analysis)\n",
      "â–· ì„¤ë¬¸ì¡°ì‚¬\n",
      "- ì¡°ì‚¬ìš”ì› : 2ëª…ì˜ ì„¤ë¬¸ì¡°ì‚¬ì›(íˆ¬ì…ì „ ì‚¬ì „êµìœ¡ ìˆ˜ë£Œ)\n",
      "- ì¡°ì‚¬ê¸°ê°„: 2006ë…„ 11ì›” 2ì¼ ~ 2006ë…„ 11ì›” 15ì¼(10ì¼ê°„)\n",
      "- ì¡°ì‚¬ì¥ì†Œ: íš¡ì„±êµ° ì¢…í•©ë¯¼ì›ì‹¤\n",
      "- ì¡°ì‚¬ë°©ë²•: 1:1 ë©´ì ‘ì‹ ì„¤ë¬¸ì¡°ì‚¬ ë° ìê¸°ê¸°ì…ì‹ ì¡°ì‚¬ì‹¤ì‹œ\n",
      "ì œ2ì¥ ì¡°ì‚¬ë¶„ì„\n",
      "I ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# 1. KoSBERT ëª¨ë¸ ë¡œë“œ\n",
    "model = SentenceTransformer('jhgan/ko-sbert-sts')\n",
    "\n",
    "# \n",
    "# 2. JSON íŒŒì¼ ì½ê¸°\n",
    "folder_path = \"./new2\"\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "raw_documents = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)\n",
    "        # ìµœìƒìœ„ì— \"data\" í‚¤ê°€ ìˆê³ , ê·¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ê°€ ìˆë‹¤ê³  ê°€ì •\n",
    "        if \"data\" in data and isinstance(data[\"data\"], list):\n",
    "            for item in data[\"data\"]:\n",
    "                # corpus í•„ë“œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€\n",
    "                if \"corpus\" in item:\n",
    "                    raw_documents.append(item[\"corpus\"])\n",
    "\n",
    "\n",
    "print(f\"âœ… ì´ {len(raw_documents)}ê°œì˜ JSON ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# =========================\n",
    "# 3. FAISS ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "# =========================\n",
    "def create_faiss_index(texts, model):\n",
    "    if not texts:\n",
    "        raise ValueError(\"âŒ\")\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = embeddings.reshape(1, -1)\n",
    "    embeddings = normalize(embeddings, axis=1)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings\n",
    "\n",
    "# =========================\n",
    "# 4. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "# =========================\n",
    "def retrieve_documents(query, index, model, texts, top_k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = normalize(query_embedding, axis=1)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [(texts[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# =========================\n",
    "# 5. ì²­í‚¹ ë° ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜\n",
    "# =========================\n",
    "def process_chunking_and_retrieval(chunk_size, chunk_overlap, texts, query, model):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\",  # ì¤„ë°”ê¿ˆ ë‹¨ìœ„ê°€ ì•„ë‹Œ ë¬¸ì ë‹¨ìœ„ ì²­í¬\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    joined_text = \"\\n\".join(texts)\n",
    "    chunked_texts = text_splitter.split_text(joined_text)\n",
    "\n",
    "    if not chunked_texts:\n",
    "        raise ValueError(\"âŒ ì²­í‚¹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    index, _ = create_faiss_index(chunked_texts, model)\n",
    "    retrieved = retrieve_documents(query, index, model, chunked_texts, top_k=3)\n",
    "    return retrieved\n",
    "\n",
    "# =========================\n",
    "# 6. ì§ˆì˜ ë° ì²­í‚¹ ì„¸íŠ¸ ì •ì˜\n",
    "# =========================\n",
    "query = \"íš¡ì„±êµ° ì¢…í•©ë¯¼ì›ì‹¤ì˜ ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ ì¡°ì‚¬ëŠ” ë©°ì¹ ì„ ì„ ì •í•˜ì—¬ ì‹¤ì‹œí•˜ì˜€ì–´?\"\n",
    "chunking_configs = [\n",
    "    (500, 50)\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 7. ì‹¤í–‰\n",
    "# =========================\n",
    "for chunk_size, chunk_overlap in chunking_configs:\n",
    "    print(f\"\\n===============================\")\n",
    "    print(f\"ğŸ” ì²­í‚¹ ì„¤ì •: {chunk_size}+{chunk_overlap}\")\n",
    "    print(f\"===============================\")\n",
    "    try:\n",
    "        results = process_chunking_and_retrieval(chunk_size, chunk_overlap, raw_documents, query, model)\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"\\nâ–¶ï¸ [Top {i+1}] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "            print(doc[:700], \"...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce083afa-efd6-459a-b390-a00e2df88d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 10ê°œì˜ JSON ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "===============================\n",
      "ğŸ” MultiVectorRetriever: ì²­í‚¹ 500+50\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# =========================\n",
    "# 1. ëª¨ë¸ ë¡œë“œ\n",
    "# =========================\n",
    "model_name = 'jhgan/ko-sbert-sts'\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# =========================\n",
    "# 2. JSON ë¬¸ì„œ ë¡œë“œ\n",
    "# =========================\n",
    "folder_path = \"./new2\"\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "raw_documents = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"data\" in data and isinstance(data[\"data\"], list):\n",
    "            for item in data[\"data\"]:\n",
    "                if \"corpus\" in item:\n",
    "                    raw_documents.append(item[\"corpus\"])\n",
    "\n",
    "print(f\"âœ… ì´ {len(raw_documents)}ê°œì˜ JSON ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# =========================\n",
    "# 3. ì²­í‚¹ í•¨ìˆ˜\n",
    "# =========================\n",
    "def chunk_documents(texts, chunk_size, chunk_overlap):\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\",  # ë¬¸ì ë‹¨ìœ„ ì²­í‚¹\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    full_text = \"\\n\".join(texts)\n",
    "    chunks = splitter.split_text(full_text)\n",
    "\n",
    "    documents = [\n",
    "        Document(page_content=chunk, metadata={\"doc_id\": f\"doc_{i}\"})\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "# =========================\n",
    "# 4. FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "# =========================\n",
    "def create_langchain_faiss(documents, embedding_model):\n",
    "    return FAISS.from_documents(documents, embedding=embedding_model)\n",
    "\n",
    "# =========================\n",
    "# 5. MultiVectorRetriever ì‹¤í–‰ í•¨ìˆ˜ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)\n",
    "# =========================\n",
    "def run_multivector_retrieval(chunk_size, chunk_overlap, query, texts):\n",
    "    print(f\"\\n===============================\")\n",
    "    print(f\"ğŸ” MultiVectorRetriever: ì²­í‚¹ {chunk_size}+{chunk_overlap}\")\n",
    "    print(f\"===============================\")\n",
    "\n",
    "    documents = chunk_documents(texts, chunk_size, chunk_overlap)\n",
    "    if not documents:\n",
    "        print(\"âŒ ì²­í‚¹ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 1) FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "    vectorstore = create_langchain_faiss(documents, embedding_model)\n",
    "\n",
    "    # 2) InMemory ë¬¸ì„œ ì €ì¥ì†Œ\n",
    "    docstore = InMemoryStore()\n",
    "    for doc in documents:\n",
    "        doc_id = doc.metadata[\"doc_id\"]\n",
    "        docstore.mset([(doc_id, doc)])\n",
    "\n",
    "    # 3) MultiVectorRetriever êµ¬ì„±\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=\"doc_id\"\n",
    "    )\n",
    "\n",
    "    # -- A. MultiVectorRetrieverë¡œ ìƒìœ„ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ì ìˆ˜ ì—†ìŒ)\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # -- B. FAISSì—ì„œ ì§ì ‘ ìœ ì‚¬ë„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    #     (vectorstore.similarity_search_with_score)\n",
    "    #     k=3ìœ¼ë¡œ ì ìˆ˜ì™€ í•¨ê»˜ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "    results_with_score = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    if not results_with_score:\n",
    "        print(\"âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    print(\"\\nğŸ” [ìœ ì‚¬ë„ ì ìˆ˜ ì§ì ‘ í™•ì¸ - vectorstore ê¸°ì¤€ (Top 3)]\")\n",
    "    for i, (doc_, score) in enumerate(results_with_score):\n",
    "        print(f\"\\nâ–¶ï¸ [Top {i+1}] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "        print(doc_.page_content[:300], \"...\\n\")\n",
    "\n",
    "# =========================\n",
    "# 6. ì‹¤í–‰\n",
    "# =========================\n",
    "query = \"íš¡ì„±êµ° ì¢…í•©ë¯¼ì›ì‹¤ì˜ ë¯¼ì›í–‰ì •ì„œë¹„ìŠ¤ ì¡°ì‚¬ëŠ” ë©°ì¹ ì„ ì„ ì •í•˜ì—¬ ì‹¤ì‹œí•˜ì˜€ì–´?\"\n",
    "chunking_configs = [\n",
    "    (500, 50)\n",
    "]\n",
    "\n",
    "for chunk_size, chunk_overlap in chunking_configs:\n",
    "    run_multivector_retrieval(chunk_size, chunk_overlap, query, raw_documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
