{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11259510,"sourceType":"datasetVersion","datasetId":7037079}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain-experimental","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:06:13.576185Z","iopub.execute_input":"2025-04-04T06:06:13.576385Z","iopub.status.idle":"2025-04-04T06:06:24.268445Z","shell.execute_reply.started":"2025-04-04T06:06:13.576363Z","shell.execute_reply":"2025-04-04T06:06:24.267635Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-experimental\n  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\nCollecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental)\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-core<0.4.0,>=0.3.28 (from langchain-experimental)\n  Downloading langchain_core-0.3.50-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.21 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.26.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.12.2)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.0a2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain<1.0.0,>=0.3.21->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.29.0)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.2.2)\nDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.50-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.4/423.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.22-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, httpx-sse, async-timeout, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain-community, langchain-experimental\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3 httpx-sse-0.4.0 langchain-0.3.22 langchain-community-0.3.20 langchain-core-0.3.50 langchain-experimental-0.3.4 langchain-text-splitters-0.3.7 pydantic-settings-2.8.1 python-dotenv-1.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# ... (라이브러리 임포트 - 이전과 동일) ...\nimport os, glob, json, hashlib, torch\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nimport pandas as pd\n\n# --- 1. 설정 (Configuration) ---\n# ... (INPUT_DIR, OUTPUT_DIR, MODEL_NAME, BATCH_SIZE 등 설정 - 이전과 동일) ...\nINPUT_DIR = '/kaggle/input/predata/'\nOUTPUT_DIR = '/kaggle/working/output'\nOUTPUT_EMBEDDING_FILE = os.path.join(OUTPUT_DIR, 'chunk_embeddings.jsonl')\nMODEL_NAME = 'jhgan/ko-sbert-sts'\nBATCH_SIZE = 32\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"입력 디렉토리: {INPUT_DIR}\")\nprint(f\"출력 디렉토리: {OUTPUT_DIR}\")\nprint(f\"출력 파일: {OUTPUT_EMBEDDING_FILE}\")\n\n# --- 2. 모델 및 환경 설정 ---\n# ... (GPU 설정, 모델 로딩, Splitter 생성 - 이전과 동일, 오류 처리 포함) ...\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"사용 장치: {device}\")\nembedding_model = None\ntext_splitter = None\ntry:\n    print(f\"'{MODEL_NAME}' 임베딩 모델 로딩 시작...\")\n    embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n    print(\"임베딩 모델 로딩 완료.\")\n    langchain_embeddings = HuggingFaceEmbeddings(\n        model_name=MODEL_NAME, model_kwargs={'device': device}, encode_kwargs={'normalize_embeddings': False}\n    )\n    print(\"LangChain Embeddings 래퍼 생성 완료.\")\n    text_splitter = SemanticChunker(langchain_embeddings, breakpoint_threshold_type=\"percentile\")\n    print(\"SemanticChunker 생성 완료.\")\nexcept Exception as e:\n    print(f\"모델 또는 Splitter 초기화 중 오류 발생: {e}\")\n\n# --- 3. 이어하기 기능: 기존 데이터 로드 (ID와 해시 포함) --- ### 수정됨 ###\n\nexisting_data_dict = {} # 기존 데이터를 저장할 딕셔너리 {chunk_unique_id: data_dict}\nif os.path.exists(OUTPUT_EMBEDDING_FILE):\n    print(f\"기존 출력 파일 '{OUTPUT_EMBEDDING_FILE}'을 읽어옵니다...\")\n    try:\n        with open(OUTPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for line in f_in:\n                try:\n                    data = json.loads(line)\n                    note_id = data.get('note_id')\n                    chunk_id = data.get('chunk_id')\n                    # text_hash와 vector가 있는지 확인 (이전 버전 파일 호환성)\n                    text_hash = data.get('text_hash')\n                    vector = data.get('vector')\n                    if note_id is not None and chunk_id is not None and text_hash and vector:\n                        chunk_unique_id = f\"{note_id}_{chunk_id}\"\n                        existing_data_dict[chunk_unique_id] = data # 전체 데이터 저장\n                    # else:\n                    #     print(f\"경고: 기존 데이터에 필요한 필드(text_hash, vector 등)가 누락되었습니다. 해당 라인 건너뛰기: {line.strip()}\")\n                except json.JSONDecodeError:\n                    print(f\"경고: 출력 파일의 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n        print(f\"총 {len(existing_data_dict)}개의 기존 청크 데이터를 로드했습니다.\")\n    except Exception as e:\n        print(f\"경고: 기존 출력 파일을 읽는 중 오류 발생: {e}. 처음부터 다시 처리합니다.\")\n        existing_data_dict = {} # 오류 시 초기화\n\n# --- 4. 데이터 처리 및 임베딩 대상 선정 --- ### 수정됨 ###\n\nif embedding_model and text_splitter:\n    print(\"\\n--- 청크 분할 및 변경 사항 확인 시작 ---\")\n\n    chunks_to_embed = [] # 새로 임베딩해야 할 청크 데이터 리스트\n    processed_this_run = {} # 현재 실행에서 유효한 데이터 저장 (기존+신규+업데이트)\n\n    input_files = glob.glob(os.path.join(INPUT_DIR, '*.json'))\n    print(f\"총 {len(input_files)}개의 JSON 파일을 찾았습니다.\")\n\n    for filepath in tqdm(input_files, desc=\"파일 읽기 및 청킹/비교 중\"):\n        display_filepath = os.path.basename(filepath)\n        try:\n            with open(filepath, 'r', encoding='utf-8-sig') as f_in: content = json.load(f_in)\n\n            if 'data' in content and isinstance(content['data'], list):\n                for doc_data in content['data']:\n                    note_id = doc_data.get('source_id'); corpus = doc_data.get('corpus')\n                    doc_title = doc_data.get('title', '')\n                    if not note_id or not corpus or not isinstance(corpus, str): continue\n\n                    current_chunk_id_counter = 0\n                    try:\n                        chunks = text_splitter.split_text(corpus)\n                    except Exception as split_err:\n                        print(f\"오류: {display_filepath} (ID: {note_id}) 청킹 중 오류: {split_err}\")\n                        continue\n\n                    for chunk_text in chunks:\n                        if not chunk_text.strip(): continue\n                        chunk_id = current_chunk_id_counter\n                        chunk_unique_id = f\"{note_id}_{chunk_id}\"\n                        current_hash = hashlib.sha256(chunk_text.strip().encode('utf-8')).hexdigest()\n\n                        # *** 이어하기 + 변경 감지 로직 ***\n                        process_this_chunk = False\n                        previous_data = existing_data_dict.get(chunk_unique_id)\n\n                        if previous_data is None: # 신규 청크\n                            process_this_chunk = True\n                        elif previous_data.get('text_hash') != current_hash: # 내용 변경됨\n                            # print(f\"정보: 내용 변경 감지, 재처리: {chunk_unique_id}\") # 로그 (선택적)\n                            process_this_chunk = True\n\n                        # 현재 실행에서 유효한 데이터 구성\n                        current_chunk_data = {\n                            \"note_id\": note_id, \"chunk_id\": chunk_id,\n                            \"doc_title\": doc_title, \"filepath\": filepath,\n                            \"text\": chunk_text.strip(), \"text_hash\": current_hash\n                            # 'vector'는 나중에 추가됨\n                        }\n\n                        if process_this_chunk:\n                            chunks_to_embed.append(current_chunk_data) # 임베딩 대상 리스트에 추가\n                        else:\n                            # 변경 없는 경우, 기존 벡터 사용\n                            current_chunk_data['vector'] = previous_data.get('vector')\n\n                        # 현재 실행의 최종 데이터 딕셔너리에 저장/업데이트\n                        processed_this_run[chunk_unique_id] = current_chunk_data\n                        current_chunk_id_counter += 1\n\n            else: print(f\"경고: 파일 {display_filepath} 건너뛰기 ('data' 리스트 없음).\")\n        except json.JSONDecodeError as json_err: print(f\"오류: 파일 {display_filepath} JSON 구조 오류: {json_err}\")\n        except Exception as file_err: print(f\"파일 {display_filepath} 처리 중 오류: {file_err}\")\n\n    # --- 5. 배치 임베딩 (새 대상만) ---\n    newly_processed_count = len(chunks_to_embed)\n    print(f\"\\n총 {newly_processed_count}개의 신규/변경된 청크에 대해 임베딩을 시작합니다.\")\n\n    if newly_processed_count > 0:\n        for i in tqdm(range(0, newly_processed_count, BATCH_SIZE), desc=\"임베딩 중\"):\n            batch_data_to_embed = chunks_to_embed[i : i + BATCH_SIZE]\n            batch_texts = [item['text'] for item in batch_data_to_embed]\n            if not batch_texts: continue\n\n            try:\n                batch_embeddings = embedding_model.encode(\n                    batch_texts, convert_to_numpy=True, show_progress_bar=False, batch_size=len(batch_texts)\n                )\n                batch_embeddings_list = batch_embeddings.tolist()\n\n                # 계산된 벡터를 processed_this_run 딕셔너리에 업데이트\n                for j, embedding_vector in enumerate(batch_embeddings_list):\n                    # 임베딩 대상 리스트의 원본 딕셔너리를 직접 수정하면 안됨\n                    # processed_this_run 에서 해당 청크를 찾아 업데이트\n                    original_chunk_data = batch_data_to_embed[j]\n                    chunk_unique_id = f\"{original_chunk_data['note_id']}_{original_chunk_data['chunk_id']}\"\n                    if chunk_unique_id in processed_this_run:\n                         processed_this_run[chunk_unique_id]['vector'] = embedding_vector\n                    # else: # 이론상 이 경우는 없어야 함\n                    #     print(f\"경고: 임베딩된 청크 {chunk_unique_id}를 processed_this_run에서 찾을 수 없습니다.\")\n\n            except Exception as embed_err:\n                print(f\"오류: 배치 {i // BATCH_SIZE} 임베딩 중 오류: {embed_err}\")\n    else:\n        print(\"새로 처리할 청크가 없습니다.\")\n\n    # --- 6. 최종 결과 파일 저장 (전체 덮어쓰기) --- ### 수정됨 ###\n    print(f\"\\n--- 최종 결과 파일 저장 시작 ({len(processed_this_run)}개 청크) ---\")\n    # 출력 파일을 쓰기 모드('w')로 열어 전체 내용을 새로 씀\n    try:\n        with open(OUTPUT_EMBEDDING_FILE, 'w', encoding='utf-8') as f_out:\n            # 정렬 기준 설정 (note_id, chunk_id 순서) - 선택 사항이지만 일관성에 좋음\n            sorted_chunk_ids = sorted(processed_this_run.keys(), key=lambda x: (x.split('_')[0], int(x.split('_')[1])))\n\n            for chunk_unique_id in tqdm(sorted_chunk_ids, desc=\"최종 파일 저장 중\"):\n                final_chunk_data = processed_this_run[chunk_unique_id]\n                # 벡터 데이터가 있는지 최종 확인 (임베딩 오류 등으로 누락될 수 있음)\n                if 'vector' in final_chunk_data and final_chunk_data['vector']:\n                    json_string = json.dumps(final_chunk_data, ensure_ascii=False)\n                    f_out.write(json_string + '\\n')\n                else:\n                    print(f\"경고: 청크 {chunk_unique_id}의 벡터 데이터가 없어 최종 파일에 저장하지 않습니다.\")\n        print(f\"최종 결과가 '{OUTPUT_EMBEDDING_FILE}'에 저장되었습니다.\")\n    except Exception as write_err:\n        print(f\"오류: 최종 결과 파일 '{OUTPUT_EMBEDDING_FILE}' 저장 중 오류 발생: {write_err}\")\n\n\n    print(\"\\n--- 임베딩 생성 및 저장 완료 ---\")\n    print(\"-\" * 40)\n\nelse:\n    print(\"\\n오류: 모델 또는 텍스트 스플리터가 제대로 초기화되지 않아 임베딩 프로세스를 시작할 수 없습니다.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:06:24.270041Z","iopub.execute_input":"2025-04-04T06:06:24.270370Z","iopub.status.idle":"2025-04-04T06:07:02.197930Z","shell.execute_reply.started":"2025-04-04T06:06:24.270322Z","shell.execute_reply":"2025-04-04T06:07:02.196956Z"}},"outputs":[{"name":"stdout","text":"입력 디렉토리: /kaggle/input/predata/\n출력 디렉토리: /kaggle/working/output\n출력 파일: /kaggle/working/output/chunk_embeddings.jsonl\n사용 장치: cuda\n'jhgan/ko-sbert-sts' 임베딩 모델 로딩 시작...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518cfce7130348f3934bfe14907a335e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a7c71e6515440cac1ac6ff792f1900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ecc57c68bee4a339b3f5fc9721593e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b91f653d8a4c1da400361dd084ff29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93a78a6ebc834c26b886b5c3e815933d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c38e910bfb4608ad03c351087a771c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3391d907d65a4085bc2a7f3178a74958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7035eabd3fa7464dba46bcbbd19c3878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e230d614dfe44d8eb8e36e8cd2e396c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3bb10da117f46068fe35c1d5f45fabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e75c21b5f2e40288a8102963fd84c99"}},"metadata":{}},{"name":"stdout","text":"임베딩 모델 로딩 완료.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-5b2e683e7b5f>:32: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  langchain_embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":"LangChain Embeddings 래퍼 생성 완료.\nSemanticChunker 생성 완료.\n기존 출력 파일 '/kaggle/working/output/chunk_embeddings.jsonl'을 읽어옵니다...\n총 130개의 기존 청크 데이터를 로드했습니다.\n\n--- 청크 분할 및 변경 사항 확인 시작 ---\n총 10개의 JSON 파일을 찾았습니다.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"파일 읽기 및 청킹/비교 중:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c7ccde3f334211a3dfa6041b02526b"}},"metadata":{}},{"name":"stdout","text":"\n총 0개의 신규/변경된 청크에 대해 임베딩을 시작합니다.\n새로 처리할 청크가 없습니다.\n\n--- 최종 결과 파일 저장 시작 (130개 청크) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"최종 파일 저장 중:   0%|          | 0/130 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408f9d23d11a40c19552ee8279874992"}},"metadata":{}},{"name":"stdout","text":"최종 결과가 '/kaggle/working/output/chunk_embeddings.jsonl'에 저장되었습니다.\n\n--- 임베딩 생성 및 저장 완료 ---\n----------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\nimport networkx as nx # 그래프 기반 그룹핑을 위해 사용 (설치 필요 시 !pip install networkx)\nfrom tqdm.notebook import tqdm\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_EMBEDDING_FILE = '/kaggle/working/output/chunk_embeddings.jsonl' # 임베딩 데이터 파일\nOUTPUT_GROUP_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl' # 그룹핑 결과 파일\nSIMILARITY_THRESHOLD = 0.7  # 유사도 임계값 (실험적으로 조절 필요)\nMIN_GROUP_SIZE = 2          # 그룹으로 간주할 최소 청크 수 (예: 2개 이상 유사해야 그룹)\n\nprint(f\"입력 임베딩 파일: {INPUT_EMBEDDING_FILE}\")\nprint(f\"출력 그룹 파일: {OUTPUT_GROUP_FILE}\")\nprint(f\"유사도 임계값: {SIMILARITY_THRESHOLD}\")\nprint(f\"최소 그룹 크기: {MIN_GROUP_SIZE}\")\n\n# --- 2. 데이터 로드 ---\n\nall_chunk_data = [] # 청크 데이터 (메타데이터 + 벡터) 저장 리스트\nchunk_embeddings = [] # 벡터만 따로 저장할 리스트 (유사도 계산용)\nchunk_lookup = {}     # 인덱스 -> 청크 메타데이터 매핑용 딕셔너리\n\nprint(f\"\\n--- 임베딩 데이터 로딩 시작 ---\")\nif not os.path.exists(INPUT_EMBEDDING_FILE):\n    print(f\"오류: 임베딩 파일 '{INPUT_EMBEDDING_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(INPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for i, line in enumerate(f_in):\n                try:\n                    data = json.loads(line)\n                    # 필수 데이터 확인\n                    if 'note_id' in data and 'chunk_id' in data and 'vector' in data and 'text' in data:\n                        vector = np.array(data['vector'], dtype=np.float32) # numpy 배열로 변환\n                        # 벡터 차원 일관성 확인 (선택적)\n                        if i > 0 and vector.shape[0] != chunk_embeddings[0].shape[0]:\n                             print(f\"경고: {i}번째 청크 벡터 차원({vector.shape[0]})이 이전 벡터 차원({chunk_embeddings[0].shape[0]})과 다릅니다. 건너<0xEB><0x9B><0x84>니다.\")\n                             continue\n\n                        all_chunk_data.append(data) # 전체 데이터 저장\n                        chunk_embeddings.append(vector) # 벡터만 따로 저장\n                        chunk_lookup[i] = data # 인덱스로 메타데이터 조회 가능하도록\n                    else:\n                        print(f\"경고: {i}번째 라인에 필수 필드가 누락되어 건너<0xEB><0x9B><0x84>니다.\")\n                except json.JSONDecodeError:\n                    print(f\"경고: 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n                except Exception as parse_err:\n                     print(f\"경고: 데이터 처리 중 오류 발생 ({i}번째 라인): {parse_err}\")\n\n        if not all_chunk_data or not chunk_embeddings:\n             print(\"오류: 유효한 임베딩 데이터를 로드하지 못했습니다.\")\n             chunk_embeddings = None # 이후 처리 방지\n        else:\n            chunk_embeddings = np.array(chunk_embeddings) # 최종적으로 numpy 배열로 변환\n            print(f\"총 {len(all_chunk_data)}개의 청크 데이터 및 임베딩 로드 완료.\")\n            print(f\"임베딩 배열 형태: {chunk_embeddings.shape}\")\n\n    except Exception as e:\n        print(f\"임베딩 파일 로딩 중 오류 발생: {e}\")\n        chunk_embeddings = None\n\n# --- 3. 유사도 계산 및 그룹핑 ---\n\nif chunk_embeddings is not None and len(chunk_embeddings) >= MIN_GROUP_SIZE:\n    print(f\"\\n--- 유사도 계산 및 그룹핑 시작 (임계값: {SIMILARITY_THRESHOLD}) ---\")\n\n    # 코사인 유사도 계산 (모든 쌍)\n    print(\"코사인 유사도 행렬 계산 중...\")\n    # 벡터 정규화 (L2 norm) - cosine 유사도는 내적(dot product)으로 계산 가능\n    # SentenceTransformer 모델이 이미 정규화된 벡터를 반환할 수도 있지만, 안전하게 다시 정규화\n    # from sklearn.preprocessing import normalize\n    # embeddings_normalized = normalize(chunk_embeddings, axis=1, norm='l2')\n    # similarity_matrix = np.dot(embeddings_normalized, embeddings_normalized.T)\n    # 또는 sklearn 함수 직접 사용 (내부적으로 정규화 처리 가능성 있음)\n    similarity_matrix = cosine_similarity(chunk_embeddings)\n    print(\"유사도 행렬 계산 완료.\")\n\n    # 임계값 이상인 쌍들을 엣지로 하는 그래프 생성\n    print(\"유사도 기반 그래프 생성 중...\")\n    graph = nx.Graph()\n    num_chunks = len(all_chunk_data)\n    # 자기 자신과의 유사도는 제외하고, 임계값 이상인 엣지만 추가\n    for i in tqdm(range(num_chunks), desc=\"엣지 추가 중\"):\n        graph.add_node(i) # 모든 청크를 노드로 추가\n        for j in range(i + 1, num_chunks): # 중복 계산 피하기 위해 i+1 부터 시작\n            if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD:\n                graph.add_edge(i, j, weight=similarity_matrix[i, j]) # 엣지 추가 (weight는 유사도)\n\n    # 연결된 컴포넌트(그룹) 찾기\n    print(\"연결된 컴포넌트(그룹) 찾는 중...\")\n    connected_components = list(nx.connected_components(graph))\n\n    # 최소 크기 기준을 만족하는 그룹만 필터링\n    valid_groups = [group for group in connected_components if len(group) >= MIN_GROUP_SIZE]\n    print(f\"총 {len(valid_groups)}개의 유효한 그룹 (크기 >= {MIN_GROUP_SIZE})을 찾았습니다.\")\n\n    # --- 4. 그룹 정보 및 액션 대상 생성/저장 ---\n    print(f\"\\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\")\n    grouped_results = []\n    group_id_counter = 0\n\n    for group_indices in tqdm(valid_groups, desc=\"그룹 정보 생성 중\"):\n        group_data = {\n            \"group_id\": group_id_counter,\n            \"similarity_threshold\": SIMILARITY_THRESHOLD,\n            \"member_chunks\": [],\n            \"synthesis_input_texts\": [],\n            \"backlink_candidate_notes\": set() # 중복 제거 위해 Set 사용\n        }\n\n        # 그룹 멤버 정보 추가\n        member_texts = []\n        note_ids_in_group = set()\n        for index in group_indices:\n            chunk_info = chunk_lookup.get(index)\n            if chunk_info:\n                # 그룹 멤버 정보 구성 (필요한 메타데이터 추가)\n                member_chunk_info = {\n                    \"note_id\": chunk_info.get(\"note_id\"),\n                    \"chunk_id\": chunk_info.get(\"chunk_id\"),\n                    \"doc_title\": chunk_info.get(\"doc_title\", \"\"),\n                    \"filepath\": chunk_info.get(\"filepath\", \"\"),\n                    \"text_preview\": chunk_info.get(\"text\", \"\")[:100] + \"...\" # 텍스트 미리보기\n                    # 필요시 그룹 내 다른 멤버와의 평균/최대 유사도 등 추가 가능\n                }\n                group_data[\"member_chunks\"].append(member_chunk_info)\n                member_texts.append(chunk_info.get(\"text\", \"\")) # 통합용 텍스트\n                note_ids_in_group.add(chunk_info.get(\"note_id\")) # 백링크용 노트 ID\n\n        group_data[\"synthesis_input_texts\"] = member_texts\n        # Set을 리스트로 변환하여 저장\n        group_data[\"backlink_candidate_notes\"] = sorted(list(note_ids_in_group))\n\n        grouped_results.append(group_data)\n        group_id_counter += 1\n\n    # 최종 결과를 JSON Lines 파일로 저장\n    try:\n        with open(OUTPUT_GROUP_FILE, 'w', encoding='utf-8') as f_out:\n            for group_result in grouped_results:\n                json_string = json.dumps(group_result, ensure_ascii=False)\n                f_out.write(json_string + '\\n')\n        print(f\"그룹핑 결과가 '{OUTPUT_GROUP_FILE}'에 저장되었습니다.\")\n    except Exception as write_err:\n        print(f\"오류: 그룹핑 결과 파일 저장 중 오류 발생: {write_err}\")\n\n    print(\"\\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\")\n    print(\"-\" * 40)\n\nelif len(chunk_embeddings) < MIN_GROUP_SIZE:\n     print(f\"\\n오류: 로드된 청크 수가 최소 그룹 크기({MIN_GROUP_SIZE})보다 작아 그룹핑을 수행할 수 없습니다.\")\nelse:\n    print(\"\\n오류: 임베딩 데이터가 로드되지 않아 그룹핑 프로세스를 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:07:02.199442Z","iopub.execute_input":"2025-04-04T06:07:02.199694Z","iopub.status.idle":"2025-04-04T06:07:02.495176Z","shell.execute_reply.started":"2025-04-04T06:07:02.199673Z","shell.execute_reply":"2025-04-04T06:07:02.494153Z"}},"outputs":[{"name":"stdout","text":"입력 임베딩 파일: /kaggle/working/output/chunk_embeddings.jsonl\n출력 그룹 파일: /kaggle/working/output/grouped_chunks_info.jsonl\n유사도 임계값: 0.7\n최소 그룹 크기: 2\n\n--- 임베딩 데이터 로딩 시작 ---\n총 130개의 청크 데이터 및 임베딩 로드 완료.\n임베딩 배열 형태: (130, 768)\n\n--- 유사도 계산 및 그룹핑 시작 (임계값: 0.7) ---\n코사인 유사도 행렬 계산 중...\n유사도 행렬 계산 완료.\n유사도 기반 그래프 생성 중...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"엣지 추가 중:   0%|          | 0/130 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95ba8fa8a18485e97070d6e6fce75ae"}},"metadata":{}},{"name":"stdout","text":"연결된 컴포넌트(그룹) 찾는 중...\n총 15개의 유효한 그룹 (크기 >= 2)을 찾았습니다.\n\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"그룹 정보 생성 중:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78e50d20bf240538a26266849880baf"}},"metadata":{}},{"name":"stdout","text":"그룹핑 결과가 '/kaggle/working/output/grouped_chunks_info.jsonl'에 저장되었습니다.\n\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\n----------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport json\nimport os\n\n# 그룹핑 결과 파일 경로\nGROUP_INFO_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl'\n\nprint(f\"--- 그룹핑 결과 확인 ({GROUP_INFO_FILE}) ---\")\n\nif not os.path.exists(GROUP_INFO_FILE):\n    print(f\"오류: 그룹핑 결과 파일 '{GROUP_INFO_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(GROUP_INFO_FILE, 'r', encoding='utf-8') as f_in:\n            group_found = False\n            for line in f_in:\n                group_found = True\n                try:\n                    group_data = json.loads(line)\n                    group_id = group_data.get(\"group_id\", \"N/A\")\n                    threshold = group_data.get(\"similarity_threshold\", \"N/A\")\n                    member_chunks = group_data.get(\"member_chunks\", [])\n                    backlink_notes = group_data.get(\"backlink_candidate_notes\", [])\n\n                    print(f\"\\n===== 그룹 ID: {group_id} (임계값: {threshold}) =====\")\n                    print(f\"포함된 청크 수: {len(member_chunks)}\")\n                    print(f\"백링크 후보 노트 ID: {backlink_notes}\")\n\n                    print(\"\\n--- 포함된 청크 목록 (미리보기) ---\")\n                    if not member_chunks:\n                        print(\"  (포함된 청크 정보 없음)\")\n                    else:\n                        for i, chunk_info in enumerate(member_chunks):\n                            note_id = chunk_info.get(\"note_id\", \"?\")\n                            chunk_id = chunk_info.get(\"chunk_id\", \"?\")\n                            title = chunk_info.get(\"doc_title\", \"제목 없음\")\n                            preview = chunk_info.get(\"text_preview\", \"내용 없음\")\n                            print(f\"  {i+1}. [노트:{note_id} / 청크:{chunk_id}] (제목: {title})\")\n                            print(f\"     내용: {preview}\")\n\n                    # 통합용 텍스트는 너무 길 수 있으니 필요한 경우 별도 확인\n                    # synthesis_texts = group_data.get(\"synthesis_input_texts\", [])\n                    # print(\"\\n--- 통합 대상 텍스트 목록 ---\")\n                    # for i, text in enumerate(synthesis_texts):\n                    #     print(f\"  {i+1}. {text[:150]}...\") # 일부만 출력\n\n                    print(\"=\" * (len(str(group_id)) + 20)) # 구분선\n\n                except json.JSONDecodeError:\n                    print(f\"\\n오류: 잘못된 JSON 라인 발견 - {line.strip()}\")\n                except Exception as parse_err:\n                    print(f\"\\n오류: 그룹 데이터 처리 중 오류 발생 - {parse_err}\")\n\n            if not group_found:\n                print(\"결과 파일은 존재하지만, 유효한 그룹 정보를 찾지 못했습니다.\")\n\n    except Exception as e:\n        print(f\"그룹핑 결과 파일 읽기 중 오류 발생: {e}\")\n\nprint(\"\\n--- 그룹핑 결과 확인 완료 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:07:02.496246Z","iopub.execute_input":"2025-04-04T06:07:02.497286Z","iopub.status.idle":"2025-04-04T06:07:02.534841Z","shell.execute_reply.started":"2025-04-04T06:07:02.497252Z","shell.execute_reply":"2025-04-04T06:07:02.534182Z"}},"outputs":[{"name":"stdout","text":"--- 그룹핑 결과 확인 (/kaggle/working/output/grouped_chunks_info.jsonl) ---\n\n===== 그룹 ID: 0 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000105']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:1] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 연구배경 및 목적\n1. 연구의 배경민원행정서비스에 대한 만족도 조사는 한국행정연구원에서 1996년 조사모델과 방법을 개발한 이후 현재까지 지속적으로 수행하고 있는 중앙정부 및 지방...\n  2. [노트:S0000105 / 청크:3] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 11. 30(35일)\n▷ 공간적 범위\n- 횡성군 종합민원실\n▷ 내용적 범위\n- 서론(연구 개요로서 연구의 배경과 목적 및 범위 및 방법)\n- 민원 만족도 조사(민원서비스 시설, 민...\n=====================\n\n===== 그룹 ID: 1 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000105']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:8] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 6. 분산분석(ANOVA; analysis of variance)결과\n가. 연령별 분산분석\n1)연령별 민원시설관련 분산분석응답자들의 연령대별 평균차이를 비교하여 통계적으로 유의한 ...\n  2. [노트:S0000105 / 청크:9] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 나. 민원분야에 따른 분산분석\n1)민원시설관련 만족도 분산분석응답자들이 제공받은 민원서비스 분야별 접근성에 대한 만족도는 평균 72.70점으로, 주차장과 관련한 만족도는 61.85...\n  3. [노트:S0000105 / 청크:10] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 종합 평가\n1. 민원행정 시설에 대한 만족도\n▷ 민원행정 시설에 대한 종합만족지수는 70.33점으로 상반기에 비해 4.32점이 높아졌음. ▷ 접근성에 대한 만족도는 71.14점으로...\n=====================\n\n===== 그룹 ID: 2 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000286']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000286 / 청크:7] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 인증기관에서 발급한다. 3.8 인코딩(Encoding)\n기계적 처리를 위해 고안된 구문 또는 신호를 특정한 부호들의 나열로 그 형태를 바꾸는 것\n[정보통신용어사전, KS X ISO...\n  2. [노트:S0000286 / 청크:0] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 전자기록물 장기보존패키지 - 제2부: 디렉토리로 구조화된 방식(NEO3)\n1. 적용범위\n이 표준은 디지털객체에 대한 논리적 또는 물리적 캡슐화 규격을 규정하는 동시에 진본성, 무결...\n  3. [노트:S0000286 / 청크:10] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 영구기록물관리기관 등은 디렉토리로 구조화된 방식을 채택하는 경우라 하더라도 기관의 환경 등에 따라 NEO3가 아닌 다른 캡슐화 방식을 정의하고 적용할 수 있다. 5. 디렉토리로 구...\n=====================\n\n===== 그룹 ID: 3 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000337']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000337 / 청크:11] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 〈표3. 2 _ 2 4 ＞ 는 구직 희망자 재취업 교육훈련프로그램 필요여부를 살펴보았다. 분석결과 •필 요하다\" 가 3 0. 5 %로 가장 높은 비율로 나타났고 •매우필요하다• 3...\n  2. [노트:S0000337 / 청크:12] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 서비스직” 1 0. 3 %, ••임시직. 단순노무직'1 0. 3 % 순으로 나타 났다. 〈표3.2-26〉는 구직 형식을 살펴보았다. 분석결과 *자영업'아 43.1%로 가장 높은 비...\n=====================\n\n===== 그룹 ID: 4 (임계값: 0.7) =====\n포함된 청크 수: 4\n백링크 후보 노트 ID: ['S0000473']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000473 / 청크:0] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 『최저임금 적용효과에 관한 실태조사』품질개선 컨설팅 최종결과보고서 \n요약\n「최저임금 적용효과에 관한 실태조사」는 최저임금이 기업경영과 고용에 미치는 효과, 사용자와 근로자가 실제 ...\n  2. [노트:S0000473 / 청크:1] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 최저임금 등 임금분포와 조사대상\n○ 최저임금과 비교되는 시급 임금분포와 최저임금의 중위임금에 대비한 상대적 수준 추이는'최저임금 적용효과에 관한 실태조사'의 조사대상 선정기준이 현...\n  3. [노트:S0000473 / 청크:3] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 조사대상 업종 및 사업체 규모 범위 확대\n〇 현행 「최저임금 적용효과에 관한 실태조사」의 모집단은 저임금 근로자의고용 비중이 높은 업종(C, G, H, I, L, N, P, Q, ...\n  4. [노트:S0000473 / 청크:4] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: ○ 조사의 용이성\n- 조사 표본이 수월하게 확보될 수 있도록 설정되어야 함\n⇒ 각 기준들이 서로 상충될 수도 있음: 예를 들어, 국제적으로 통용되는 저임금근로자의 개념은 중위임금의...\n=====================\n\n===== 그룹 ID: 5 (임계값: 0.7) =====\n포함된 청크 수: 6\n백링크 후보 노트 ID: ['S0000474']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000474 / 청크:0] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 『여성농업인실태조사』품질개선 컨설팅 최종결과보고서\nFinal Report on Quality Improvement Consulting for 『Survey on the Korean...\n  2. [노트:S0000474 / 청크:3] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 그리고 다문화가정여성 또한 실제 가구수 비율(1.2%)보다 많은 250가구(12.5%)로 할당을 함\n- 다문화가구와 귀농여성의 실제 가구수 비율보다 많이 할당한 이유는\n귀농여성농업...\n  3. [노트:S0000474 / 청크:6] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: □ 조사표 개선 방향 도출\n○ 조사대상 재설정\n- 농업을 주업으로 하면서, 농업외 소득활동이 있는 겸업 여성농업인을 포함하기 위해 여성농업인의 정의를 재설정할 필요가 있다. - 조...\n  4. [노트:S0000474 / 청크:8] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 유사통계 개요\n여성농업인실태조사와 관련있는 15종의 승인통계(주로 조사통계)들을 검토하였다. 크게 여성관련 조사 6종, 농업관련 조사 4종, 귀농관련 조사 2종, 다문화 관련 조사...\n  5. [노트:S0000474 / 청크:9] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 그 외 조사의 경우 부록3을 참고하도록 한다. 제 2 절 유사통계 검토결과\n1. 조사대상 남성 포함\n여성농업인 실태를 파악하기 위해 현재의 조사대상인 여성 뿐 아니라 남성역시 조사...\n  6. [노트:S0000474 / 청크:10] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 따라서 여성관리자패널조사는 제외한 나머지 유사통계 및 농림축산식품부의 귀농귀촌실태조사의 예산을 비교 검토하였다. - 2018 여성농업인 실태 조사 : 일반여성농업인 1,500가구,...\n=====================\n\n===== 그룹 ID: 6 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000474']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000474 / 청크:4] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 1. 표본설계 기본현황\n- 일반여성의 경우, 1500명의 표본이 모집단인 조사시점 우리나라 동지역을 제외한 읍/면 지역에 거주하는 만 18세 이상의 농가 여성을 대표할 수있도록 시...\n  2. [노트:S0000474 / 청크:14] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 표본 감소로 인한 제약에 따라일부 지역들을 제외하여 전체 모집단의 지역별 특성을 충분히 반영하지 못하였으며, 일반여성농업인의 표본비율이 전체 모집단의 단 0.16% 수준 밖에미치지...\n=====================\n\n===== 그룹 ID: 7 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000474']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000474 / 청크:12] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 현재 여성농업인실태조사는 지역별 통계를 공표하지 않고있는데, 이러한 층화기준을 사용함으로써 지역별 공표 및 공표수준별 추정이가능할 것이다. 제 4 절 표본크기 결정 및 표본배분\n1...\n  2. [노트:S0000474 / 청크:13] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 제 6 절 가중치 및 모수 추정\n1. 가중치 조정\n표본설계는 표본조사의 결과가 모집단을 잘 대표할 수 있도록 설계하는 데에 궁극적인 목적이 있으나, 잘 계획된 표본설계가 실제 표본...\n=====================\n\n===== 그룹 ID: 8 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000565']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000565 / 청크:6] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: )\n◯ 유형별 평가결과를 보면 7개 유형 모두 사업평가 보다 회계평가 점수가 높았음. ◯ 1유형 사회통합의 종합평가 점수가 88.45점으로 가장 높았고, 5유형 평화증진 및 국가안...\n  2. [노트:S0000565 / 청크:9] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: (유형별 평균의 평균이 아님. )\n◯ 유형별 회계평가 결과를 항목 별로 살펴보면 자부담 집행 적법성(99.25점/100점 환산점수), 정산자료 구비(95.83점, 100점 환산점수...\n=====================\n\n===== 그룹 ID: 9 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000565']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000565 / 청크:10] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: · 사업평가 결과가 가장 우수한 단체는 1유형의 다년도 단체 A(92.61점)이었으며, 가장 미흡한 단체는 7유형의 다년도 단체 I(64.64점)이었음. · 10개 다년도 사업 중...\n  2. [노트:S0000565 / 청크:11] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: ◯ 컨소시엄 사업을 진행한 단체는 1개(4유형 1개)이며, 종합평가 점수가 전체사업 평균 대비 매우 낮은 58.40점을 기록하였음. 사업평가 점수는 62.67점으로 전체사업 평균 ...\n=====================\n\n===== 그룹 ID: 10 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000865']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:6] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 강릉아산병원 \n7. 1차 의료기관 15곳: 전국 분포 의원급 등 2차 병원 5개소와 1차기관 15개소 대상 감시망 구성. (2)감시망 운영\n1)각 협력병원에 초회 내원하는 호흡기질...\n  2. [노트:S0000865 / 청크:0] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 보고서 요약문\n본 사업은 전국 지역사회를 기반으로 하는 협력 병의원과 질병관리본부를 연계하여, 호흡기 감염증 병원체의 실험실 감시망을 구축하고 운영함으로서 국내 주요 호흡기 감염증...\n  3. [노트:S0000865 / 청크:17] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: pneumoniae 균주는 각각 1이 검출되었다. 호흡기감염증 감시 체계를 통하여 각 병원체의 지역별, 연령별, 월별 양성률을 파악하고원인 병원체의 발생 추이, 환자 임상 정보 및...\n======================\n\n===== 그룹 ID: 11 (임계값: 0.7) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000865']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:12] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, 3. 인천의료원 4. 평촌성심병원 5....\n  2. [노트:S0000865 / 청크:5] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, \n3. 인천의료원 \n4. 평촌성심병원 \n5 춘천 성심병원 \n6....\n======================\n\n===== 그룹 ID: 12 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000865', 'S0000957']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:11] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 점검회의\n(1)일시 : 2017년 1월 10일 14:00 ~ 16:00\n(2)장소 : 반포동스마트웍센터(3)참석자\n(4)내용\n- 대표성 확보 : region, age, season...\n  2. [노트:S0000865 / 청크:13] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 춘천 성심병원 6. 원주기독병원 강릉아산병원 \n(2)지역사회폐렴 감시망 참여병원 관리\n- 해당 참여병원에 대하여 지속적인 교육을 실시하고 있으며, 교육내용은 사업목적, 검체 채취방...\n  3. [노트:S0000957 / 청크:9] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 4. 참여기관 현장 방문 조사 및 건의 사항 청취\n1)현장 방문을 위한 조사지 구성\n2)현장 방문 필요 병원 연구진 회의를 통해 선정 \n- 6개 병원\n3)현장 방문 조사 내용\n▸방...\n======================\n\n===== 그룹 ID: 13 (임계값: 0.7) =====\n포함된 청크 수: 7\n백링크 후보 노트 ID: ['S0000954']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000954 / 청크:0] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 편집순서 6 : 총괄용역과제의 연구결과\n학술연구개발용역과제 연구결과\n1.1 목표\n가. 연구 주제: 줄기세포 연구 활용 촉진을 위한 사람 전분화능줄기세포의 심근세포 분화 표준 프로토...\n  2. [노트:S0000954 / 청크:1] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 나. 인간 전분화능줄기세포에서 심근세포로 분화하는 방법은 배아의 심장 발생 과정과 환경을 모티브로 하는 기법들이 개발되었으며 대표적인 3가지 방법으로 1)배발생 단계의 환경과 유사...\n  3. [노트:S0000954 / 청크:2] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 공배양 기법은 발생학적 관점에서 접근하여, 심장 발생 과정에 중요한 내배엽성세포를 이용한 심근세포의 분화 유도 방법으로, 인간전분화능줄기세포와 내배엽성세포(END2)의 공배양을 통...\n  4. [노트:S0000954 / 청크:7] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 발주부서 연구원에게 CMC3 심근세포 분화 SOP 기술이전 \n가. CMC3 세포주를 이용한 심근세포 분화 기술 개발관련 SOP 확립: SOP 제공 전, 2017.10.18. 발주부...\n  5. [노트:S0000954 / 청크:8] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 2017.10.26. CMC3 세포주를 이용한 인간전분화능줄기세포 유래 심근세포 분화기술 SOP 문서를 제공함. 다....\n  6. [노트:S0000954 / 청크:11] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 마. 교육 및 기술이전 내용: SOP와 동일한 방법으로 CMC3 세포주를 이용하여 미분화배양, 계대배양, 심근세포 분화, 분화된 심근세포(응수축 심근세포)확인, 심근세포 정제 관련...\n  7. [노트:S0000954 / 청크:14] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 세포치료제 개발을 위한 심근세포 프로토콜 개발 전략\n가. 안전성 확보를 위한 프로토콜 개발: Xeno- free 배양을 위한 matrigel 대체 vitronectin, lamin...\n======================\n\n===== 그룹 ID: 14 (임계값: 0.7) =====\n포함된 청크 수: 3\n백링크 후보 노트 ID: ['S0000957']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000957 / 청크:2] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 중환자실에서 유치도뇨관 삽입률을 줄이기 어렵다는 것이 주요원인이었다. 이와 관련하여 프로그램을 보완하는 것이 필요해 보인다. ◦ 국내에서는 유치도뇨관 관련 요로감염 예방 프로그램은...\n  2. [노트:S0000957 / 청크:3] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 유치도뇨관 삽입의 대체 방법 강구\n3. 소변량 측정을 위한 방광 스캐너 사용\n4. 폐쇄된 요관 시스템 사용\n5. 정기적인 유치도뇨관 교체\n6. 무증상 세균뇨에 대한 선별\n7. 손위...\n  3. [노트:S0000957 / 청크:1] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 예방프로그램의 주요 목표를 표 1에 정리하였다. 매일 유치도뇨관 장착 여부와 필요성을 확인하고, 유치도뇨관보다는 다른 대체방법을 강구하고, 유치도뇨관을 삽입할 때 무균적으로 시행하...\n======================\n\n--- 그룹핑 결과 확인 완료 ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_EMBEDDING_FILE = '/kaggle/working/output/chunk_embeddings.jsonl' # 임베딩 데이터 파일\n\nprint(f\"입력 임베딩 파일: {INPUT_EMBEDDING_FILE}\")\n\n# --- 2. 데이터 로드 및 노트별 그룹핑 ---\n\nnotes_data = defaultdict(list) # {note_id: [vector1, vector2, ...]} 형태\nnote_chunk_counts = defaultdict(int) # 노트별 청크 수 저장\n\nprint(f\"\\n--- 임베딩 데이터 로드 및 노트별 그룹핑 시작 ---\")\nif not os.path.exists(INPUT_EMBEDDING_FILE):\n    print(f\"오류: 임베딩 파일 '{INPUT_EMBEDDING_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        line_count = 0\n        with open(INPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for line in f_in:\n                line_count += 1\n                try:\n                    data = json.loads(line)\n                    note_id = data.get('note_id')\n                    vector = data.get('vector')\n                    if note_id and vector:\n                        # 벡터를 float32 numpy 배열로 변환하여 추가\n                        notes_data[note_id].append(np.array(vector, dtype=np.float32))\n                        note_chunk_counts[note_id] += 1\n                except json.JSONDecodeError:\n                    print(f\"경고: 잘못된 JSON 라인 건너뛰기 ({line_count}번째 라인)\")\n                except Exception as parse_err:\n                     print(f\"경고: 데이터 처리 중 오류 발생 ({line_count}번째 라인): {parse_err}\")\n\n        print(f\"총 {len(notes_data)}개의 노트에서 {sum(note_chunk_counts.values())}개의 청크 임베딩 로드 완료.\")\n\n    except Exception as e:\n        print(f\"임베딩 파일 로딩 중 오류 발생: {e}\")\n        notes_data = None # 이후 처리 방지\n\n# --- 3. 문서 내 유사도 계산 및 통계 분석 ---\n\nif notes_data:\n    print(\"\\n--- 각 문서 내 청크 간 유사도 분포 분석 시작 ---\")\n\n    results_per_note = [] # 각 노트별 통계 저장 리스트\n    all_similarities = [] # 전체 유사도 값 저장 리스트\n\n    # 각 노트를 순회하며 분석\n    for note_id, vectors in tqdm(notes_data.items(), desc=\"문서별 유사도 분석 중\"):\n        num_chunks = len(vectors)\n\n        # 청크가 2개 이상인 경우에만 유사도 계산 가능\n        if num_chunks >= 2:\n            # 벡터 리스트를 numpy 배열로 변환\n            embeddings_array = np.array(vectors)\n\n            # 코사인 유사도 계산\n            try:\n                # 벡터가 이미 정규화되었다고 가정하고 내적 사용 or cosine_similarity 직접 사용\n                # sim_matrix = np.dot(embeddings_array, embeddings_array.T)\n                sim_matrix = cosine_similarity(embeddings_array)\n\n                # 대각선(자기 자신과의 유사도) 및 하삼각행렬 제외하고 유사도 값 추출\n                # np.triu_indices: 상삼각행렬의 인덱스를 반환 (k=1은 대각선 제외)\n                indices = np.triu_indices(num_chunks, k=1)\n                unique_similarities = sim_matrix[indices]\n\n                # 전체 유사도 리스트에 추가\n                all_similarities.extend(unique_similarities)\n\n                # 기술 통계량 계산 (numpy 사용)\n                if len(unique_similarities) > 0:\n                    stats = {\n                        'note_id': note_id,\n                        'num_chunks': num_chunks,\n                        'num_pairs': len(unique_similarities),\n                        'mean': np.mean(unique_similarities),\n                        'std': np.std(unique_similarities),\n                        'min': np.min(unique_similarities),\n                        '25% (Q1)': np.percentile(unique_similarities, 25),\n                        '50% (Median)': np.median(unique_similarities),\n                        '75% (Q3)': np.percentile(unique_similarities, 75),\n                        'max': np.max(unique_similarities)\n                    }\n                    results_per_note.append(stats)\n                else: # 비교할 쌍이 없는 경우 (이론상 num_chunks >= 2 이므로 발생 안 함)\n                     results_per_note.append({'note_id': note_id, 'num_chunks': num_chunks, 'num_pairs': 0})\n\n            except Exception as calc_err:\n                print(f\"오류: 노트 '{note_id}' 유사도 계산 중 오류 발생: {calc_err}\")\n        else:\n             # 청크가 1개인 노트는 비교 불가\n             results_per_note.append({'note_id': note_id, 'num_chunks': num_chunks, 'num_pairs': 0})\n\n    # --- 4. 결과 출력 ---\n    print(\"\\n--- 문서별 유사도 통계 결과 ---\")\n    if results_per_note:\n        df_results = pd.DataFrame(results_per_note)\n        # 보기 좋게 출력 (note_id 기준 정렬)\n        print(df_results.sort_values(by='note_id').to_string())\n    else:\n        print(\"분석할 노트가 없습니다.\")\n\n    print(\"\\n--- 전체 문서 내 청크 간 유사도 통계 (고유 쌍 기준) ---\")\n    if all_similarities:\n        all_similarities_array = np.array(all_similarities)\n        overall_stats = {\n            'Total Pairs': len(all_similarities_array),\n            'Overall Mean': np.mean(all_similarities_array),\n            'Overall Std': np.std(all_similarities_array),\n            'Overall Min': np.min(all_similarities_array),\n            'Overall 25% (Q1)': np.percentile(all_similarities_array, 25),\n            'Overall 50% (Median)': np.median(all_similarities_array),\n            'Overall 75% (Q3)': np.percentile(all_similarities_array, 75),\n            'Overall Max': np.max(all_similarities_array)\n        }\n        # 보기 좋게 출력\n        for key, value in overall_stats.items():\n             # 소수점 4자리까지 표시\n             if isinstance(value, (float, np.float32, np.float64)):\n                 print(f\"{key}: {value:.4f}\")\n             else:\n                 print(f\"{key}: {value}\")\n\n        # 히스토그램 시각화 (선택 사항, matplotlib 필요: !pip install matplotlib)\n        # import matplotlib.pyplot as plt\n        # plt.figure(figsize=(10, 6))\n        # plt.hist(all_similarities_array, bins=50, color='skyblue', edgecolor='black')\n        # plt.title('Distribution of Cosine Similarities between Chunks within Documents')\n        # plt.xlabel('Cosine Similarity')\n        # plt.ylabel('Frequency')\n        # plt.grid(axis='y', alpha=0.75)\n        # plt.show()\n\n    else:\n        print(\"계산된 유사도 값이 없습니다.\")\n\n    print(\"\\n--- 분석 완료 ---\")\n    print(\"위 통계 결과(특히 전체 중앙값, Q3 등)를 참고하여 그룹핑 임계값 후보를 정할 수 있습니다.\")\n    print(\"-\" * 40)\n\nelse:\n    print(\"\\n오류: 임베딩 데이터가 로드되지 않아 분석을 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:21:17.634053Z","iopub.execute_input":"2025-04-05T16:21:17.634319Z","iopub.status.idle":"2025-04-05T16:21:19.589342Z","shell.execute_reply.started":"2025-04-05T16:21:17.634297Z","shell.execute_reply":"2025-04-05T16:21:19.588355Z"}},"outputs":[{"name":"stdout","text":"입력 임베딩 파일: /kaggle/working/output/chunk_embeddings.jsonl\n\n--- 임베딩 데이터 로드 및 노트별 그룹핑 시작 ---\n총 10개의 노트에서 130개의 청크 임베딩 로드 완료.\n\n--- 각 문서 내 청크 간 유사도 분포 분석 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"문서별 유사도 분석 중:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6181daa41cab4de2a82805757d02be52"}},"metadata":{}},{"name":"stdout","text":"\n--- 문서별 유사도 통계 결과 ---\n    note_id  num_chunks  num_pairs      mean       std       min  25% (Q1)  50% (Median)  75% (Q3)       max\n0  S0000105          12         66  0.402825  0.222825 -0.080198  0.263595      0.464515  0.574849  0.789231\n1  S0000286          12         66  0.506660  0.118531  0.251630  0.429345      0.526255  0.600050  0.784638\n2  S0000337          24        276  0.398637  0.125208  0.093367  0.311305      0.405393  0.488760  0.731864\n3  S0000473           5         10  0.606027  0.107232  0.455347  0.497419      0.621908  0.694066  0.764980\n4  S0000474          15        105  0.501833  0.209834 -0.051862  0.496606      0.568077  0.619850  0.755033\n5  S0000565          12         66  0.458255  0.174801  0.024859  0.426608      0.511979  0.567507  0.741638\n6  S0000803           5         10  0.372503  0.165366  0.185897  0.216926      0.300597  0.560831  0.599471\n7  S0000865          18        153  0.382753  0.179589  0.012226  0.261097      0.404519  0.501824  0.886461\n8  S0000954          15        105  0.436240  0.197642  0.001856  0.268906      0.476777  0.593371  0.761264\n9  S0000957          12         66  0.410563  0.188077 -0.002264  0.289720      0.451702  0.545454  0.728094\n\n--- 전체 문서 내 청크 간 유사도 통계 (고유 쌍 기준) ---\nTotal Pairs: 923\nOverall Mean: 0.4271\nOverall Std: 0.1771\nOverall Min: -0.0802\nOverall 25% (Q1): 0.3117\nOverall 50% (Median): 0.4529\nOverall 75% (Q3): 0.5577\nOverall Max: 0.8865\n\n--- 분석 완료 ---\n위 통계 결과(특히 전체 중앙값, Q3 등)를 참고하여 그룹핑 임계값 후보를 정할 수 있습니다.\n----------------------------------------\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\nimport networkx as nx # 그래프 기반 그룹핑을 위해 사용\nfrom tqdm.notebook import tqdm\nimport pandas as pd # 결과 확인용 (선택 사항)\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_EMBEDDING_FILE = '/kaggle/working/output/chunk_embeddings.jsonl' # 임베딩 데이터 파일\nOUTPUT_GROUP_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl' # 그룹핑 결과 파일\nSIMILARITY_THRESHOLD = 0.6  # 유사도 임계값 ### 0.8로 수정됨 ###\nMIN_GROUP_SIZE = 2          # 그룹으로 간주할 최소 청크 수\n\nprint(f\"입력 임베딩 파일: {INPUT_EMBEDDING_FILE}\")\nprint(f\"출력 그룹 파일: {OUTPUT_GROUP_FILE}\")\nprint(f\"유사도 임계값: {SIMILARITY_THRESHOLD}\")\nprint(f\"최소 그룹 크기: {MIN_GROUP_SIZE}\")\n\n# --- 2. 데이터 로드 ---\n\nall_chunk_data = [] # 청크 데이터 (메타데이터 + 벡터) 저장 리스트\nchunk_embeddings = [] # 벡터만 따로 저장할 리스트 (유사도 계산용)\nchunk_lookup = {}     # 인덱스 -> 청크 메타데이터 매핑용 딕셔너리\n\nprint(f\"\\n--- 임베딩 데이터 로딩 시작 ---\")\nif not os.path.exists(INPUT_EMBEDDING_FILE):\n    print(f\"오류: 임베딩 파일 '{INPUT_EMBEDDING_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(INPUT_EMBEDDING_FILE, 'r', encoding='utf-8') as f_in:\n            for i, line in enumerate(f_in):\n                try:\n                    data = json.loads(line)\n                    if 'note_id' in data and 'chunk_id' in data and 'vector' in data and 'text' in data:\n                        vector = np.array(data['vector'], dtype=np.float32)\n                        if i > 0 and vector.shape[0] != chunk_embeddings[0].shape[0]:\n                             print(f\"경고: {i}번째 청크 벡터 차원 불일치. 건너<0xEB><0x9B><0x84>니다.\")\n                             continue\n                        all_chunk_data.append(data)\n                        chunk_embeddings.append(vector)\n                        chunk_lookup[i] = data\n                    else: print(f\"경고: {i}번째 라인 필수 필드 누락. 건너<0xEB><0x9B><0x84>니다.\")\n                except json.JSONDecodeError: print(f\"경고: 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n                except Exception as parse_err: print(f\"경고: 데이터 처리 중 오류 ({i}번째 라인): {parse_err}\")\n\n        if not all_chunk_data or not chunk_embeddings:\n             print(\"오류: 유효한 임베딩 데이터를 로드하지 못했습니다.\")\n             chunk_embeddings = None\n        else:\n            chunk_embeddings = np.array(chunk_embeddings)\n            print(f\"총 {len(all_chunk_data)}개의 청크 데이터 및 임베딩 로드 완료.\")\n            print(f\"임베딩 배열 형태: {chunk_embeddings.shape}\")\n\n    except Exception as e:\n        print(f\"임베딩 파일 로딩 중 오류 발생: {e}\")\n        chunk_embeddings = None\n\n# --- 3. 유사도 계산 및 그룹핑 ---\n\nif chunk_embeddings is not None and len(chunk_embeddings) >= MIN_GROUP_SIZE:\n    print(f\"\\n--- 유사도 계산 및 그룹핑 시작 (임계값: {SIMILARITY_THRESHOLD}) ---\")\n\n    # 코사인 유사도 계산 (모든 쌍)\n    print(\"코사인 유사도 행렬 계산 중...\")\n    try:\n        similarity_matrix = cosine_similarity(chunk_embeddings)\n        print(\"유사도 행렬 계산 완료.\")\n    except Exception as sim_err:\n        print(f\"오류: 유사도 행렬 계산 중 오류 발생: {sim_err}\")\n        similarity_matrix = None\n\n    if similarity_matrix is not None:\n        # 임계값 이상인 쌍들을 엣지로 하는 그래프 생성\n        print(\"유사도 기반 그래프 생성 중...\")\n        graph = nx.Graph()\n        num_chunks = len(all_chunk_data)\n        for i in tqdm(range(num_chunks), desc=\"엣지 추가 중\"):\n            graph.add_node(i)\n            for j in range(i + 1, num_chunks):\n                # 부동 소수점 비교 시 작은 오차 고려 (선택적)\n                # if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD - 1e-9:\n                if similarity_matrix[i, j] >= SIMILARITY_THRESHOLD:\n                    graph.add_edge(i, j, weight=similarity_matrix[i, j])\n\n        # 연결된 컴포넌트(그룹) 찾기\n        print(\"연결된 컴포넌트(그룹) 찾는 중...\")\n        connected_components = list(nx.connected_components(graph))\n\n        # 최소 크기 기준을 만족하는 그룹만 필터링\n        valid_groups = [group for group in connected_components if len(group) >= MIN_GROUP_SIZE]\n        print(f\"총 {len(valid_groups)}개의 유효한 그룹 (크기 >= {MIN_GROUP_SIZE})을 찾았습니다.\")\n\n        # --- 4. 그룹 정보 및 액션 대상 생성/저장 ---\n        print(f\"\\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\")\n        grouped_results = []\n        group_id_counter = 0\n\n        for group_indices in tqdm(valid_groups, desc=\"그룹 정보 생성 중\"):\n            # 그룹 인덱스들을 set으로 변환 (조회 속도 향상)\n            group_indices_set = set(group_indices)\n\n            group_data = {\n                \"group_id\": group_id_counter,\n                \"similarity_threshold\": SIMILARITY_THRESHOLD,\n                \"member_chunks\": [],\n                \"synthesis_input_texts\": [],\n                \"backlink_candidate_notes\": set()\n            }\n\n            member_texts = []\n            note_ids_in_group = set()\n            # 그룹 멤버 정보 추가\n            for index in group_indices_set:\n                chunk_info = chunk_lookup.get(index)\n                if chunk_info:\n                    member_chunk_info = {\n                        \"note_id\": chunk_info.get(\"note_id\"),\n                        \"chunk_id\": chunk_info.get(\"chunk_id\"),\n                        \"doc_title\": chunk_info.get(\"doc_title\", \"\"),\n                        \"filepath\": chunk_info.get(\"filepath\", \"\"),\n                        \"text_preview\": chunk_info.get(\"text\", \"\")[:100] + \"...\"\n                        # 그룹 내 다른 멤버와의 유사도 정보 추가 (선택적, 계산량 증가)\n                        # \"similarities_within_group\": {}\n                    }\n                    # # 그룹 내 다른 멤버와의 유사도 계산 (선택적)\n                    # for other_index in group_indices_set:\n                    #     if index != other_index:\n                    #         sim = similarity_matrix[index, other_index]\n                    #         member_chunk_info[\"similarities_within_group\"][f\"chunk_{other_index}\"] = round(sim, 4)\n\n                    group_data[\"member_chunks\"].append(member_chunk_info)\n                    member_texts.append(chunk_info.get(\"text\", \"\"))\n                    note_ids_in_group.add(chunk_info.get(\"note_id\"))\n\n            group_data[\"synthesis_input_texts\"] = member_texts\n            group_data[\"backlink_candidate_notes\"] = sorted(list(note_ids_in_group))\n\n            # member_chunks 리스트를 chunk_id 기준으로 정렬 (선택적)\n            group_data[\"member_chunks\"].sort(key=lambda x: (x[\"note_id\"], x[\"chunk_id\"]))\n\n            grouped_results.append(group_data)\n            group_id_counter += 1\n\n        # 최종 결과를 JSON Lines 파일로 저장 (덮어쓰기 'w')\n        try:\n            with open(OUTPUT_GROUP_FILE, 'w', encoding='utf-8') as f_out:\n                for group_result in grouped_results:\n                    json_string = json.dumps(group_result, ensure_ascii=False)\n                    f_out.write(json_string + '\\n')\n            print(f\"그룹핑 결과가 '{OUTPUT_GROUP_FILE}'에 저장되었습니다.\")\n        except Exception as write_err:\n            print(f\"오류: 그룹핑 결과 파일 저장 중 오류 발생: {write_err}\")\n\n        print(\"\\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\")\n        print(\"-\" * 40)\n\n    else:\n         print(\"오류: 유사도 행렬 계산에 실패하여 그룹핑을 진행할 수 없습니다.\")\n\nelif len(chunk_embeddings) < MIN_GROUP_SIZE:\n     print(f\"\\n오류: 로드된 청크 수가 최소 그룹 크기({MIN_GROUP_SIZE})보다 작아 그룹핑을 수행할 수 없습니다.\")\nelse:\n    print(\"\\n오류: 임베딩 데이터가 로드되지 않아 그룹핑 프로세스를 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:23:26.090499Z","iopub.execute_input":"2025-04-06T21:23:26.090986Z","iopub.status.idle":"2025-04-06T21:23:26.260485Z","shell.execute_reply.started":"2025-04-06T21:23:26.090957Z","shell.execute_reply":"2025-04-06T21:23:26.259405Z"}},"outputs":[{"name":"stdout","text":"입력 임베딩 파일: /kaggle/working/output/chunk_embeddings.jsonl\n출력 그룹 파일: /kaggle/working/output/grouped_chunks_info.jsonl\n유사도 임계값: 0.6\n최소 그룹 크기: 2\n\n--- 임베딩 데이터 로딩 시작 ---\n총 130개의 청크 데이터 및 임베딩 로드 완료.\n임베딩 배열 형태: (130, 768)\n\n--- 유사도 계산 및 그룹핑 시작 (임계값: 0.6) ---\n코사인 유사도 행렬 계산 중...\n유사도 행렬 계산 완료.\n유사도 기반 그래프 생성 중...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"엣지 추가 중:   0%|          | 0/130 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c185cc5f80e415caeb7feefb814eeb0"}},"metadata":{}},{"name":"stdout","text":"연결된 컴포넌트(그룹) 찾는 중...\n총 8개의 유효한 그룹 (크기 >= 2)을 찾았습니다.\n\n--- 그룹 정보 및 액션 대상 파일 저장 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"그룹 정보 생성 중:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1473329fd98840cbacdf1b0fa7c3716e"}},"metadata":{}},{"name":"stdout","text":"그룹핑 결과가 '/kaggle/working/output/grouped_chunks_info.jsonl'에 저장되었습니다.\n\n--- 유사 청크 그룹핑 및 액션 정보 생성 완료 ---\n----------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport json\nimport os\n\n# 그룹핑 결과 파일 경로\nGROUP_INFO_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl'\n\nprint(f\"--- 그룹핑 결과 확인 ({GROUP_INFO_FILE}) ---\")\n\nif not os.path.exists(GROUP_INFO_FILE):\n    print(f\"오류: 그룹핑 결과 파일 '{GROUP_INFO_FILE}'을 찾을 수 없습니다.\")\nelse:\n    try:\n        with open(GROUP_INFO_FILE, 'r', encoding='utf-8') as f_in:\n            group_found = False\n            for line in f_in:\n                group_found = True\n                try:\n                    group_data = json.loads(line)\n                    group_id = group_data.get(\"group_id\", \"N/A\")\n                    threshold = group_data.get(\"similarity_threshold\", \"N/A\")\n                    member_chunks = group_data.get(\"member_chunks\", [])\n                    backlink_notes = group_data.get(\"backlink_candidate_notes\", [])\n\n                    print(f\"\\n===== 그룹 ID: {group_id} (임계값: {threshold}) =====\")\n                    print(f\"포함된 청크 수: {len(member_chunks)}\")\n                    print(f\"백링크 후보 노트 ID: {backlink_notes}\")\n\n                    print(\"\\n--- 포함된 청크 목록 (미리보기) ---\")\n                    if not member_chunks:\n                        print(\"  (포함된 청크 정보 없음)\")\n                    else:\n                        for i, chunk_info in enumerate(member_chunks):\n                            note_id = chunk_info.get(\"note_id\", \"?\")\n                            chunk_id = chunk_info.get(\"chunk_id\", \"?\")\n                            title = chunk_info.get(\"doc_title\", \"제목 없음\")\n                            preview = chunk_info.get(\"text_preview\", \"내용 없음\")\n                            print(f\"  {i+1}. [노트:{note_id} / 청크:{chunk_id}] (제목: {title})\")\n                            print(f\"     내용: {preview}\")\n\n                    # 통합용 텍스트는 너무 길 수 있으니 필요한 경우 별도 확인\n                    # synthesis_texts = group_data.get(\"synthesis_input_texts\", [])\n                    # print(\"\\n--- 통합 대상 텍스트 목록 ---\")\n                    # for i, text in enumerate(synthesis_texts):\n                    #     print(f\"  {i+1}. {text[:150]}...\") # 일부만 출력\n\n                    print(\"=\" * (len(str(group_id)) + 20)) # 구분선\n\n                except json.JSONDecodeError:\n                    print(f\"\\n오류: 잘못된 JSON 라인 발견 - {line.strip()}\")\n                except Exception as parse_err:\n                    print(f\"\\n오류: 그룹 데이터 처리 중 오류 발생 - {parse_err}\")\n\n            if not group_found:\n                print(\"결과 파일은 존재하지만, 유효한 그룹 정보를 찾지 못했습니다.\")\n\n    except Exception as e:\n        print(f\"그룹핑 결과 파일 읽기 중 오류 발생: {e}\")\n\nprint(\"\\n--- 그룹핑 결과 확인 완료 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:23:26.290800Z","iopub.execute_input":"2025-04-06T21:23:26.291098Z","iopub.status.idle":"2025-04-06T21:23:26.346227Z","shell.execute_reply.started":"2025-04-06T21:23:26.291077Z","shell.execute_reply":"2025-04-06T21:23:26.345336Z"}},"outputs":[{"name":"stdout","text":"--- 그룹핑 결과 확인 (/kaggle/working/output/grouped_chunks_info.jsonl) ---\n\n===== 그룹 ID: 0 (임계값: 0.6) =====\n포함된 청크 수: 57\n백링크 후보 노트 ID: ['S0000105', 'S0000337', 'S0000473', 'S0000474', 'S0000565', 'S0000865', 'S0000957']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:1] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 연구배경 및 목적\n1. 연구의 배경민원행정서비스에 대한 만족도 조사는 한국행정연구원에서 1996년 조사모델과 방법을 개발한 이후 현재까지 지속적으로 수행하고 있는 중앙정부 및 지방...\n  2. [노트:S0000105 / 청크:3] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 11. 30(35일)\n▷ 공간적 범위\n- 횡성군 종합민원실\n▷ 내용적 범위\n- 서론(연구 개요로서 연구의 배경과 목적 및 범위 및 방법)\n- 민원 만족도 조사(민원서비스 시설, 민...\n  3. [노트:S0000105 / 청크:6] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 민원행정 서비스 시설에 관한 만족도 분석결과\n가. 접근성에 대한 만족도 분석결과\n민원처리를 위하여 종합민원실까지의 접근성과 관련한 이용고객들의 만족도 평가는 평균71.14점으로 나...\n  4. [노트:S0000105 / 청크:7] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 다. 민원처리 공정성에 대한 만족도\n신청한 민원의 처리와 관련하여 업무담당자가 공정하게 처리하고 있는가에 대한 만족도 결과는 70.47점으로 나타났다. 이는 상반기의 65.82점 ...\n  5. [노트:S0000105 / 청크:8] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 6. 분산분석(ANOVA; analysis of variance)결과\n가. 연령별 분산분석\n1)연령별 민원시설관련 분산분석응답자들의 연령대별 평균차이를 비교하여 통계적으로 유의한 ...\n  6. [노트:S0000105 / 청크:9] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 나. 민원분야에 따른 분산분석\n1)민원시설관련 만족도 분산분석응답자들이 제공받은 민원서비스 분야별 접근성에 대한 만족도는 평균 72.70점으로, 주차장과 관련한 만족도는 61.85...\n  7. [노트:S0000105 / 청크:10] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 종합 평가\n1. 민원행정 시설에 대한 만족도\n▷ 민원행정 시설에 대한 종합만족지수는 70.33점으로 상반기에 비해 4.32점이 높아졌음. ▷ 접근성에 대한 만족도는 71.14점으로...\n  8. [노트:S0000105 / 청크:11] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 발전방안\n1. 만족도 평가와 비교분석을 통한 문제점 탐색\n▷ 2006년 상반기 조사결과에서 도출된 만족도 점수와 순위를 하반기 결과와 비교하여 연결한 결과 편의시설 항목이 가장 만...\n  9. [노트:S0000337 / 청크:0] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 1.2 제1기 지역사회복지계획 실행결과 분석\n○ 2기 계획 수립을 위한 개선과제를 파악하기 위해 1기 계획 수립 과정 및 실행을 분석하고자 함. 이를 위해 ① 1기 계획의 문제와 ...\n  10. [노트:S0000337 / 청크:4] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: ○ 여론주도층은 구청 관련 공무원 및 사회복지 기관 종사자를 중심으로 선정하였으며, 관련 공무원은 구청 주민생활과, 사회복지과, 가정복지과, 고용정책과, 보건소, 동사무소 사회복지...\n  11. [노트:S0000337 / 청크:7] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 구성형태는 부부+자녀가 62.0%로 가장 높은 비율을 보여주었고 다음으로 조부(모)+부부 중 1일+자녀 12.7%, 64세 이하 부부(2명 모두)7.0%, 청장년 1인 가구 4.4...\n  12. [노트:S0000337 / 청크:15] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 5 1 ）. •건전한 가구경 제 유지와 소비에 대한 교육” （3.42） 순으로 나타났다, \n〈표3.2-41＞는 용산구내 복지기관 및 사업에 관한 정보를 가장 많이 얻는 곳 대해 조...\n  13. [노트:S0000337 / 청크:21] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 용산구의 주민들이 사회복지 발전을 위해 향후 우선적으로 개선되어야 할 사항으로서는 ‘재개발 등 주택, 주거환경 개선 19.7%’, 의료시선과 복지시설 확충이 각각 15.9%로 나타...\n  14. [노트:S0000473 / 청크:0] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 『최저임금 적용효과에 관한 실태조사』품질개선 컨설팅 최종결과보고서 \n요약\n「최저임금 적용효과에 관한 실태조사」는 최저임금이 기업경영과 고용에 미치는 효과, 사용자와 근로자가 실제 ...\n  15. [노트:S0000473 / 청크:1] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 최저임금 등 임금분포와 조사대상\n○ 최저임금과 비교되는 시급 임금분포와 최저임금의 중위임금에 대비한 상대적 수준 추이는'최저임금 적용효과에 관한 실태조사'의 조사대상 선정기준이 현...\n  16. [노트:S0000473 / 청크:3] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: 조사대상 업종 및 사업체 규모 범위 확대\n〇 현행 「최저임금 적용효과에 관한 실태조사」의 모집단은 저임금 근로자의고용 비중이 높은 업종(C, G, H, I, L, N, P, Q, ...\n  17. [노트:S0000473 / 청크:4] (제목: 『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     내용: ○ 조사의 용이성\n- 조사 표본이 수월하게 확보될 수 있도록 설정되어야 함\n⇒ 각 기준들이 서로 상충될 수도 있음: 예를 들어, 국제적으로 통용되는 저임금근로자의 개념은 중위임금의...\n  18. [노트:S0000474 / 청크:0] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 『여성농업인실태조사』품질개선 컨설팅 최종결과보고서\nFinal Report on Quality Improvement Consulting for 『Survey on the Korean...\n  19. [노트:S0000474 / 청크:2] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 조사항목여성농업인실태조사에서 공표되는 주요 분류 수준은 거주지별, 가구형태별, 가족수별, 월평균지출액 등이다. 이때 거주지는 7개 분류(수도권/강원권/충청권/전라권/경북권/경남권/...\n  20. [노트:S0000474 / 청크:3] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 그리고 다문화가정여성 또한 실제 가구수 비율(1.2%)보다 많은 250가구(12.5%)로 할당을 함\n- 다문화가구와 귀농여성의 실제 가구수 비율보다 많이 할당한 이유는\n귀농여성농업...\n  21. [노트:S0000474 / 청크:4] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 1. 표본설계 기본현황\n- 일반여성의 경우, 1500명의 표본이 모집단인 조사시점 우리나라 동지역을 제외한 읍/면 지역에 거주하는 만 18세 이상의 농가 여성을 대표할 수있도록 시...\n  22. [노트:S0000474 / 청크:5] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 모집단 및 표본 분포\n- 통계청에서 제공하는 2015년 농림어업총조사 자료를 사용하여 거주지역별모집단과 표본의 분포를 살펴본다. - 총 가구 108만 8,518가구 중 조사 지역이...\n  23. [노트:S0000474 / 청크:6] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: □ 조사표 개선 방향 도출\n○ 조사대상 재설정\n- 농업을 주업으로 하면서, 농업외 소득활동이 있는 겸업 여성농업인을 포함하기 위해 여성농업인의 정의를 재설정할 필요가 있다. - 조...\n  24. [노트:S0000474 / 청크:7] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: ○ 조사표 영역 재설정\n- 기존 복지현황 영역을 삭제하고, 양성평등 영역을 새로 신설한다. ○ 문항 수정\n- 전문가 자문, 농림축산부, 통계청의 의견을 검토하여 문항 수정\n2. 조...\n  25. [노트:S0000474 / 청크:8] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 유사통계 개요\n여성농업인실태조사와 관련있는 15종의 승인통계(주로 조사통계)들을 검토하였다. 크게 여성관련 조사 6종, 농업관련 조사 4종, 귀농관련 조사 2종, 다문화 관련 조사...\n  26. [노트:S0000474 / 청크:9] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 그 외 조사의 경우 부록3을 참고하도록 한다. 제 2 절 유사통계 검토결과\n1. 조사대상 남성 포함\n여성농업인 실태를 파악하기 위해 현재의 조사대상인 여성 뿐 아니라 남성역시 조사...\n  27. [노트:S0000474 / 청크:10] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 따라서 여성관리자패널조사는 제외한 나머지 유사통계 및 농림축산식품부의 귀농귀촌실태조사의 예산을 비교 검토하였다. - 2018 여성농업인 실태 조사 : 일반여성농업인 1,500가구,...\n  28. [노트:S0000474 / 청크:11] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 모집단 분석\n2020년 농림어업총조사의 농가 데이터를 이용하여 조사모집단을 파악하고자 한다. 2020년 농림어업총조사의 지역별(17개)가구 수를 살펴보면 다음과같다. 전체 가구수는...\n  29. [노트:S0000474 / 청크:12] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 현재 여성농업인실태조사는 지역별 통계를 공표하지 않고있는데, 이러한 층화기준을 사용함으로써 지역별 공표 및 공표수준별 추정이가능할 것이다. 제 4 절 표본크기 결정 및 표본배분\n1...\n  30. [노트:S0000474 / 청크:13] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 제 6 절 가중치 및 모수 추정\n1. 가중치 조정\n표본설계는 표본조사의 결과가 모집단을 잘 대표할 수 있도록 설계하는 데에 궁극적인 목적이 있으나, 잘 계획된 표본설계가 실제 표본...\n  31. [노트:S0000474 / 청크:14] (제목: 『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     내용: 표본 감소로 인한 제약에 따라일부 지역들을 제외하여 전체 모집단의 지역별 특성을 충분히 반영하지 못하였으며, 일반여성농업인의 표본비율이 전체 모집단의 단 0.16% 수준 밖에미치지...\n  32. [노트:S0000565 / 청크:1] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: (주)더브릿지컨설팅\n[제1장]\n사업 및 평가 개요\n1. 사업개요\n1)사업 추진개요\n○ 추진목적\n비영리민간단체 공익활동 지원사업은 비영리민간단체의 자발적 활동을 보장하고 건전한 민간...\n  33. [노트:S0000565 / 청크:4] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: ○ 사업실행 및 집행\n단체가 계획한 단위사업에 따른 사업실행 및 그에 따른 예산집행을 보조금 교부 이후부터 12월 31일까지 진행함. ○ 교육 및 워크숍 진행\n단체들의 사업 이해도...\n  34. [노트:S0000565 / 청크:5] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: <회계평가 항목별 평가결과>\n◯ 회계평가 결과 가장 점수가 높은 상위 8개 우수단체는 2유형에 2개, 3유형에 3개, 4유형에 1개, 5유형에 2개로 집계됨. ◯ 8개 우수단체 모...\n  35. [노트:S0000565 / 청크:6] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: )\n◯ 유형별 평가결과를 보면 7개 유형 모두 사업평가 보다 회계평가 점수가 높았음. ◯ 1유형 사회통합의 종합평가 점수가 88.45점으로 가장 높았고, 5유형 평화증진 및 국가안...\n  36. [노트:S0000565 / 청크:7] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: <유형별 사업평가 결과>\n※ 전체 평균값은 187개 단체 개별 데이터를 모수로 한 평균값임. (유형별 평균의 평균이 아님. )\n◯ 유형별 사업평가 결과를 항목 별로 살펴보면, 운영...\n  37. [노트:S0000565 / 청크:8] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: 그러나, 사업성과는 이에 미치지 못했는데, 이는 코로나19 팬데믹의 영향으로 다수 사업이 취소되거나 변경되었기 때문인 것으로 파악됨. · 유형별 사업평가 결과를 사업평가 항목 별로...\n  38. [노트:S0000565 / 청크:9] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: (유형별 평균의 평균이 아님. )\n◯ 유형별 회계평가 결과를 항목 별로 살펴보면 자부담 집행 적법성(99.25점/100점 환산점수), 정산자료 구비(95.83점, 100점 환산점수...\n  39. [노트:S0000565 / 청크:10] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: · 사업평가 결과가 가장 우수한 단체는 1유형의 다년도 단체 A(92.61점)이었으며, 가장 미흡한 단체는 7유형의 다년도 단체 I(64.64점)이었음. · 10개 다년도 사업 중...\n  40. [노트:S0000565 / 청크:11] (제목: 2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     내용: ◯ 컨소시엄 사업을 진행한 단체는 1개(4유형 1개)이며, 종합평가 점수가 전체사업 평균 대비 매우 낮은 58.40점을 기록하였음. 사업평가 점수는 62.67점으로 전체사업 평균 ...\n  41. [노트:S0000865 / 청크:0] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 보고서 요약문\n본 사업은 전국 지역사회를 기반으로 하는 협력 병의원과 질병관리본부를 연계하여, 호흡기 감염증 병원체의 실험실 감시망을 구축하고 운영함으로서 국내 주요 호흡기 감염증...\n  42. [노트:S0000865 / 청크:2] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: pneumoniae 균주는 각각 1건이 검출되었다. 지역사회폐렴 목표 220건 중 102건이 등록되었고, 그 중에서 75건에서 원인균이 분리되었 다. 지역사회폐렴 중에서 원인균이 ...\n  43. [노트:S0000865 / 청크:4] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: pneumoniae 가 차지하였다. 우리나라에서는 항체 검사를 이용한 손 등의 연구에서 M. pneumoniae 가 6.3%, C. pneumoniae가 7.1%의 발생 빈도를 보...\n  44. [노트:S0000865 / 청크:6] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 강릉아산병원 \n7. 1차 의료기관 15곳: 전국 분포 의원급 등 2차 병원 5개소와 1차기관 15개소 대상 감시망 구성. (2)감시망 운영\n1)각 협력병원에 초회 내원하는 호흡기질...\n  45. [노트:S0000865 / 청크:9] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: D. 280nm)를 확인\n- 유전자 검사는 발주부서에서 제시한 primer set를 사용하고, 반응조건은 다음과 같다(기타 변경시 발주부서와 협의)\n(4)검체의 수송체계 확립 \n1...\n  46. [노트:S0000865 / 청크:11] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 점검회의\n(1)일시 : 2017년 1월 10일 14:00 ~ 16:00\n(2)장소 : 반포동스마트웍센터(3)참석자\n(4)내용\n- 대표성 확보 : region, age, season...\n  47. [노트:S0000865 / 청크:13] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 춘천 성심병원 6. 원주기독병원 강릉아산병원 \n(2)지역사회폐렴 감시망 참여병원 관리\n- 해당 참여병원에 대하여 지속적인 교육을 실시하고 있으며, 교육내용은 사업목적, 검체 채취방...\n  48. [노트:S0000865 / 청크:15] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 급성인후염 감시망별 호흡기 병원체의 분리/검출 및 항균제 내성 결과\n가. 급성 인후염(상기도 감염증)감시망의 호흡기세균 분리/검출 결과\n급성 인후염은 전국 16개 병의원에서 감시하...\n  49. [노트:S0000865 / 청크:17] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: pneumoniae 균주는 각각 1이 검출되었다. 호흡기감염증 감시 체계를 통하여 각 병원체의 지역별, 연령별, 월별 양성률을 파악하고원인 병원체의 발생 추이, 환자 임상 정보 및...\n  50. [노트:S0000957 / 청크:0] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 편집순서 4 : 총괄 연구사업 연구실적\n정책연구용역사업 당해연도 연구실적\n제1장 연구의 필요성\n가. 연구의 목적 및 필요성\n◦ 요로감염은 가장 흔한 의료관련감염의 하나로 상급의료기...\n  51. [노트:S0000957 / 청크:1] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 예방프로그램의 주요 목표를 표 1에 정리하였다. 매일 유치도뇨관 장착 여부와 필요성을 확인하고, 유치도뇨관보다는 다른 대체방법을 강구하고, 유치도뇨관을 삽입할 때 무균적으로 시행하...\n  52. [노트:S0000957 / 청크:2] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 중환자실에서 유치도뇨관 삽입률을 줄이기 어렵다는 것이 주요원인이었다. 이와 관련하여 프로그램을 보완하는 것이 필요해 보인다. ◦ 국내에서는 유치도뇨관 관련 요로감염 예방 프로그램은...\n  53. [노트:S0000957 / 청크:3] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 유치도뇨관 삽입의 대체 방법 강구\n3. 소변량 측정을 위한 방광 스캐너 사용\n4. 폐쇄된 요관 시스템 사용\n5. 정기적인 유치도뇨관 교체\n6. 무증상 세균뇨에 대한 선별\n7. 손위...\n  54. [노트:S0000957 / 청크:5] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 그러나 좀 더 국내의 대표성을 가지기 위해서는 참여기관을 수도권 이외의 지역으로 확대할 필요가 있다. - 참여의료기관의 특성 : KONIS를 기준으로 병원의 규모를 나누어 본다면 ...\n  55. [노트:S0000957 / 청크:6] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: kr / 사무국 ☎<|start|>#@전번1#<|end|>\n※ 연구에 참여하시는 병원 감염관리 담당자에게는 소액의 정보입력비를 제공하며, 요로감염 증진사업에 필요한 리플릿, 포스터...\n  56. [노트:S0000957 / 청크:8] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 감염감시 및 모니터링 지표 기반 효과 평가 \n- 2017년 9월 전산입력프로그램 개발 완료 \n- 2017년 10월 자료 입력시작. 7,8,9월 자료 우선 입력후 분석 시행\n1)참여...\n  57. [노트:S0000957 / 청크:9] (제목: 의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     내용: 4. 참여기관 현장 방문 조사 및 건의 사항 청취\n1)현장 방문을 위한 조사지 구성\n2)현장 방문 필요 병원 연구진 회의를 통해 선정 \n- 6개 병원\n3)현장 방문 조사 내용\n▸방...\n=====================\n\n===== 그룹 ID: 1 (임계값: 0.6) =====\n포함된 청크 수: 4\n백링크 후보 노트 ID: ['S0000105', 'S0000337']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:4] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 분석결과\n1. 표본의 특성본 연구에는 자기기입식 및 1:1 면접식으로 수집된 총 524부의 설문이 유효표본으로 분석에 활용되었다. 인구통계학적인 특성을 파악하기 위하여 빈도분석을 ...\n  2. [노트:S0000337 / 청크:1] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 남성이 116,954명. 여성이 121,754명으로 여성이 약간 많으며 세대 당 인구수는 222명임. □ 타 자치구와의 비교를 위해 2008년 12월 말 기준으로 인구수를 비교한 ...\n  3. [노트:S0000337 / 청크:2] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 영등포구 68.8%에 이어 7번째로 높은 것으로 나타남. 2.3.2 용산구 복지재정 현황\n□ 2009년 예산 기준 총사회복지부문 세출 규모를 비교한 결과, 용산구의 경우 598억 ...\n  4. [노트:S0000337 / 청크:6] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 도한, 연령, 등급에 따른 집단별 교차분석 및 분산 분석을 실시하여 유의미한 차이를 나타낸 문항을 분석하였다. 3.2.2 분석 결과\n1） 인구사회학적 특성\n먼저 성별을 보면, 남녀...\n=====================\n\n===== 그룹 ID: 2 (임계값: 0.6) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000105', 'S0000954']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000105 / 청크:5] (제목: 횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     내용: 2....\n  2. [노트:S0000954 / 청크:12] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 사....\n=====================\n\n===== 그룹 ID: 3 (임계값: 0.6) =====\n포함된 청크 수: 10\n백링크 후보 노트 ID: ['S0000286']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000286 / 청크:0] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 전자기록물 장기보존패키지 - 제2부: 디렉토리로 구조화된 방식(NEO3)\n1. 적용범위\n이 표준은 디지털객체에 대한 논리적 또는 물리적 캡슐화 규격을 규정하는 동시에 진본성, 무결...\n  2. [노트:S0000286 / 청크:2] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 발행연도가 표기되지 않은 인용표준은 최신판(모든 개정내용을 포함)을 적용한다. ㆍ ISO 15489-1:2016, Information and documentation - Reco...\n  3. [노트:S0000286 / 청크:3] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: ㆍ KS X 6030 확장가능한 마크업 언어(XML)- Extensible Markup Language(XML)1.0 \nㆍ KS X 6033 확장가능한 마크업 언어 전자서명 구문과...\n  4. [노트:S0000286 / 청크:4] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 3.1 기록관리 메타데이터(Metadata for managing records)\n시간과 공간을 초월하여 기록의 생산, 관리와 이용이 가능하도록 하는 구조화된 혹은 반구조화된 정보...\n  5. [노트:S0000286 / 청크:5] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: DOI는 인터넷 상에서 동작하도록 설계되었으며 객체의 물리적 위치(IP 주소, 디렉토리 경로 등)가 변경되어도 DOI명은 변하지 않는다. DOI시스템을 통해 해당 DOI명과 관련된...\n  6. [노트:S0000286 / 청크:7] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 인증기관에서 발급한다. 3.8 인코딩(Encoding)\n기계적 처리를 위해 고안된 구문 또는 신호를 특정한 부호들의 나열로 그 형태를 바꾸는 것\n[정보통신용어사전, KS X ISO...\n  7. [노트:S0000286 / 청크:8] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 전달된 메시지나 문서의 원래 내용이 변조되지 않았다는 것을 보증하기 위해 사용될 수 있다. [「전자서명법」제2조제2항 참조하여 개작]\n3.12 전자서명 알고리즘(Digital si...\n  8. [노트:S0000286 / 청크:9] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: HTML, 텍스트(txt), PDF, 포스트스크립트(PostScript), 이미지(PNG 등), 한글(HWP)등의 형식으로 변환할 수 있다. [TTA 정보통신용어사전]\n4. 장기보...\n  9. [노트:S0000286 / 청크:10] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 영구기록물관리기관 등은 디렉토리로 구조화된 방식을 채택하는 경우라 하더라도 기관의 환경 등에 따라 NEO3가 아닌 다른 캡슐화 방식을 정의하고 적용할 수 있다. 5. 디렉토리로 구...\n  10. [노트:S0000286 / 청크:11] (제목: 전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     내용: 비고 3\n객체콘텐트(M9)는'NAK 31-1:2022(v2.3)전자기록물 장기보존패키지 기술규격- 제1부: XML로 포맷화된 방식(NEO2)'의 객체컨텐츠(M9)와 동일한 요소로 ...\n=====================\n\n===== 그룹 ID: 4 (임계값: 0.6) =====\n포함된 청크 수: 4\n백링크 후보 노트 ID: ['S0000337']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000337 / 청크:10] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 분석결과. \"자녀교육비'가 23.3%의 비율로 가장 높은 비율을 나타냈다, 이어서'주택관련비용'16.4%. ••세금관련부분” 15.1%. _사회 활동 비. 각종 축, 조의금'13....\n  2. [노트:S0000337 / 청크:11] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 〈표3. 2 _ 2 4 ＞ 는 구직 희망자 재취업 교육훈련프로그램 필요여부를 살펴보았다. 분석결과 •필 요하다\" 가 3 0. 5 %로 가장 높은 비율로 나타났고 •매우필요하다• 3...\n  3. [노트:S0000337 / 청크:12] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 서비스직” 1 0. 3 %, ••임시직. 단순노무직'1 0. 3 % 순으로 나타 났다. 〈표3.2-26〉는 구직 형식을 살펴보았다. 분석결과 *자영업'아 43.1%로 가장 높은 비...\n  4. [노트:S0000337 / 청크:14] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 〈 표 3. 2 - 3 3 ＞ 는 가 정 건 강 을 점 검 하 고 안 내 할 방 문 간 호 원 서 비 스 선 호 도 를 조 사 한 결 과 이 다. 분석결과 M필요없다'가 3 4. ...\n=====================\n\n===== 그룹 ID: 5 (임계값: 0.6) =====\n포함된 청크 수: 5\n백링크 후보 노트 ID: ['S0000337', 'S0000803']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000337 / 청크:18] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 〈표3.2-50>는 어떤 활동을 참여했는지를 조사한 결과이다. 분석결과'무의탁 어르산 사회복자시설의 가사일 돕기. 반찬 배달. 차량지원 둥 가정봉가 3 6. 1 %로 가장 높게 나...\n  2. [노트:S0000337 / 청크:19] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: 2-5 2>는 참여하길 희망하는 봉사활동을 조사한 결과이다. 분석결과 \"무의탁 어르신, 사회복지 시설의 가사일 돕기, 밑반찬 배달. 차량지원 등 가정봉사胃 가 19.3%의 비율로 ...\n  3. [노트:S0000337 / 청크:22] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: ■•자원봉사센터'14.5%, •지역사회복지협의'10.3'지역아동센 테방 과후교실）” 9.4%....\n  4. [노트:S0000337 / 청크:23] (제목: 용산구 지역사회복지계획/제2기(2011~2014))\n     내용: \"청소년수련관'7.7%순으로 이용경험을 가지고 있다. 앞으로의 이용의향매너도 •종 합사회복지관\" 이 17.2%로 가장 높게 나타났고、'노인종합복지관\" 15.0%. '자원봉사센터” ...\n  5. [노트:S0000803 / 청크:2] (제목: 2022년 복지소외계층 발굴 및 민간자원연계지원(좋은 이웃들) 사업안내)\n     내용: 홍보활용 \n1)캠페인\n○ 캠페인명 : 좋은이웃들 사업 ‘소외계층 발굴의 날’ 캠페인\n○ 추진배경 : 지역 내 복지소외계층 발굴 상시화 및 좋은이웃들 봉사자의 참여 활성화를 위한 캠...\n=====================\n\n===== 그룹 ID: 6 (임계값: 0.6) =====\n포함된 청크 수: 2\n백링크 후보 노트 ID: ['S0000865']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:5] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, \n3. 인천의료원 \n4. 평촌성심병원 \n5 춘천 성심병원 \n6....\n  2. [노트:S0000865 / 청크:12] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: 서울시립 보라매병원, 3. 인천의료원 4. 평촌성심병원 5....\n=====================\n\n===== 그룹 ID: 7 (임계값: 0.6) =====\n포함된 청크 수: 13\n백링크 후보 노트 ID: ['S0000865', 'S0000954']\n\n--- 포함된 청크 목록 (미리보기) ---\n  1. [노트:S0000865 / 청크:7] (제목: 호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     내용: pneumoniae, Legionella, B. pertussis 등 검사\n- Mycoplasma pneumoniae : PCR과 배양접종 시행. 검체 200μl를 1,800μlH...\n  2. [노트:S0000954 / 청크:0] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 편집순서 6 : 총괄용역과제의 연구결과\n학술연구개발용역과제 연구결과\n1.1 목표\n가. 연구 주제: 줄기세포 연구 활용 촉진을 위한 사람 전분화능줄기세포의 심근세포 분화 표준 프로토...\n  3. [노트:S0000954 / 청크:1] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 나. 인간 전분화능줄기세포에서 심근세포로 분화하는 방법은 배아의 심장 발생 과정과 환경을 모티브로 하는 기법들이 개발되었으며 대표적인 3가지 방법으로 1)배발생 단계의 환경과 유사...\n  4. [노트:S0000954 / 청크:2] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 공배양 기법은 발생학적 관점에서 접근하여, 심장 발생 과정에 중요한 내배엽성세포를 이용한 심근세포의 분화 유도 방법으로, 인간전분화능줄기세포와 내배엽성세포(END2)의 공배양을 통...\n  5. [노트:S0000954 / 청크:3] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 4.2 인간 전분화능줄기세포주별 배양조건 확립 및 분화 능력 탐색\n가. 인간 전분화능줄기세포주별 미분화 유지 배양\n(1)발주부서에서 제공받은 3종의 세포주를 기 확립한 방법인 fe...\n  6. [노트:S0000954 / 청크:4] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 전기 생리학적 특성 분석을 통한 심근세포 기능 분석\n(1)심근세포의 가장 중요한 기능 분석 방법으로 patch clamp 기법을 이용함. Patch clamp 전압고정기법(volt...\n  7. [노트:S0000954 / 청크:5] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 가. 유지배양, 분화, 정제 및 동결 배양액 및 시약 정보\n나. 미분화 인간전분화능줄기세포(CMC-3)유지배양 방법\n(1)미분화 유지배양용 시약 Y27632 제조방법\n(가)구매한 ...\n  8. [노트:S0000954 / 청크:6] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: (다)상온에서 보관 중인 0.5 mM EDTA 용액을 2 ml 처리한 후 37°C 인큐베이터에서 4분동안 EDTA 용액과의 반응을 유도함. (4분간의 반응 시간 동안에 vitron...\n  9. [노트:S0000954 / 청크:7] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 발주부서 연구원에게 CMC3 심근세포 분화 SOP 기술이전 \n가. CMC3 세포주를 이용한 심근세포 분화 기술 개발관련 SOP 확립: SOP 제공 전, 2017.10.18. 발주부...\n  10. [노트:S0000954 / 청크:8] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 2017.10.26. CMC3 세포주를 이용한 인간전분화능줄기세포 유래 심근세포 분화기술 SOP 문서를 제공함. 다....\n  11. [노트:S0000954 / 청크:11] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 마. 교육 및 기술이전 내용: SOP와 동일한 방법으로 CMC3 세포주를 이용하여 미분화배양, 계대배양, 심근세포 분화, 분화된 심근세포(응수축 심근세포)확인, 심근세포 정제 관련...\n  12. [노트:S0000954 / 청크:13] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: SOP 활용 방안: 본 과제의 목적은 국내 심근세포 연구자들에게 활용 가능한 분화 프로토콜을 제공하는 것임. 따라서 구축한 SOP 기술에 대한 소유권은 발주부서에 있다고 판단되며,...\n  13. [노트:S0000954 / 청크:14] (제목: 사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     내용: 세포치료제 개발을 위한 심근세포 프로토콜 개발 전략\n가. 안전성 확보를 위한 프로토콜 개발: Xeno- free 배양을 위한 matrigel 대체 vitronectin, lamin...\n=====================\n\n--- 그룹핑 결과 확인 완료 ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# 필요한 라이브러리 임포트\nimport os\nimport json\nimport time\nimport google.generativeai as genai # Gemini API 사용\nfrom kaggle_secrets import UserSecretsClient # Kaggle Secrets 사용 시\nfrom tqdm.notebook import tqdm\n\n# --- 1. 설정 (Configuration) ---\n\nINPUT_GROUP_FILE = '/kaggle/working/output/grouped_chunks_info.jsonl' # 그룹핑 결과 파일\nOUTPUT_SYNTHESIS_FILE = '/kaggle/working/output/synthesized_notes.jsonl' # 새 글 생성 결과 파일\n\n# LLM 모델 설정 (팀원 코드 참고 또는 다른 모델 선택)\n# LLM_MODEL_NAME = 'gemini-2.0-flash' # 팀원 코드 기준 (모델 목록 확인 필요!)\n# LLM_MODEL_NAME = 'gemini-1.5-flash-latest' # 예시: 1.5 Flash 사용 시\n# 사용 가능한 모델 이름을 정확히 확인하고 설정해야 함\n# 이전 모델 목록 확인 결과를 보면 'models/gemini-2.0-flash' 도 사용 가능함.\nLLM_MODEL_NAME = 'models/gemini-2.0-flash'\n\n# API 호출 관련 설정\nMAX_RETRIES = 3 # API 호출 재시도 횟수\nRETRY_DELAY = 5 # 재시도 간 기본 지연 시간 (초)\n# 입력 텍스트 길이 제한 (토큰 기준, 모델별 제한 확인 필요)\n# 예시: Gemini Flash 모델은 컨텍스트 창이 크지만, 비용/시간 고려하여 적절히 설정\nMAX_INPUT_LENGTH = 100000 # 예시: 최대 입력 글자 수 (토큰이 아닌 글자 수로 단순 제한)\n\n# Gemini API 키 설정 (Kaggle Secrets 사용)\napi_key = None\ntry:\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    if api_key:\n        genai.configure(api_key=api_key)\n        print(\"Gemini API 설정 완료.\")\n    else:\n        print(\"오류: Kaggle 시크릿에서 API 키를 찾을 수 없습니다.\")\nexcept Exception as e:\n    print(f\"API 키 로딩/설정 오류: {e}\")\n\n# LLM 모델 인스턴스 생성\nllm_model = None\nif api_key:\n    try:\n        # 안전 설정 (필요시 조정)\n        llm_safety_settings = [\n            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n        ]\n        # 생성 설정 (필요시 조정)\n        llm_generation_config = {\n          \"temperature\": 0.7, # 약간의 창의성 허용\n          \"top_p\": 1.0,\n          \"top_k\": 1, # top_k=1은 가장 확률 높은 단어만 선택\n          # \"max_output_tokens\": 8192, # 모델별 최대 출력 확인 후 설정\n        }\n        llm_model = genai.GenerativeModel(\n            model_name=LLM_MODEL_NAME,\n            generation_config=llm_generation_config,\n            safety_settings=llm_safety_settings\n        )\n        print(f\"LLM 모델 '{LLM_MODEL_NAME}' 생성 완료.\")\n    except Exception as e:\n        print(f\"LLM 모델 생성 오류: {e}\")\n\nprint(f\"입력 그룹 파일: {INPUT_GROUP_FILE}\")\nprint(f\"출력 통합 노트 파일: {OUTPUT_SYNTHESIS_FILE}\")\n\n# --- 2. LLM 호출 함수 (재시도 로직 포함) ---\n\ndef generate_text_with_llm_retry(prompt, model_instance, max_retries=MAX_RETRIES, delay=RETRY_DELAY):\n    \"\"\"주어진 프롬프트로 LLM API를 호출하고, 오류 시 재시도합니다.\"\"\"\n    if not model_instance:\n        print(\"오류: LLM 모델이 초기화되지 않았습니다.\")\n        return None\n\n    for attempt in range(max_retries):\n        try:\n            # GenerationConfig은 모델 생성 시 지정했으므로 여기서 다시 안 넣어도 될 수 있음\n            response = model_instance.generate_content(prompt)\n\n            # 응답 텍스트 추출 (안전하게)\n            if response.candidates and hasattr(response.candidates[0], 'content') and hasattr(response.candidates[0].content, 'parts') and response.candidates[0].content.parts:\n                 return response.candidates[0].content.parts[0].text.strip()\n            elif response.prompt_feedback and response.prompt_feedback.block_reason:\n                 # 차단된 경우 (안전 설정 등)\n                 raise ValueError(f\"API 응답 차단됨 (사유: {response.prompt_feedback.block_reason})\")\n            else:\n                 # 기타 이유로 응답이 비어있는 경우\n                 print(f\"경고: API 응답이 비어 있거나 예상치 못한 구조입니다. 응답: {response}\")\n                 return \"\" # 빈 문자열 반환\n\n        except Exception as e:\n            error_message = str(e)\n            print(f\"LLM API 오류 (시도 {attempt+1}/{max_retries}): {error_message}\")\n            # Rate Limit 오류 처리\n            if \"429\" in error_message or \"Resource has been exhausted\" in error_message or \"rate limit\" in error_message.lower():\n                wait_time = (delay * (2 ** attempt)) + (hash(prompt) % delay) / 10.0\n                print(f\"Rate limit. {wait_time:.2f}초 후 재시도...\")\n                time.sleep(wait_time)\n            # 모델 찾을 수 없음 오류 (404)는 재시도 불필요\n            elif \"404\" in error_message and \"is not found\" in error_message:\n                 print(f\"오류: 모델 '{LLM_MODEL_NAME}'을(를) 찾을 수 없습니다. 모델 이름을 확인하세요.\")\n                 return None # 즉시 실패 반환\n            # 안전 설정 차단 오류\n            elif \"safety settings\" in error_message.lower() or \"SAFETY\" in error_message:\n                 print(f\"안전 설정 차단. {delay * (attempt + 1)}초 후 재시도...\")\n                 time.sleep(delay * (attempt + 1))\n            # 마지막 시도 실패\n            elif attempt == max_retries - 1:\n                print(\"최대 재시도 횟수 도달. 이 그룹 처리 실패.\")\n                return None\n            # 기타 오류\n            else:\n                time.sleep(delay)\n    return None # 모든 재시도 실패\n\n# --- 3. 그룹핑된 청크 로드 및 새 글 생성 ---\n\nif llm_model: # LLM 모델이 준비되었는지 확인\n    print(f\"\\n--- 그룹핑된 청크 로드 및 새 글 생성 시작 ---\")\n\n    if not os.path.exists(INPUT_GROUP_FILE):\n        print(f\"오류: 그룹 정보 파일 '{INPUT_GROUP_FILE}'을 찾을 수 없습니다.\")\n    else:\n        # 출력 파일 (새 글)은 항상 새로 작성 ('w' 모드)\n        with open(OUTPUT_SYNTHESIS_FILE, 'w', encoding='utf-8') as f_out:\n            processed_group_count = 0\n            # 그룹핑 결과 파일을 한 줄씩 읽기\n            with open(INPUT_GROUP_FILE, 'r', encoding='utf-8') as f_in:\n                # 전체 라인 수를 세어 tqdm에 사용 (선택적)\n                total_lines = sum(1 for _ in f_in)\n                f_in.seek(0) # 파일 포인터를 다시 처음으로\n\n                for line in tqdm(f_in, total=total_lines, desc=\"그룹 처리 중\"):\n                    try:\n                        group_data = json.loads(line)\n                        group_id = group_data.get(\"group_id\")\n                        input_texts = group_data.get(\"synthesis_input_texts\", [])\n                        original_chunks = group_data.get(\"member_chunks\", []) # 원본 청크 정보\n\n                        if not input_texts or group_id is None:\n                            print(f\"경고: 그룹 ID 또는 통합할 텍스트가 없는 그룹 건너뛰기: {group_id}\")\n                            continue\n\n                        # 입력 텍스트 길이 제한 확인 및 처리\n                        combined_text = \"\\n\\n---\\n\\n\".join(input_texts) # 청크 사이에 구분선 추가\n                        if len(combined_text) > MAX_INPUT_LENGTH:\n                            print(f\"경고: 그룹 {group_id}의 입력 텍스트 길이가 너무 깁니다 ({len(combined_text)} > {MAX_INPUT_LENGTH}). 일부만 사용하거나 건너<0xEB><0x9B><0x84>니다.\")\n                            # 처리 방법 선택:\n                            # 1. 앞 부분만 사용: combined_text = combined_text[:MAX_INPUT_LENGTH]\n                            # 2. 건너뛰기: continue\n                            # 여기서는 일단 건너뛰기\n                            continue\n\n                        # --- LLM 프롬프트 설계 ---\n                        # 프롬프트 내용을 필요에 따라 자유롭게 수정/개선하세요.\n                        synthesis_prompt = f\"\"\"\n다음은 서로 의미적으로 관련성이 높은 여러 텍스트 조각(청크)들입니다.\n\n[입력 텍스트 조각 목록]\n{combined_text}\n\n[요청 사항]\n위 텍스트 조각들의 핵심 내용을 종합하고 논리적인 순서로 재구성하여, 하나의 완성된 글을 작성해주세요.\n각 조각의 주요 정보는 유지하되, 자연스러운 문장으로 연결하고 중복되는 내용은 간결하게 정리해주세요.\n새로운 통찰이나 관점을 추가해도 좋지만, 원본 내용에서 크게 벗어나지 않도록 해주세요.\n결과는 완성된 글의 본문만 작성해주세요. (별도의 제목이나 서론/결론 형식 불필요)\n\"\"\"\n\n                        # --- LLM API 호출 ---\n                        synthesized_text = generate_text_with_llm_retry(synthesis_prompt, llm_model)\n\n                        # --- 결과 저장 ---\n                        if synthesized_text is not None: # API 호출 성공 시 (빈 문자열 포함)\n                            result = {\n                                \"group_id\": group_id,\n                                \"original_chunks_info\": original_chunks, # 원본 청크 메타데이터 포함\n                                \"synthesized_text\": synthesized_text,   # LLM이 생성한 글\n                                \"llm_model_used\": LLM_MODEL_NAME        # 사용된 모델 정보\n                            }\n                            json_string = json.dumps(result, ensure_ascii=False)\n                            f_out.write(json_string + '\\n')\n                            processed_group_count += 1\n                        # else: # API 호출 최종 실패 시 이미 함수 내에서 로그 출력됨\n\n                        # API 호출 간 지연 (Rate Limit 방지 - 모델별 RPM 확인 후 조절)\n                        # gemini-2.0-flash 모델의 RPM은 등급에 따라 다르므로 확인 필요\n                        # 무료 등급 15 RPM 가정 시 최소 4초\n                        # 여기서는 이전 설정(30RPM, 2.1초)을 일단 유지 (필요시 수정)\n                        time.sleep(2.1)\n\n                    except json.JSONDecodeError:\n                        print(f\"경고: 그룹 파일의 잘못된 JSON 라인 건너뛰기: {line.strip()}\")\n                    except Exception as e:\n                        print(f\"그룹 처리 중 오류 발생: {e}\")\n\n        print(f\"\\n--- 새 글 생성 완료 ---\")\n        print(f\"총 {processed_group_count}개의 그룹에 대한 통합 글 생성을 완료하여 '{OUTPUT_SYNTHESIS_FILE}'에 저장했습니다.\")\n        print(\"-\" * 40)\n\nelse:\n    print(\"\\n오류: LLM 모델이 초기화되지 않아 새 글 생성을 시작할 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:46:35.533229Z","iopub.execute_input":"2025-04-06T17:46:35.533606Z","iopub.status.idle":"2025-04-06T17:48:24.647477Z","shell.execute_reply.started":"2025-04-06T17:46:35.533582Z","shell.execute_reply":"2025-04-06T17:48:24.646589Z"}},"outputs":[{"name":"stdout","text":"Gemini API 설정 완료.\nLLM 모델 'models/gemini-2.0-flash' 생성 완료.\n입력 그룹 파일: /kaggle/working/output/grouped_chunks_info.jsonl\n출력 통합 노트 파일: /kaggle/working/output/synthesized_notes.jsonl\n\n--- 그룹핑된 청크 로드 및 새 글 생성 시작 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"그룹 처리 중:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d84503dfa645ad9dd0c6f923e1aab4"}},"metadata":{}},{"name":"stdout","text":"\n--- 새 글 생성 완료 ---\n총 15개의 그룹에 대한 통합 글 생성을 완료하여 '/kaggle/working/output/synthesized_notes.jsonl'에 저장했습니다.\n----------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport json\nimport os\nimport textwrap # 긴 텍스트 줄 바꿈용\n\n# 생성된 통합 글 파일 경로\nSYNTHESIZED_FILE = '/kaggle/working/output/synthesized_notes.jsonl'\n# 출력 시 각 청크 미리보기 길이\nCHUNK_PREVIEW_LENGTH = 80\n# 출력 시 생성된 텍스트 미리보기 길이 (0이면 전체 출력)\nSYNTHESIS_PREVIEW_LENGTH = 0 # 0으로 설정하여 전체 텍스트 출력\n\nprint(f\"--- 생성된 통합 글 확인 ({SYNTHESIZED_FILE}) ---\")\n\nif not os.path.exists(SYNTHESIZED_FILE):\n    print(f\"오류: 통합 글 파일 '{SYNTHESIZED_FILE}'을 찾을 수 없습니다.\")\nelse:\n    line_count = 0\n    processed_groups = 0\n    try:\n        with open(SYNTHESIZED_FILE, 'r', encoding='utf-8') as f_in:\n            for line in f_in:\n                line_count += 1\n                try:\n                    data = json.loads(line)\n                    group_id = data.get(\"group_id\", \"N/A\")\n                    original_chunks = data.get(\"original_chunks_info\", [])\n                    synthesized_text = data.get(\"synthesized_text\", \"[내용 없음]\")\n                    model_used = data.get(\"llm_model_used\", \"N/A\")\n\n                    print(f\"\\n===== 그룹 ID: {group_id} (LLM: {model_used}) =====\")\n                    print(f\"--- 원본 청크 ({len(original_chunks)}개) ---\")\n                    if not original_chunks:\n                        print(\"  (원본 청크 정보 없음)\")\n                    else:\n                        for i, chunk_info in enumerate(original_chunks):\n                            note_id = chunk_info.get(\"note_id\", \"?\")\n                            chunk_id = chunk_info.get(\"chunk_id\", \"?\")\n                            title = chunk_info.get(\"doc_title\", \"제목 없음\")\n                            # 미리보기 길이 적용\n                            preview = chunk_info.get(\"text_preview\", \"내용 없음\")\n                            if CHUNK_PREVIEW_LENGTH > 0 and len(preview) > CHUNK_PREVIEW_LENGTH:\n                                preview = preview[:CHUNK_PREVIEW_LENGTH] + \"...\"\n\n                            print(f\"  {i+1}. [노트:{note_id}/청크:{chunk_id}] ({title})\")\n                            # textwrap으로 자동 줄 바꿈\n                            wrapped_preview = textwrap.fill(preview, width=100, initial_indent=\"     \", subsequent_indent=\"     \")\n                            print(wrapped_preview)\n\n\n                    print(\"\\n--- LLM 생성 통합 글 ---\")\n                    # 미리보기 길이 적용 (0이면 전체 출력)\n                    if SYNTHESIS_PREVIEW_LENGTH > 0 and len(synthesized_text) > SYNTHESIS_PREVIEW_LENGTH:\n                        synthesized_text_display = synthesized_text[:SYNTHESIS_PREVIEW_LENGTH] + \"\\n... [내용 더보기]\"\n                    else:\n                        synthesized_text_display = synthesized_text\n\n                    # textwrap으로 자동 줄 바꿈\n                    wrapped_synthesis = textwrap.fill(synthesized_text_display, width=100, initial_indent=\"  \", subsequent_indent=\"  \")\n                    print(wrapped_synthesis)\n\n                    print(\"=\" * (len(str(group_id)) + 25)) # 구분선\n                    processed_groups += 1\n\n                except json.JSONDecodeError:\n                    print(f\"\\n오류: 잘못된 JSON 라인 발견 ({line_count}번째 라인)\")\n                except Exception as parse_err:\n                    print(f\"\\n오류: 그룹 데이터 처리 중 오류 발생 ({line_count}번째 라인): {parse_err}\")\n\n            print(f\"\\n총 {processed_groups}개의 그룹에 대한 통합 글 출력을 완료했습니다.\")\n\n    except Exception as e:\n        print(f\"통합 글 파일 읽기 중 오류 발생: {e}\")\n\nprint(\"\\n--- 통합 글 확인 완료 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:50:51.222983Z","iopub.execute_input":"2025-04-06T17:50:51.223322Z","iopub.status.idle":"2025-04-06T17:50:51.276891Z","shell.execute_reply.started":"2025-04-06T17:50:51.223300Z","shell.execute_reply":"2025-04-06T17:50:51.275938Z"}},"outputs":[{"name":"stdout","text":"--- 생성된 통합 글 확인 (/kaggle/working/output/synthesized_notes.jsonl) ---\n\n===== 그룹 ID: 0 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000105/청크:1] (횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     연구배경 및 목적 1. 연구의 배경민원행정서비스에 대한 만족도 조사는 한국행정연구원에서 1996년 조사모델과 방법을 개발한 이후 현재까지 지속적...\n  2. [노트:S0000105/청크:3] (횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     11. 30(35일) ▷ 공간적 범위 - 횡성군 종합민원실 ▷ 내용적 범위 - 서론(연구 개요로서 연구의 배경과 목적 및 범위 및 방법) - 민...\n\n--- LLM 생성 통합 글 ---\n  본 연구는 횡성군 종합민원실에서 제공하는 민원행정서비스에 대한 고객 만족도를 조사하고, 이를 바탕으로 고객 중심의 행정 서비스 체계를 구축하여 서비스 질적 향상에 기여하는 것을\n  목표로 한다. 1996년부터 한국행정연구원에서 개발한 모델을 기반으로 지속적으로 수행되어 온 민원행정서비스 만족도 조사의 일환으로, 횡성군 종합민원실 이용객을 대상으로 민원행정\n  서비스 전달체계, 공무원 행태, 민원 제도 등에 대한 체감 만족도를 측정한다. 특히, 상반기 조사 결과와 비교 분석하여 하반기 고객 만족도 수준을 평가하고, 불합리한 제도 개선\n  및 발전 방향을 제시하며, 맞춤형 민원행정 서비스 및 전자민원 행정 구현을 위한 기초 자료를 확보하고자 한다. 궁극적으로는 횡성군 종합민원실이 고객 중심의 행정 서비스를\n  구현하고, 행정 서비스의 질을 향상시켜 횡성군의 경쟁력 제고에 이바지하는 것을 목표로 한다.  조사는 2006년 11월 2일부터 11월 15일까지 10일간 횡성군 종합민원실에서\n  민원행정서비스를 이용한 524명의 민원인을 대상으로 진행되었다. 민원 대행 업무를 보는 기관이나 단체는 조사 대상에서 제외되었다. 전체 민원행정서비스 이용자를 대상으로 하되,\n  민원 분야별로 균형 있는 결과 도출을 위해 가능한 범위 내에서 샘플을 통제하고자 하였다. 최초 500명에서 524명으로 조사 대상이 증가한 이유는 상대적으로 낮은 샘플 수를\n  기록한 업무 분야에 대한 조사를 추가적으로 실시하기 위함이었다.  설문 조사는 사전 교육을 수료한 2명의 조사원이 1:1 면접식 조사 방법과 자기기입식 방법을 병행하여\n  실시되었다. 설문 참여자에게는 소정의 기념품이 제공되었다. 설문지는 민원시설, 서비스, 민원 처리, 인구통계적 요소 등 총 22개 항목으로 구성되었으며, 중앙행정기관 및\n  광역지방자치단체의 만족도 조사에 이용된 항목을 참고하여 작성되었다. 특히, 상반기 조사에 활용된 설문 항목에서 분야별 종합 만족도에 대한 설문 항목은 제외하고, 업무 담당자와\n  민원 처리 부서의 업무 처리 방법에 대한 설문을 추가하였다. 설문 문항은 Likert 5 level scale을 사용하여 등간 척도로 구성되었다.  수집된 자료는 SPSS\n  for win 12.0 통계 패키지를 활용하여 분석되었다. 기술 통계, T-test, 분산 분석(ANOVA) 등의 통계적 방법을 적용하여 분석하였으며, 만족도 산정 방식은\n  항목별로 5점 척도를 사용하고 이를 100점 척도로 환산하였다. 연구는 서론(연구 배경, 목적, 범위 및 방법), 민원 만족도 조사(민원서비스 시설, 민원업무 담당 공무원의\n  친절도, 민원처리 내용 중심), 설문 결과 분석, 상반기 조사 결과와의 비교 분석, 종합 평가 및 발전 방안 등의 내용으로 구성되었다.\n==========================\n\n===== 그룹 ID: 1 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (3개) ---\n  1. [노트:S0000105/청크:8] (횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     6. 분산분석(ANOVA; analysis of variance)결과 가. 연령별 분산분석 1)연령별 민원시설관련 분산분석응답자들의 연령대별 평...\n  2. [노트:S0000105/청크:9] (횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     나. 민원분야에 따른 분산분석 1)민원시설관련 만족도 분산분석응답자들이 제공받은 민원서비스 분야별 접근성에 대한 만족도는 평균 72.70점으로,...\n  3. [노트:S0000105/청크:10] (횡성군 민원행정 고객만족도 설문조사 용역보고서 2006.하반기)\n     종합 평가 1. 민원행정 시설에 대한 만족도 ▷ 민원행정 시설에 대한 종합만족지수는 70.33점으로 상반기에 비해 4.32점이 높아졌음. ▷ 접...\n\n--- LLM 생성 통합 글 ---\n  민원 서비스 만족도 하반기 평가 결과, 시설, 안내 서비스, 민원 처리 전반에서 상반기 대비 향상된 것으로 나타났다.  **1. 민원행정 시설 만족도:** 종합만족지수는\n  70.33점으로 4.32점 상승했다. 접근성 만족도는 소폭 하락했으나 유의미한 수준은 아니었다. 주차장 이용 만족도는 63.79점으로 가장 낮았으며, 편의시설 만족도는\n  73.76점으로 가장 높았다. 특히, 연령별 분석에서 주차장 만족도는 전반적으로 낮게 평가되었고, 60대 이상은 편의시설에 높은 만족도를 보였다. 민원창구 테이블에 대한\n  만족도는 거주지별 분석에서 가장 낮은 수준으로 나타나 개선 필요성이 제기되었다.  **2. 민원안내 서비스 및 친절도:** 종합만족도는 4.18점 상승했다. 안내 서비스\n  담당자의 친절도는 70.32점, 민원창구 담당자의 친절도는 72.42점으로 높게 평가되었다. 하지만 민원 신청 및 처리 절차에 대한 안내 서비스 만족도는 상대적으로 낮았다.\n  온라인 상담 친절도 만족도는 67.22점으로 개선되었으나 추가적인 보완이 필요하다. 민원분야별 분석에서는 온라인 민원 상담 만족도가 가장 낮게 평가되어 온라인 민원 안내 및\n  처리에 대한 대책 마련이 시급하다.  **3. 민원행정 처리:** 종합 만족도는 65.30점에서 70.16점으로 약 5점 상승했다. 창구 담당자들의 설명에 대한 만족도가 크게\n  향상되었으며, 공무원의 공정성에 대한 만족도는 70.47점으로 비교적 높았다. 민원 처리 신속성 항목은 70.27점으로 만족 수준을 보였다. 다만, 민원 신청 및 처리 절차\n  안내 관련 만족도는 68.85점으로 상대적으로 낮았다. 민원분야별 분석 결과, 민원인들은 민원 신청 및 처리 절차 파악 정도에 대해 낮은 만족도를 보였다. 특히, 부동산 관련\n  민원의 경우 처리 과정의 복잡성으로 인해 만족도가 낮게 나타났다. 그러나 민원처리의 신속성과 공정성은 70점 이상으로 평가되었고, 담당 공무원들의 자리 배치 또한 만족도 제고에\n  기여한 것으로 분석되었다. 거주지별 분석에서는 민원 처리 신속성에 대한 평가가 75점 이상으로 매우 높게 나타났다.  전반적으로 민원 서비스 만족도가 향상되었으나, 주차장,\n  온라인 상담, 민원 절차 안내 등 특정 영역에서는 개선 노력이 필요하다. 특히, 민원인들이 민원 신청 및 처리 절차를 쉽게 이해할 수 있도록 정보 제공 방식을 개선하고, 온라인\n  민원 서비스 품질 향상을 위한 노력이 요구된다.\n==========================\n\n===== 그룹 ID: 2 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (3개) ---\n  1. [노트:S0000286/청크:0] (전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     전자기록물 장기보존패키지 - 제2부: 디렉토리로 구조화된 방식(NEO3) 1. 적용범위 이 표준은 디지털객체에 대한 논리적 또는 물리적 캡슐화 ...\n  2. [노트:S0000286/청크:7] (전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     인증기관에서 발급한다. 3.8 인코딩(Encoding) 기계적 처리를 위해 고안된 구문 또는 신호를 특정한 부호들의 나열로 그 형태를 바꾸는 것...\n  3. [노트:S0000286/청크:10] (전자기록물 장기보존패키지 기술규격 – 제2부 디렉토리로 구조화된 방식(NEO3))\n     영구기록물관리기관 등은 디렉토리로 구조화된 방식을 채택하는 경우라 하더라도 기관의 환경 등에 따라 NEO3가 아닌 다른 캡슐화 방식을 정의하고 ...\n\n--- LLM 생성 통합 글 ---\n  전자기록물의 장기보존은 오랜 시간이 경과한 후에도 전자기록물에 접근 가능하고 진본 상태를 유지하여 증거로서 인정받을 수 있도록 하는 보존 행위이다. 이를 위해 전자기록물의\n  원문, 보존 포맷, 메타데이터, 진본확인 정보를 하나의 논리적 또는 실제적 객체로 캡슐화한 장기보존패키지(Long-term preservation package)가 활용된다.\n  「공공기록물 관리에 관한 법률」에 따라 공공기관 및 기록물관리기관은 전자기록물을 장기간 보존해야 하며, 이때 전자기록물 장기보존패키지 표준(NEO3)을 적용할 수 있다. 다만,\n  영구기록물관리기관 등은 소장 기록물의 특성, 기관의 환경 등에 따라 다른 캡슐화 방식을 적용할 수 있다. NEO3 표준은 디지털 객체에 대한 논리적 또는 물리적 캡슐화 규격을\n  규정하고, 진본성, 무결성, 이용가능성을 유지하기 위한 기술 규격을 기술한다. NEO3에서 제시되는 장기보존패키지는 ISO 14721 OAIS(Open Archival\n  Information System) 참조 모형에서 말하는 기록관에서 영구기록물관리기관으로 전자기록물을 이관하는 포맷(SIP), 이용자에게 전자기록물을 제공하는 포맷(DIP)\n  등으로 활용될 수 있다.  NEO3 장기보존패키지 방식은 기관이 정한 규칙에 따라 계층적 디렉토리 구조로 연관된 파일들을 분류·관리하며, 최상위 디렉토리를 기준으로 ZIP64\n  등으로 압축하거나 압축 없이 하나의 디지털 객체로 묶어 보관·활용할 수 있다. 장기보존패키지 내 모든 파일에 대한 논리적 구성은 패키징 정보에 포함되어 파일에 대한 접근과\n  확인을 가능하게 한다. 또한, 전자서명용 인증서나 이력 정보 등 부속 파일의 해시 값과 파일 크기 정보를 패키징 정보에 포함하면 파일의 무결성을 확인할 수 있다. 패키징 정보에\n  포함되는 파일의 경로 정보는 URL, URI, URN 등의 정보로 원격지의 위치를 포함할 수 있어 실제적으로 캡슐화하지 않은 논리적 장기보존패키지로 활용할 수도 있다.\n  OAIS 정보 패키지의 보존기술정보(Preservation Description Information)에 해당하는 구성요소의 명칭을 장기보존 메타데이터로 한 것을 제외하고는\n  모두 OAIS 정보 패키지의 구성요소와 명칭을 따른다. NEO3 장기보존패키지에는 검색이나 추출 기능에 사용되기 위한 메타데이터(기술 정보)를 포함할 수 있다. 원문 및 보존\n  포맷 등 원본 기록물 파일이나 물리적 객체에 대한 정보는 콘텐트 정보에 해당한다. 출처 정보, 맥락 정보, 참조 정보, 고정 정보, 접근 권한 정보 등은 장기보존 메타데이터에\n  포함되며, 진본확인 정보, 이력 정보 등을 추가적으로 포함할 수 있다. 장기보존패키지를 논리적으로 구성하기 위해서는 패키징 정보에 DOI 시스템 등 객체 위치 독립적인\n  식별체계를 기반으로 할당된 식별자를 반드시 포함하고 관리해야 한다.  패키징 정보는 장기보존패키지 내 모든 정보 구성 요소를 묶는 역할을 하며, 원문 및 보존 포맷에 대한\n  콘텐트 정보와 장기보존 메타데이터를 논리적으로 묶고, 장기보존패키지에 포함되는 추가 부속 파일에 대한 정보를 목록화한다. 또한, 장기보존패키지가 수정될 경우 이력 정보에 대한\n  경로 또는 식별자를 포함하여 파일의 무결성 확인 및 진본 확인을 위한 정보로 활용된다. 기술 정보는 장기보존패키지에 대한 메타데이터로, 기록물관리시스템 등에서 검색이나 추출\n  기능을 적용할 때 활용될 수 있도록 선택적으로 추가할 수 있다.  원문은 생산자가 생산 또는 접수한 기록물 원본(진본)을 의미하며, 업무 활동의 증거로서 법적 가치를 가진다.\n  전자기록물의 경우 위변조가 쉽고 계속 변화하므로, 진본 확인을 위해 적법한 절차에 따라 관리되어 온 과정 등이 메타데이터를 통해 관리되어야 한다. 보존 포맷은 원문이 생성된\n  당시의 애플리케이션 없이도 해당 원문의 내용과 외형을 그대로 재현하거나 기술적 복원이 가능하게 하는 포맷이다. 원문을 변환할 적합한 보존 포맷이 없거나 에뮬레이션 등 다른 보존\n  방식으로 재현이 가능한 경우 장기보존패키지에 보존 포맷을 포함하지 않을 수 있다.  전자기록물의 장기보존을 위해서는 기록물 원문과 함께 기록물의 생산부터 관리, 보존에 이르는\n  전 과정을 기술한 정보(장기보존 메타데이터)를 보존해야 한다. 장기보존패키지 자체에 대한 정보(패키지명, 버전 정보 등), 기록물관리의 전 과정에 관한 메타데이터(기록물철,\n  기록물건 메타데이터 등), 진본확인을 위한 정보(전자서명 정보 등), 재변환 전후 관계에 관한 정보 등이 관리되어야 한다. NEO3의 경우 패키징 정보, 장기보존 메타데이터,\n  콘텐트 정보 등을 하나의 XML 문서 내에 포함하여 데이터를 간단하게 처리할 수 있도록 구성한다. NEO3는 기록관리 메타데이터의 계층과 장기보존 메타데이터 계층이 차이 없이\n  구성된다.  기록물철 장기보존 메타데이터는 객체 메타데이터, 객체 콘텐트 등으로 구성되며, 기록물철 메타데이터는 생산자, 기록 식별자, 기록물명 등 15여 개의 상위 요소를\n  가진다. 전자서명 정보 생성은 기록물철 장기보존패키지의 장기보존 메타데이터를 대상으로 한다. 또한, 장기보존패키지에 존재하는 모든 컴포넌트의 해시 값, 식별자 등을 포함하는\n  콘텐트 정보에 대한 전자서명을 통해 진본성을 확인할 수 있다. NEO3 장기보존패키지에 포함되는 진본확인 정보, 원문 및 보존 포맷은 디렉토리 내에 계층적으로 관리되고\n  ZIP64 등으로 압축되어 보관될 수 있어 별도의 인코딩이나 처리가 불필요하며, 디렉토리 내 각종 부속 파일의 논리적 연계 정보가 장기보존 메타데이터에 포함되어 실제적으로 또는\n  논리적으로 캡슐화가 가능하다. 영구기록물관리기관 등은 기관 환경, 기록물 특성 등에 따라 전자서명 외의 진본확인 기술을 적용할 수 있다.\n==========================\n\n===== 그룹 ID: 3 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000337/청크:11] (용산구 지역사회복지계획/제2기(2011~2014))\n     〈표3. 2 _ 2 4 ＞ 는 구직 희망자 재취업 교육훈련프로그램 필요여부를 살펴보았다. 분석결과 •필 요하다\" 가 3 0. 5 %로 가장 높은...\n  2. [노트:S0000337/청크:12] (용산구 지역사회복지계획/제2기(2011~2014))\n     서비스직” 1 0. 3 %, ••임시직. 단순노무직'1 0. 3 % 순으로 나타 났다. 〈표3.2-26〉는 구직 형식을 살펴보았다. 분석결과 *...\n\n--- LLM 생성 통합 글 ---\n  구직 희망자 대상 재취업 교육훈련프로그램 필요성에 대한 조사 결과, 응답자의 30.5%가 \"필요하다\", 30.5%가 \"매우 필요하다\"고 응답하여 높은 관심을 보였다. 구직 희망\n  분야로는 자영업이 36.2%로 가장 높게 나타났으며, 전문직/준전문직 19.0%, 판매/서비스직, 임시직/단순노무직이 각각 10.3%로 뒤를 이었다. 구직 형태 역시 자영업을\n  희망하는 비율이 43.1%로 가장 높았고, 풀타임 정규직 20.7%, 파트타임/반나절 임시직 17.2% 순으로 나타났다.  취업 활성화를 위한 요구사항으로는 \"주민을 위한\n  일자리를 만들어야 한다\"는 의견이 47.5%로 가장 많았으며, 취업정보센터 확대(18.4%), 직업훈련/자격증 교실 확대(15.2%), 주민창업기금 지원(8.2%), 보육시설\n  및 방과후 교실 확대(8.2%) 등이 뒤를 이었다.  한편, 일반 구민 조사 결과, 응답자의 29.7%가 가족 중에 아픈 사람이 \"있다\"고 응답했다. 아픈 가족 구성원으로는\n  본인(배우자)이 60.9%로 가장 많았고, 노부모 21.9%, 자녀 15.6% 순이었다. 아픈 사람의 성별은 여성 50.0%, 남성 46.9%로 비슷하게 나타났으며, 연령대는\n  19~64세가 62.5%로 가장 높은 비율을 차지했고, 65세 이상 31.3%, 7~18세 3.1% 순이었다.\n==========================\n\n===== 그룹 ID: 4 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (4개) ---\n  1. [노트:S0000473/청크:0] (『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     『최저임금 적용효과에 관한 실태조사』품질개선 컨설팅 최종결과보고서  요약 「최저임금 적용효과에 관한 실태조사」는 최저임금이 기업경영과 고용에 미...\n  2. [노트:S0000473/청크:1] (『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     최저임금 등 임금분포와 조사대상 ○ 최저임금과 비교되는 시급 임금분포와 최저임금의 중위임금에 대비한 상대적 수준 추이는'최저임금 적용효과에 관한...\n  3. [노트:S0000473/청크:3] (『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     조사대상 업종 및 사업체 규모 범위 확대 〇 현행 「최저임금 적용효과에 관한 실태조사」의 모집단은 저임금 근로자의고용 비중이 높은 업종(C, G...\n  4. [노트:S0000473/청크:4] (『최저임금 적용효과에 관한 실태조사 』 품질개선 컨설팅 최종결과보고서)\n     ○ 조사의 용이성 - 조사 표본이 수월하게 확보될 수 있도록 설정되어야 함 ⇒ 각 기준들이 서로 상충될 수도 있음: 예를 들어, 국제적으로 통용...\n\n--- LLM 생성 통합 글 ---\n  「최저임금 적용효과에 관한 실태조사」는 최저임금이 기업경영과 고용에 미치는 영향, 그리고 사용자와 근로자가 느끼는 최저임금 수준의 만족도 등을 파악하여 최저임금 심의의\n  기초자료로 활용하는 것을 목적으로 한다. 그러나 기존 조사에서는 조사대상 선정 기준, 조사 체계, 조사 방법 등에서 개선이 필요한 것으로 파악되었다. 이에 본 연구는\n  2020~2021년 선행 연구 및 조사 결과를 바탕으로 조사 목적에 부합하는 조사대상 기준, 조사표 설계, 조사 체계 및 조사 방법의 적절성을 검토하고, 표본설계 전문가\n  자문회의를 통해 품질 개선 방안을 제시하고자 한다.  **1. 조사대상 선정 기준 개선**  현재 「최저임금 적용효과에 관한 실태조사」는 조사 대상 사업체와 근로자를\n  ‘최저임금액의 1.5배 이하를 받는 저임금 근로자와 이들을 고용하는 사업주’로 설정하고 있다. 그러나 최저임금이 지속적으로 상승하면서 최저임금의 1.5배가 최근에는 거의\n  중위임금 수준에 다다랐다는 점을 고려할 때, 조사대상 근로자에 대한 보다 명확하고 합리적인 기준 설정이 필요하다. 이에 본 연구에서는 세 가지 방안을 제시한다.  *\n  **제1안: 현행 유지 (최저임금의 1.5배 이하 근로자)** 통계 작성의 일관성을 유지할 수 있다는 장점이 있지만, 최저임금 상승 추세에 따라 조사 대상인 '저임금' 근로자에\n  대한 논란이 지속될 수 있다. *   **제2안: 중위임금 이하 근로자** 중위임금은 임금 분포 상의 지표이므로 이해하기 쉽고, 최저임금 변동과 상관없이 일정 수준의 근로자를\n  조사 대상으로 유지할 수 있다는 장점이 있다. *   **제3안: 중위임금의 2/3 이하 근로자** 이는 국제적으로 통용되는 통상적인 저임금 근로자의 정의에 부합하지만, 조사\n  대상이 협소해져 사업체 및 근로자 추출에 현실적인 어려움이 예상된다.  각 방안의 장단점을 고려하여 현실에 맞는 조사대상 근로자 선정 방안을 정책적으로 결정해야 할 것이다.\n  **2. 조사 범위 확대 및 현실적 제약 고려**  현재 10개 업종에 대해 조사하고 있는 업종의 포괄 범위를 17개 업종으로 확대하여 더 다양한 업종에서 최저임금 관련 인식을\n  조사하는 것은 바람직하다. 다만, 일부 업종에서는 저임금 근로자 수가 적어 조사대상 확보가 어려울 수 있으므로, 조사의 수월성을 고려하여 점진적으로 확대해 나가는 것이\n  합리적이다. 같은 논리로, 조사대상 기업체 규모 확대 문제도 현실적인 어려움을 감안하여 결정해야 한다.  **3. 표본 설계 개선**  표본 추출 틀은 현재와 같이\n  고용형태별근로실태조사와 사업체노동실태현황을 활용한 2중 추출 형식을 유지하는 것이 불가피하다. 표본 배분 방식은 현재 업종 및 기업 규모를 기준으로 비례 배분하는 방식에서\n  제곱근 비례 배분 방식으로 변경하고, 표본 오차를 고려하여 사업체 조사 목표 표본 수를 현행 3,000개에서 3,500개로 확대하는 것이 바람직하다. 또한, 업종 및 기업 규모\n  등의 층화를 바탕으로 가중치를 적용하여 모수 추정이 가능하도록 해야 한다.  **4. 조사표 구성 개선**  조사표 구성과 관련하여, 조사 대상 근로자 수는 ‘상용근로자 수’를\n  기준으로 하는 반면, 사업체 대상 조사표에서는 일용/기간제 근로자까지 포함하고 있는 상황이므로 응답 대상을 일치시키도록 해야 한다. 또한, 근로자 응답 대상자 선정과 구분이\n  용이하도록 사업체 조사표에서 관련 정보가 도출되도록 할 필요가 있다. 사업체의 정보 확인 및 기타 행정 자료 연계를 위해 사업체 조사표에 사업자 등록 번호를 추가하고, 시간\n  흐름 순으로 응답란이 제시되도록 하는 것이 좋다.  **5. 조사 체계 변경 및 전문성 강화**  그동안 고용노동부 지방관서의 근로감독관 주도 하에 임시 조사원을 통해\n  조사되었던 조사 체계를 조사 전문 기관이 보다 체계적으로 조사하도록 변경하는 것은 바람직하다. 이는 근로감독관의 부담을 경감시키고, 조사원 채용/교육/관리 측면에서 전문성을\n  보유한 전문 조사 기관이 담당함으로써 조사의 효율성을 제고할 수 있을 것으로 기대된다. 다만, 조사 체계 변경 시에도 기업체들이 조사에 협조할 수 있도록 근로감독관과의 협력이\n  필요하며, 조사 불응 또는 비협력 사업체에 대해서는 근로감독관이 조사 지원 업무를 수행하는 것이 불가피하다. 전문 조사 기관에서는 조사표 회수 후 바로 전산 입력, 내용 검토,\n  전산 처리하고 필요시 보완 조사를 실시하여 항목 무응답 및 분석 불가능 조사표를 최소화해야 한다.  **6. 조사지침서 개선**  조사 우선순위에 대한 기준이 모호하여 조사원에\n  따라 다른 조사 대상을 조사할 가능성이 높으므로, 명확한 조사 우선순위 기준을 설정해야 한다. 또한, 표본 대체는 동일한 층 내에서만 이루어져야 하며, 조사자가 임의로 표본\n  추출 틀 외의 사업체 표본을 선정하는 것을 방지하기 위해 원표본 외 대체 표본을 준비하고 명확한 표본 대체 지침에 따라 표본을 대체하여 조사해야 한다.  **7. 결론 및\n  제언**  합리적인 포괄 범위 설정을 위해서는 객관성, 일관성, 목적 부합성, 조사의 용이성이라는 기준을 세우고, 실증적인 분석 결과들을 참고하여 방안들을 마련하고, 각\n  대안들의 장단점들을 종합적으로 고려한 후 조사의 목적에 부합하는 가장 현실적으로 선호되는 방안을 찾아내는 과정이 필요하다. 조사 대상 업종 및 사업체 규모를 확대하는 데\n  있어서는 현실적인 제약들을 고려해야 하며, 표본 설계 및 조사표 구성을 개선하여 조사의 정확성과 효율성을 높여야 한다.\n==========================\n\n===== 그룹 ID: 5 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (6개) ---\n  1. [노트:S0000474/청크:0] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     『여성농업인실태조사』품질개선 컨설팅 최종결과보고서 Final Report on Quality Improvement Consulting for 『...\n  2. [노트:S0000474/청크:3] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     그리고 다문화가정여성 또한 실제 가구수 비율(1.2%)보다 많은 250가구(12.5%)로 할당을 함 - 다문화가구와 귀농여성의 실제 가구수 비율...\n  3. [노트:S0000474/청크:6] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     □ 조사표 개선 방향 도출 ○ 조사대상 재설정 - 농업을 주업으로 하면서, 농업외 소득활동이 있는 겸업 여성농업인을 포함하기 위해 여성농업인의 ...\n  4. [노트:S0000474/청크:8] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     유사통계 개요 여성농업인실태조사와 관련있는 15종의 승인통계(주로 조사통계)들을 검토하였다. 크게 여성관련 조사 6종, 농업관련 조사 4종, 귀...\n  5. [노트:S0000474/청크:9] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     그 외 조사의 경우 부록3을 참고하도록 한다. 제 2 절 유사통계 검토결과 1. 조사대상 남성 포함 여성농업인 실태를 파악하기 위해 현재의 조사...\n  6. [노트:S0000474/청크:10] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     따라서 여성관리자패널조사는 제외한 나머지 유사통계 및 농림축산식품부의 귀농귀촌실태조사의 예산을 비교 검토하였다. - 2018 여성농업인 실태 조...\n\n--- LLM 생성 통합 글 ---\n  여성농업인실태조사는 여성농어업인육성법 및 통계법에 근거한 승인통계(제114036호)로서, 여성농업인의 전반적인 현황을 파악하여 통계 이용자 수요에 부응하고, 정부의 여성농업인\n  육성 5개년 계획의 중간 평가 및 관련 정책 수립을 위한 기초 자료로 활용되는 것을 목적으로 한다. 2003년 최초 실시 이후 5년 주기로 조사가 이루어졌으며, 조사 기간,\n  표본 설계 변경, 조사 항목 수정 등의 이유로 통계 작성 변경 승인을 받은 바 있다. 2018년 조사 결과 검토 결과, 표본 규모 확대, 표본 설계 개선, 유사 통계 비교,\n  조사표 설계 개선 등이 필요한 것으로 파악되었다.  이에 본 연구는 여성농업인실태조사의 품질 개선을 위해 2018년 조사 결과와 기존 표본 설계를 바탕으로 새로운 표본 설계\n  방안, 적절한 모수 추정 방안, 조사 목적에 부합하는 조사표 및 조사 항목 등 조사표 설계 개선안을 마련하고자 한다. 구체적인 목표는 조사 대상 변경 및 확대 검토, 이를\n  반영한 표본 크기 검토 및 제안, 표본 추출 방법 개선 방안, 표본 추출 방법에 따른 모수 추정치 및 가중치 제시이다.  이를 위해 다음과 같은 개선 방향을 설정했다. 첫째,\n  조사 목적을 충실히 달성하기 위해 합리적인 표본 추출 및 표본 배분 원칙과 실사의 효율성을 높인다. 둘째, 최신 모집단 분석 결과와 2018년 조사 데이터 분석 결과를 토대로\n  표본 배정을 위한 층화 변수 등을 찾고, 다양한 표본 배분 방법을 검토하여 조사 여건을 고려해 최종 결정한다. 셋째, 가중치 부여 과정을 포함한 모수 추정식과 추정식의 오차\n  계산 공식을 제시한다.  현재 여성농업인실태조사는 일반 여성농업인, 귀농 여성농업인, 다문화 가정 여성을 대상으로 각각 별도의 조사표와 조사 항목을 사용하고 있다. 새로운 표본\n  설계에서는 일반 여성농업인과 귀농 여성농업인 실태조사를 통합하고, 지역을 층화 기준으로 구축된 조사 모집단에 대한 충분한 분석을 통해 모집단의 특성을 잘 반영한 효과적인 표본\n  배분 방법과 표본 추출 방법을 제시한다. 정확한 통계 산출을 위해 표본 설계에 따라 설계 가중치와 조사 과정에서 발생하는 무응답 조정을 위한 무응답 조정 가중치 등을 적용한\n  가중치를 부여하는 과정을 구체적으로 기술하고, 모수 추정식을 제시한다. 다문화 여성 실태조사는 조사 목적에 적합하지 않아 조사 대상에서 제외한다.  조사 대상을 재설정하여\n  농업을 주업으로 하면서 농업 외 소득 활동이 있는 겸업 여성농업인을 포함하기 위해 여성농업인의 정의를 재설정할 필요가 있다. 조사표 중복성 및 일반 여성농업인 조사의 대표성\n  확보를 위해 일반 여성농업인, 귀농 여성농업인을 통합하여 조사하고, 다문화 여성의 경우 타 조사로 대체하거나 별도 조사하는 것이 바람직하다. 응답자 기본 사항에 출신 국가란,\n  귀농 여부 추가가 필요하며, 농외 소득 각 분야에서 얻는 소득 규모 및 활동 기간 추가가 필요하다. 복지 현황 조사 항목은 농어업인 등에 대한 복지 실태 조사 항목과 중복되므로\n  삭제 가능하며, KOSIS 공표 항목인 ‘필요 여성 시설’의 경우 존치하는 것이 바람직하다. 양성평등 조사 항목을 추가하여 여성농업인의 양성평등 의식, 가족 가치관 및\n  가족생활, 인권과 안전 등에 대한 실태를 파악하는 것이 바람직하다.  여성농업인 실태 조사의 주요 유사 통계인 농어업인 등에 대한 복지 실태 조사, 전국 다문화 가족 실태\n  조사, 양성평등 실태 조사, 여성 관리자 패널 조사의 작성 현황을 비교 검토한다. 유사 통계 검토 결과, 실태 파악 및 조사 목적 달성을 위해 조사 대상의 비교군으로 조사 대상\n  범위를 확장하여 포함한 사례들이 있는 것으로 나타났다. 여성농업인 실태를 파악하기 위해 현재의 조사 대상인 여성뿐 아니라 남성 역시 조사 대상으로 확장하여 포함할 필요가 있는지\n  검토한다.  2018년 여성농업인 실태 조사는 일반 여성농업인 1,500가구, 귀농 여성농업인 250가구, 다문화 여성 250가구, 총 2,000가구를 조사하였으며, 약 8천만\n  원의 예산이 배정되었다. 조사 비용은 건당 약 4만 원이다. 다른 유사 조사와 비교할 때 조사 비용이 낮은 것을 알 수 있다. 또한 조사 규모 역시 일반 가구를 대상으로 하는\n  타 조사의 절반에 미치지 못하는 것으로 나타난다. 따라서 조사의 정확성 및 대표성 확보를 위한 표본 수 확대와 적정 수준으로의 예산 증액이 필요한 것으로 여겨진다.\n  여성농업인실태조사의 목표 모집단은 우리나라에 거주하는 여성농업인이지만, 여성농업인 통계가 존재하지 않으므로 농림어업총조사 가구 명부를 표본 추출 틀로 하여, 농가 내 적격\n  대상자를 조사하기로 한다. 즉 조사 모집단은 2020년 농림어업총조사의 농가 내 15세 이상 여성 중 농업에 종사하는 자이다. 표본 추출 틀은 최신 모집단을 반영하기 위하여\n  2020년 농림어업총조사의 103만 5,193가구를 대상으로 한다. 여성농업인실태조사의 조사 항목은 인적 사항, 기본 사항, 가구 현황, 경제·사회 활동, 교육, 복지, 정책\n  수요 및 향후 계획의 7개 영역으로 구성된다.\n==========================\n\n===== 그룹 ID: 6 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000474/청크:4] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     1. 표본설계 기본현황 - 일반여성의 경우, 1500명의 표본이 모집단인 조사시점 우리나라 동지역을 제외한 읍/면 지역에 거주하는 만 18세 이...\n  2. [노트:S0000474/청크:14] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     표본 감소로 인한 제약에 따라일부 지역들을 제외하여 전체 모집단의 지역별 특성을 충분히 반영하지 못하였으며, 일반여성농업인의 표본비율이 전체 모...\n\n--- LLM 생성 통합 글 ---\n  여성농업인실태조사의 표본 설계는 일반 여성농업인 1,500명, 귀농 여성 및 다문화 여성 각 250명을 대상으로 한다. 일반 여성농업인은 읍/면 지역 거주 18세 이상 농가\n  여성을 대표하도록 시군별 농가 수 비례 할당 표본추출을 실시하며, 시군 추출은 층화집락추출법을 활용한다. 최종 표본은 연령/작목 등을 고려하여 특정 계층에 편중되지 않도록\n  설정된다. 귀농 여성 및 다문화 여성은 광역시도별 인구 비례 할당 방식으로 표본을 추출한다.  그러나 현재 표본 설계는 몇 가지 문제점을 안고 있다. 표본 감소로 인해 모집단의\n  지역별 특성을 충분히 반영하지 못하고 있으며, 일반 여성농업인의 표본 비율이 전체 모집단의 0.16% 수준에 불과하여 대표성 확보에 어려움이 있다. 통계의 신뢰성과 정확성을\n  높이기 위해서는 표본 크기를 2,500명 이상으로 확대하는 것이 필요하다. 지역 공표를 고려할 때 4,000개의 표본을 제곱근비례배분하는 것이 이상적이지만, 예산 및 소요\n  시간을 고려하여 3,000개의 표본을 제곱근비례배분하는 방안이 현실적인 대안으로 보인다.  따라서 여성농업인실태조사의 정확성 및 대표성 확대를 위해 전체 표본 수 확대와 이를\n  위한 예산 증액이 필요하다. 표본 수를 3,000명으로 확대할 경우, 유사 통계 면접 조사 비용(건당 약 8~10만원)을 고려하면 최소 약 2억 원의 예산이 필요할 것으로\n  판단된다. 이러한 개선을 통해 여성농업인실태조사가 보다 정확하고 필요한 정보를 제공하는 통계로 거듭나고, 궁극적으로 여성농업인 관련 정책 수립 및 지원에 기여할 수 있을\n  것이다.\n==========================\n\n===== 그룹 ID: 7 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000474/청크:12] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     현재 여성농업인실태조사는 지역별 통계를 공표하지 않고있는데, 이러한 층화기준을 사용함으로써 지역별 공표 및 공표수준별 추정이가능할 것이다. 제 ...\n  2. [노트:S0000474/청크:13] (『여성농업인실태조사』 품질개선 컨설팅 최종결과보고서)\n     제 6 절 가중치 및 모수 추정 1. 가중치 조정 표본설계는 표본조사의 결과가 모집단을 잘 대표할 수 있도록 설계하는 데에 궁극적인 목적이 있으...\n\n--- LLM 생성 통합 글 ---\n  여성농업인실태조사는 여성농업인에 대한 객관적이고 신뢰성 있는 데이터를 구축하여 정부, 산업계, 학계 등에서 여성농업인 육성 5개년 계획의 중간평가 및 관련 정책 수립을 위한\n  기초자료로 활용하는 데 목적이 있다. 2020년 정기통계품질진단 결과와 기존 표본설계를 검토한 결과, 표본규모 확대 및 표본설계 개선, 유사통계 검토, 조사표 설계 및 조사항목\n  개선 등이 필요한 것으로 파악되었다. 이에 본 연구에서는 새로운 표본설계 방안(표본배분, 가중치 작성 및 모수 추정, 패널관리 방안 등)과 조사표 개선 등 통계 품질 개선안을\n  마련하여 여성농업인실태조사의 품질을 개선하고자 한다.  먼저, 조사표 개선을 위해 일반여성농업인의 정의를 명확히 하고, 귀농여성은 일반여성에 통합하여 조사하며, 다문화여성은\n  조사대상에서 제외하는 통합된 조사표를 제시한다. 항목별 중복 및 중요도를 검토하여 복지현황 항목은 간소화하고, 양성평등 실태를 파악할 수 있는 항목들을 추가한다. 여성농업인\n  육성 기본계획의 정책 방향에 부합하는 항목, 즉 농가 여성의 양성평등 실태 확인을 위한 항목들을 조사표에 포함하는 것을 고려할 필요가 있다. 조사내용 및 항목의 활용도 및\n  중요도에 따라 조사항목을 검토하여 선택과 집중을 하고, 학계 및 정책 수립 분야 등 이용자를 대상으로 조사문항에 대한 요구를 조사하여 현실과 괴리가 있거나 응답 빈도가 낮은\n  조사항목은 삭제하고 정책결정에 필요한 문항들을 추가하는 작업이 필요하다.  둘째, 농어업인 등에 대한 복지실태조사, 다문화가족실태조사, 양성평등실태조사 등 유사통계의 조사목적\n  및 대상, 표본규모 및 예산 등 작성현황을 비교분석한 결과, 주요 조사대상 외에 조사대상의 비교군으로 대상의 범위를 확장한 사례들이 확인되었다. 이에 따라 여성농업인실태조사에\n  남성을 비교군으로 추가 조사할 근거를 확인하였으며, 비교군의 표본크기는 핵심조사대상의 약 절반 수준(전체 표본수의 약 30% 수준)으로 파악되었다. 남성 표본크기를 결정하기에\n  앞서, 현행 조사대상을 기준으로 전체 표본 규모를 검토한 후 표본설계 방법과 목표허용오차 및 현실적 제약 등을 종합적으로 고려하여 표본크기를 결정하기로 한다. 여성과 남성의\n  농업 주종사자 분포는 비슷하므로 지역별 성별 인구비례로 표본배분을 한다면 남성이 과대추정될 수 있다. 따라서 허용오차를 확보하기 위한 여성농업인의 표본규모를 미리 확보하고,\n  추가적으로 남성 표본을 배분하는 것이 적절하다. 현재 표본규모인 2,000으로 여성농업인 표본을 유지하면서 남성에 대한 표본을 추가하는 방안을 검토할 수 있다.  셋째,\n  통계청에서 작성 및 관리하고 있는 농림어업총조사를 표본추출틀로 사용하여 확률표본추출하는 표본설계 방안을 제시한다. 조사모집단 즉 여성농업인을 충분히 대표할 수 있는 적정\n  표본규모를 제시하는 한편, 지역(9개 도 및 특광역시) 및 동/읍면부를 층화기준으로 사용하여 모집단의 특성을 잘 반영한 효과적인 표본배분방법을 검토하고, 정확한 통계 산출을\n  위하여 표본설계에 따른 모수추정식, 가중치 산출 등을 포함한 새로운 표본설계 방안을 제시한다. 현재 여성농업인실태조사는 지역별 통계를 공표하지 않고 있는데, 이러한 층화기준을\n  사용함으로써 지역별 공표 및 공표 수준별 추정이 가능할 것이다.  표본조사를 계획할 때 목표허용오차를 설정하고 이를 기준으로 표본크기를 결정해야 한다. 여성농업인의 모집단\n  분포가 지역 및 동/읍면 수준에 따라 차이가 크게 나타나므로 이를 고려하여 표본크기를 결정해야 한다. 서로 이질적인 층을 고려시 전체 표본크기는 표본배분방법에 따라 다르다.\n  목표허용오차를 2% 이내로 하기 위해 필요한 최소의 표본규모는 4천 이상이다. 조사대상의 층간 이질적 특성을 반영하기 위한 대표적인 표본배분 방법으로 비례배분, 제곱근비례배분,\n  네이만배분, 최적배분을 고려할 수 있다. 본 조사에서는 지역별 조사비용과 분산에 대한 기존 근거자료가 미비하므로 조사비용과 분산을 고려하지 않고 추정량의 분산을 최소화할 수\n  있는 층의 크기만을 반영한 비례배분 및 제곱근비례배분 방법을 고려한다. 표본크기가 2,000명, 2,500명, 3,000명, 4,000명인 경우에 20개의 층으로 표본배분한\n  결과, 제곱근비례배분이 오차한계 측면에서 가장 안정적인 표본배분 결과를 보인다. 특히 표본크기가 4,000명인 경우 모든 층에서 10% 이내의 오차한계를 보인다. 10개 층으로\n  전체 표본 2,000명 또는 2,500명을 표본배분하는 경우, 각 층별 오차한계는 제주 외 모든 지역에서 10% 이내였고, 특히 3,000명을 제곱근비례배분시 제주 외 모든\n  지역의 오차한계는 5% 내외로 안정적이다.  여성농업인실태조사의 핵심 조사대상은 여성농업인이나 이에 대한 표본추출틀이 따로 없는 바, 농림어업총조사의 일반농가를 활용한 2단계\n  표본추출방법을 적용한다. 층화추출법은 층의 구조에 따라 표본추출법을 유연하게 적용할 수 있는데, 본 연구에서 표본추출은 거주지역을 기준으로 층화를 한 후, 각 층에 표본을\n  배분하고 배분된 표본의 크기대로 각 층별 모집단 정보를 활용하여 계통추출법으로 표본을 추출한다. 실제 조사를 진행하며 조사과정에서 표본으로 추출된 대상의 조사가 불가능하거나\n  무응답이 발생할 경우 동일한 속성(동일한 층)의 대상자로 대체하는 것을 원칙으로 한다. 이를 위해 표본추출과정에서 사전에 원표본과 대체표본을 추출한다.  표본의 누락 및 무응답\n  등으로 인하여 조사된 표본구조가 모집단의 구조와 차이가 있는 경우, 표본조사 결과를 모집단 결과로 일반화시키기 어려울 수 있다. 표본조사의 대표성 확보를 위해서 표본조사 실시\n  전에는 정교한 표본설계를 통해, 표본조사 실시 후에는 적절한 가중치 적용을 통해 모집단 구조와 일치시킴으로써 추정의 정확도를 제고할 수 있다. 단위 무응답이 존재하는 경우,\n  이를 보정하기 위하여 최초 목표 표본크기를 유효 표본크기로 나누어 조정한다. 표본설계에서 고려된 표본의 층별 여성농업인 분포를 조사 시점의 층별 모집단 여성농업인 분포와\n  일치시키기 위하여 사후 층화 가중치를 고려할 수 있다. 복합표본조사 데이터를 분석할 때 가중치를 무시하고 분석하면 모수 추정에 심각한 편향이 발생할 수 있고, 추정량의 분산이\n  과소평가되어 문제가 된다. 따라서 여성농업인실태조사에서 모집단의 특성치에 대한 추정 시 가중치를 이용해야 한다. 만약 단순총계를 사용하면 추정치에 편향이 발생할 수 있다.\n==========================\n\n===== 그룹 ID: 8 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000565/청크:6] (2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     ) ◯ 유형별 평가결과를 보면 7개 유형 모두 사업평가 보다 회계평가 점수가 높았음. ◯ 1유형 사회통합의 종합평가 점수가 88.45점으로 가장...\n  2. [노트:S0000565/청크:9] (2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     (유형별 평균의 평균이 아님. ) ◯ 유형별 회계평가 결과를 항목 별로 살펴보면 자부담 집행 적법성(99.25점/100점 환산점수), 정산자료 ...\n\n--- LLM 생성 통합 글 ---\n  2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가 결과, 유형별 평가에서는 사회통합(1유형)이 88.45점으로 가장 높았고, 평화증진 및 국가안보(5유형)가\n  82.26점으로 가장 낮아 유형 간 점수 차는 6.19점으로 나타났다. 사업평가와 회계평가 모두 1유형이 가장 우수했으며, 사업평가에서는 5유형, 회계평가에서는 7유형이 가장\n  미흡했다. 사업평가의 최고점(1유형, 84.54점)과 최저점(2유형, 78.38점) 간 점수 차는 6.16점, 회계평가의 최고점(1유형, 94.31점)과 최저점(7유형,\n  86.95점) 간 점수 차는 7.36점으로, 회계평가 점수 차가 사업평가 점수 차보다 약간 크게 나타났다. 전체적으로 사업평가보다 회계평가 점수가 높게 나타났다.  회계평가\n  항목별로는 자부담 집행 적법성(99.25점)과 정산자료 구비(95.83점)가 우수했고, 보조금 집행률(73.26점)이 미흡했다. 이는 코로나19 팬데믹으로 인해 대면 행사 취소\n  및 사업 변경이 발생하며 잔여 보조금이 발생했기 때문으로 분석된다. 집행등록 지연 건수는 지연비율 평가지표 개선으로 전년 대비 개선되었다. 유형별 회계평가에서는 1유형이\n  정산자료 구비, 보조금 집행 적법성, 자부담 집행 적법성 등 다수 항목에서 만점을 받으며 가장 우수했고, 2유형, 6유형, 3유형 등이 뒤를 이었다. 집행평가 교육 및 부적정\n  보완 과정으로 전반적인 회계평가 점수가 상향 조정된 것으로 보인다.  다년도 사업(총 10개)의 종합평균은 83.82점으로 전체 사업 종합평균(85.06점)보다 1.24점\n  낮았다. 다년도 사업 중에서는 1유형의 단체 A가 93.37점으로 가장 우수했고, 4유형의 단체 F가 58.40점으로 가장 미흡했다.\n==========================\n\n===== 그룹 ID: 9 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000565/청크:10] (2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     · 사업평가 결과가 가장 우수한 단체는 1유형의 다년도 단체 A(92.61점)이었으며, 가장 미흡한 단체는 7유형의 다년도 단체 I(64.64점...\n  2. [노트:S0000565/청크:11] (2020년 행정안전부 비영리민간단체 공익활동 지원사업 평가보고서)\n     ◯ 컨소시엄 사업을 진행한 단체는 1개(4유형 1개)이며, 종합평가 점수가 전체사업 평균 대비 매우 낮은 58.40점을 기록하였음. 사업평가 점...\n\n--- LLM 생성 통합 글 ---\n  2020년도 행정안전부 비영리민간단체 공익활동 지원사업 평가 결과, 다년도 사업과 컨소시엄 사업에서 다양한 시사점이 도출되었다. 다년도 사업 평가 결과, 사업평가에서는 1유형\n  단체 A가 92.61점으로 가장 우수, 7유형 단체 I가 64.64점으로 가장 미흡했다. 회계평가에서는 4유형 단체 E가 99점으로 최고, 4유형 단체 F가 52점으로 최저점을\n  기록했다. 종합평가 기준 우수 등급은 3개 단체(1유형, 3유형, 4유형 각 1개), 미흡 등급은 1개 단체(4유형), 나머지는 보통 등급이었다. 사업평가에서는 우수 등급 2개\n  단체(1유형, 7유형), 회계평가에서는 우수 등급 6개 단체(1유형 2개, 2유형, 3유형, 4유형, 7유형 각 1개), 미흡 등급 1개 단체(4유형)가 있었다.  컨소시엄\n  사업은 1개 단체(4유형)만 진행했으며, 종합평가 점수가 58.40점으로 전체 사업 평균 대비 매우 낮았다. 사업평가 점수는 62.67점, 회계평가 점수는 52.00점으로 각각\n  전체 사업 평균보다 현저히 낮았다.  회계 전문가 집행평가 검증 결과, 증빙서류 누락 및 동일 건 중복 지급 오류가 발견되었으나, 소명 및 보완, 재평가를 통해 적법하게 수정\n  처리되었다.  사업평가 시사점으로는 '단체역량' 항목에서 소규모 단체의 사업개발 노력이 부족하고, 코로나19 상황에 대한 사업 구조 및 방식 변화 노력이 필요하다는 점이\n  지적되었다. 다년도 사업의 경우 매년 진화된 사업 계획 수립이 요구된다. 조직 구성 측면에서는 회계 부문 역할 수행을 위한 인력 배정 또는 이해도가 높은 담당자 배정이\n  권고되었다. 행정관리체계와 관련해서는 NPAS 외 자체 행정관리 시스템 구축의 필요성이 강조되었으며, 사업 담당자의 잦은 교체 및 인수인계 미흡 문제 해결이 요구된다. 특히\n  코로나19로 인해 내부 행정관리 역량이 더욱 중요해졌다.  '운영과정' 항목에서는 유관기관과의 협력이 긍정적이지만, 코로나19로 인해 협력 노력이 감소하고 새로운 파트너십\n  발굴이 미흡했다는 점이 지적되었다. 기관 간 역할 명확화 및 파트너십 체계화가 필요하며, 중앙-지부 협력 사업의 경우 공식 커뮤니케이션 채널 구축 등이 요구된다. 사업 홍보\n  측면에서는 온라인 채널 활용이 증가했지만, 수혜자 모집에 치중되어 지속적인 콘텐츠 발신이 부족했다. 대상자 확보에 있어서는 개방적인 모집 방식과 수혜자 선정 체계화가 필요하다.\n  사업 관련 회의는 대체로 양호하나, 회의록의 구체성 보완 및 집행 증빙 외 체계적인 사업 관리를 위한 활용이 요구된다.  '사업성과' 항목에서는 코로나19로 인해 사업목적\n  달성도가 감소했으며, 체계적인 성과관리 부문 개선이 요구된다. 자체평가는 사업 담당자 외 단체 차원의 종합적인 평가가 필요하며, 사업의 사회적 기여도와 수혜자 만족도를 높이기\n  위한 노력이 필요하다. 수혜자 만족도 조사는 형식적인 수단이 아닌 사업 개선을 위한 중요한 도구로 활용되어야 한다.  회계평가 시사점으로는 대부분의 단체가 정산자료 구비 상태가\n  우수하고 보조금/자부담 사업비 집행비율이 원활했으나, 일부 단체의 경우 집행 시점에 증빙자료를 구비하지 못하는 경우가 있었다. 사업계획 변경은 적절한 수준이었으나, 코로나19로\n  인한 신청 건수가 증가했다. 보조금/자부담 사용은 적법하게 이루어졌으며, 전자증빙 사용비율도 높았다. 다만, 사업비 집행등록 지연 비율이 높게 나타나 개선이 요구된다.\n  사업추진실적 대비 업무추진비 사용비율은 적절하게 관리되고 있다.\n==========================\n\n===== 그룹 ID: 10 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (3개) ---\n  1. [노트:S0000865/청크:0] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     보고서 요약문 본 사업은 전국 지역사회를 기반으로 하는 협력 병의원과 질병관리본부를 연계하여, 호흡기 감염증 병원체의 실험실 감시망을 구축하고 ...\n  2. [노트:S0000865/청크:6] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     강릉아산병원  7. 1차 의료기관 15곳: 전국 분포 의원급 등 2차 병원 5개소와 1차기관 15개소 대상 감시망 구성. (2)감시망 운영 1)...\n  3. [노트:S0000865/청크:17] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     pneumoniae 균주는 각각 1이 검출되었다. 호흡기감염증 감시 체계를 통하여 각 병원체의 지역별, 연령별, 월별 양성률을 파악하고원인 병원...\n\n--- LLM 생성 통합 글 ---\n  본 사업은 전국 지역사회 기반의 협력 병의원과 질병관리본부를 연계하여 호흡기 감염증 병원체의 실험실 감시망을 구축하고 운영하는 것을 목표로 한다. 이를 통해 국내 주요 호흡기\n  감염증(폐렴, 기관지염, 인후염 등)의 원인 병원체 발생 동향을 감시하고, 세균성 병원체의 항생제 내성 현황 자료를 산출하여 국내 호흡기감염증 관리 정책의 과학적 근거 자료를\n  제공하고자 한다. 특히, 각 병원균의 유행 양상을 파악하고, 호흡기 감염증의 원인 병원균 발생 추이를 분석함으로써 적절한 치료 지침 마련에 기여하고자 한다.  감시망은 1,\n  2차 의료기관 15곳과 3차 병원 6개소를 포함, 총 21개 병의원으로 구성된다. 협력 병원에서는 호흡기 질환으로 내원하는 환자 중 감시대상 호흡기 질환 사례 정의에 부합하는\n  환자를 선별하고, 질환별 적합 검체를 채취하여 24시간 이내 검사실로 이송한다. 이송된 검체는 각 질환별 검사 대상 병원체 모두에 대한 실험실 검사를 실시하고, 수집된 임상\n  기록 정보 및 실험실 검사 결과를 질병관리청의 결과 보고 시스템인 ARI Net에 등록한다. 실험실 검사 결과 관리자는 질병보건통합관리시스템(is.cdc.go.kr)에 사용자\n  등록 후 해당 업무 권한을 부여받아야 하며, ARI Net에는 누적된 검사 결과 파일과 함께 해당 기간 수행된 실험 검사 결과(전기영동 사진 등)도 등록한다. 수집된 검체는\n  분석 균종 모두에 대한 전수 검사를 지정된 검사법에 따라 수행하며, 세부 방법은 발주 부서와 협의하여 결정한다. 객담은 점성을 제거한 후 DNA 추출 및 배양 검사에 사용된다.\n  정형균은 API kit, Crystal kit 등을 이용하여 동정을 확인하고, 비정형균은 객담에서 DNA 추출 후 Seegene multiplex PCR kit를 사용하여\n  검사한다.  2016년 5월부터 2017년 3월까지 전국 6개 병원(서울아산병원, 서울보라매병원, 평촌성심병원, 인천의료원, 춘천성심병원, 원주기독병원)에서 지역사회폐렴 환자\n  102건을 감시했다. 이 중 75건에서 원인균이 분리되었으며, K. pneumoniae, P. aeruginosa, S. aureus 순으로 높은 분리 빈도를 보였다.\n  비정형폐렴균은 4.0% 양성률을 나타냈다.  그러나 연구 진행 과정에서 개인정보보호법과 생명윤리 및 안전에 관한 법률에 따른 문서화 동의서 획득 과정이 환자 등록에 제한점으로\n  작용했다.  총 환자 수 700여 명 중 동의서에 서명한 대상자 수는 400여 명에 그쳤다. 따라서 국가 정책에 필요한 감시 사업은 동의 절차 간소화, 예를 들어 구두 동의로\n  완화하는 방안을 고려할 필요가 있다. 또한, 국가 차원의 감시 사업에 대한 공익적 측면 홍보와 참여 독려가 필요하다.  국가 차원의 호흡기 감염병 및 병원체 감시를 성공적으로\n  수행하기 위해서는 대표성 있는 감시 체계 구축, 표준화된 운영 체계 마련, 그리고 지속적인 질 관리가 필수적이다. 감시 체계를 전국적인 네트워크로 확장하고, 1, 2, 3차\n  의료기관을 골고루 포함시켜야 한다. 표준화된 운영 체계에는 대상 환자 감시 및 선별 방법, 일관된 보고 체계, 검체 확보 및 전달, 검사 방법 등이 포함된다. 더불어, 임상\n  정보 기록서의 정보 불확실성 문제를 해결하기 위해 진단검사의학과 연구진을 포함시켜 검사 방법을 표준화해야 한다. 질 관리를 위해서는 정기적인 회의를 통해 감시 현황을 점검하고\n  현장에서 접하는 문제를 공유해야 한다.  해외 과학 기술 정보에 따르면, 20세기 초와 말 사이에 호흡기 감염의 역학적 차이가 발견되었으며, 특히 원외 폐렴의 역학에 많은\n  변화가 있었다. 원외 폐렴에서 비정형균 폐렴이 차지하는 비율은 20-40% 정도이며, Mycoplasma pneumoniae, Chlamydia pneumonia,\n  Legionella pneumophila 등이 대표적이다. 우리나라에서는 비정형 폐렴에 대한 보고가 부족한 실정이다. 따라서 비정형균에 대한 신빙성 있는 자료 확보가 필요하며,\n  최근 증가하는 바이러스에 의한 호흡기 감염에 대한 주시도 필요하다.\n===========================\n\n===== 그룹 ID: 11 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (2개) ---\n  1. [노트:S0000865/청크:5] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     서울시립 보라매병원,  3. 인천의료원  4. 평촌성심병원  5 춘천 성심병원  6....\n  2. [노트:S0000865/청크:12] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     서울시립 보라매병원, 3. 인천의료원 4. 평촌성심병원 5....\n\n--- LLM 생성 통합 글 ---\n  서울시립 보라매병원, 인천의료원, 평촌성심병원, 춘천성심병원이 언급되었다.\n===========================\n\n===== 그룹 ID: 12 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (3개) ---\n  1. [노트:S0000865/청크:11] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     점검회의 (1)일시 : 2017년 1월 10일 14:00 ~ 16:00 (2)장소 : 반포동스마트웍센터(3)참석자 (4)내용 - 대표성 확보 :...\n  2. [노트:S0000865/청크:13] (호흡기감염증 병원체 분포실태 조사를 위한 감시망 구성 및 운영)\n     춘천 성심병원 6. 원주기독병원 강릉아산병원  (2)지역사회폐렴 감시망 참여병원 관리 - 해당 참여병원에 대하여 지속적인 교육을 실시하고 있으며...\n  3. [노트:S0000957/청크:9] (의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     4. 참여기관 현장 방문 조사 및 건의 사항 청취 1)현장 방문을 위한 조사지 구성 2)현장 방문 필요 병원 연구진 회의를 통해 선정  - 6개...\n\n--- LLM 생성 통합 글 ---\n  2017년 1월 10일, 반포동스마트웍센터에서 호흡기감염증 감시사업 점검회의가 개최되었다. 회의에서는 지역, 연령, 계절별 대표성 확보를 통한 검체 건수 증가 방안, 현실적인\n  동의서 확보 방안, 병원별 검체 수집 증가 방안, 그리고 병원 참여를 위한 인센티브 제공 필요성 등이 논의되었다.  호흡기감염증 감시사업은 지역사회폐렴 감시망과 급성상기도감염증\n  감시망으로 구성되어 운영되었다. 지역사회폐렴 감시망은 전국 20개 병원을 중심으로 운영되었으며, 서울아산병원이 용역수행기관 역할을 수행했다. 참여 병원에는 사업 목적, 검체\n  채취 방법, 바코드 부착, 임상정보기록서 작성 등의 교육이 지속적으로 제공되었고, 담당자 교체 시 재교육 및 유선, 방문 독려가 이루어졌다. 급성상기도감염증 감시망 또한 참여\n  병(의)원을 대상으로 운영되었다.  감시사업을 위해 검체 채취 kit, 바코드, 임상정보기록서가 제공되었다. 각 감시망별로 임상정보기록서가 제작되었으며, 지역사회폐렴 감시망의\n  임상정보기록서는 (재)씨젠의료재단의 IRB 승인을, 급성 상기도감염증 감시망은 서울아산병원과 2차 의료기관 모두에서 IRB 승인을 받았다. 기록서에는 개인정보보호법에 준하여\n  대상자의 동의하에 개인정보, 임상증상, 생활습관, 동반질환 여부, 흉부 X-선 소견, 백신 접종 여부, 항생제 처방 여부 등의 정보를 기재하도록 하였다.  검사 결과 및\n  임상정보기록서 조사 내용은 질병관리본부 질병보건통합관리시스템에 보고되었다. 보고 내용은 지역별, 연령별, 성별, 균종별 병원체 분리 결과, 항생제 감수성검사 결과,\n  임상정보기록서 수집 데이터, 검체 수집 현황 및 진행률, 물품 전달 내역, 검체 및 균주 전달 내역 등을 포함한다.  2016년 5월부터 2017년 3월까지 지역사회폐렴 감시\n  결과, 서울아산병원, 서울보라매병원, 평촌성심병원, 인천의료원, 춘천성심병원, 원주기독병원 등 6개 병원에서 총 102건의 지역사회폐렴이 확인되었다. 연령별로는 81세 이상이\n  가장 많아 고령 환자가 지역사회폐렴의 중요한 고위험군임을 시사했다.  한편, 의료관련 요로감염은 흔한 의료관련감염으로, 효과적인 예방관리를 통해 30-50% 예방이 가능하다.\n  국내에서는 2005년 감염예방 관리지침이 개발된 이래 지속적으로 개정되었으나, 의료현장 실천 증진 및 지침 효과 연구는 일부 감염에만 집중되었다. 따라서 중소병원을 포함한\n  다양한 의료기관을 대상으로 표준예방지침의 현장 실천 증진 사업을 시행하고 효과 평가를 통해 향후 지침 개정 시 반영하고, 의료기관 현장 실행을 위한 효과적인 전략 개발이\n  필요하다.  이를 위해 참여기관 현장 방문 조사가 이루어졌다. 방문 목적은 요로감염 진단 여부, 삽입 및 유지관리 체크리스트 기록 관련 내용 확인, 웹 입력 상황 점검 및\n  제안사항 검토, 요로감염 중재활동 내용의 동영상 교육자료 활용 현황 조사, 감시대상 중환자실과 병동 방문을 통한 삽입기준 점검표 기록 상황 및 요로감염 중재활동 수행 사항\n  점검, 그리고 상담 및 자문 제공 등이었다.  연구 수행 과정에서 계약 지연으로 인해 계획보다 3개월 지연되었으나, 참여기관들의 증례 등록 시작 및 전산프로그램 개발 완료로\n  인해 이후 진행에는 문제가 없을 것으로 예상되었다. 2차년도부터는 1월부터 사업 진행이 가능하므로 문제없이 사업이 진행될 것으로 전망되었다.  차기년도 연구계획은 1차년도 참여\n  병원의 지속 참여를 통해 중재 효과를 극대화하고, 추가 중재 프로그램 개발 및 적용을 위한 사전 준비를 시행하며, 추가 중재 프로그램 교육 및 시행, 참여기관 방문 조사를 통한\n  수행 여부 확인 등을 포함한다.\n===========================\n\n===== 그룹 ID: 13 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (7개) ---\n  1. [노트:S0000954/청크:0] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     편집순서 6 : 총괄용역과제의 연구결과 학술연구개발용역과제 연구결과 1.1 목표 가. 연구 주제: 줄기세포 연구 활용 촉진을 위한 사람 전분화능...\n  2. [노트:S0000954/청크:1] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     나. 인간 전분화능줄기세포에서 심근세포로 분화하는 방법은 배아의 심장 발생 과정과 환경을 모티브로 하는 기법들이 개발되었으며 대표적인 3가지 방...\n  3. [노트:S0000954/청크:2] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     공배양 기법은 발생학적 관점에서 접근하여, 심장 발생 과정에 중요한 내배엽성세포를 이용한 심근세포의 분화 유도 방법으로, 인간전분화능줄기세포와 ...\n  4. [노트:S0000954/청크:7] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     발주부서 연구원에게 CMC3 심근세포 분화 SOP 기술이전  가. CMC3 세포주를 이용한 심근세포 분화 기술 개발관련 SOP 확립: SOP 제...\n  5. [노트:S0000954/청크:8] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     2017.10.26. CMC3 세포주를 이용한 인간전분화능줄기세포 유래 심근세포 분화기술 SOP 문서를 제공함. 다....\n  6. [노트:S0000954/청크:11] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     마. 교육 및 기술이전 내용: SOP와 동일한 방법으로 CMC3 세포주를 이용하여 미분화배양, 계대배양, 심근세포 분화, 분화된 심근세포(응수축...\n  7. [노트:S0000954/청크:14] (사람 전분화능 줄기세포의 심장세포 분화 표준으로 프로토콜 개발 및 평가기준 확립)\n     세포치료제 개발을 위한 심근세포 프로토콜 개발 전략 가. 안전성 확보를 위한 프로토콜 개발: Xeno- free 배양을 위한 matrigel 대...\n\n--- LLM 생성 통합 글 ---\n  줄기세포 연구 활용 촉진을 위해 사람 전분화능줄기세포의 심근세포 분화 표준 프로토콜 개발 및 평가기준 수립 연구가 진행되었다. 줄기세포는 세포치료제 개발뿐 아니라 신약 개발과\n  독성 평가에서도 동물실험의 한계를 극복할 수 있는 자원으로 활용 가능하다. 특히 인간 전분화능줄기세포 유래 심근세포는 신약 개발 단계에서 심독성 영향 평가의 패러다임 변화를\n  주도하고 있다. 미국 FDA는 CiPA (Comprehensive in vitro Proarrhythmia Assay) 프로토콜을 정립 중이며, 인간 전분화능줄기세포 유래\n  심근세포를 활용한 부정맥 평가를 CiPA 3단계에 포함하고 있다. 따라서 비임상 시험 단계에서 줄기세포 유래 심근세포의 활용이 기대된다.  하지만 현재까지 인간\n  전분화능줄기세포의 분화 기술은 낮은 재현성과 세포의 불균일성 문제를 안고 있다. 이를 극복하고자 심근세포 분화 표준화 프로토콜 개발과 더불어 분화된 심근세포를 활용한 독성시험\n  분석 기법 표준화가 필요하다. 확립된 기술력은 국가줄기세포은행의 표준 심근세포 분화 프로토콜 SOP 작성에 활용되어 국내 줄기세포 연구 촉진에 기여할 수 있다.  인간\n  전분화능줄기세포에서 심근세포로 분화하는 방법은 배아의 심장 발생 과정과 환경을 모티브로 하는 기법들이 개발되었다. 대표적인 방법으로는 3D 배양 기법, 2D-direct 분화\n  기법, 그리고 공배양 기법이 있다. 3D 배양 기법은 배상체(Embryoid body, EB)를 형성하여 분화를 유도하지만, 배상체 크기의 불균일성으로 인한 분화 효율 차이와\n  재현성 문제가 있다. 2D-direct 배양 기법은 부착된 세포를 기반으로 분화를 유도하며 성장인자를 조절하기 용이하지만, 동물 유래 물질 사용 제한 문제가 있다. 이를\n  해결하기 위해 serum-free 배양 조건이 개발되었으며, 저분자화합물을 이용한 분화 기법도 연구되고 있다. 공배양 기법은 내배엽성 세포와의 공배양을 통해 심근세포 분화를\n  유도하지만, 공배양에 사용되는 내배엽성 세포를 제거해야 하는 단점이 있다. 각각의 인간 전분화능줄기세포주는 유전적 특성에 따라 분화 효율이 다르므로, 이를 반영한 보편적으로\n  활용 가능한 심근세포 분화 기법 개발이 요구된다.  국외에서는 미국을 선두로 인간 배아줄기세포 유래 심근세포 생산 및 특성 분석 연구가 활발하게 진행되고 있다. 국내에서도\n  2006년 이후 관련 연구가 진행되고 있지만, 세계적인 수준과는 격차가 있다. 따라서 인간 전분화능줄기세포 유래 심근세포 생산 기술력을 확보하여 국내 연구자들에게 보급하는 것이\n  중요하다.  1차 연도 연구개발 목표는 줄기세포 연구 활용 촉진을 위한 사람 전분화능 줄기세포의 심근세포 분화 표준 프로토콜 개발 및 평가기준 수립이다. 주요 연구개발 내용은\n  심근세포 분화 및 정제기법 개발, 심근세포 생산 프로토콜 확립 및 발주부서 기술 이전이다. 구체적으로 세포주별 배상체 형성을 통한 삼배엽성 분화 특성 분석, 3D 분화 기법을\n  이용한 중배엽성 및 심근세포 분화 효율 탐색, Feeder-free 조건을 이용한 미분화 세포 유지 배양 조건 구축, 2D 기반 BMP4/Activin A 및 저분화 화합물을\n  이용한 심근세포 분화 효율 탐색, 세포주별 분화 효율 탐색을 통한 분화 기법 최적화, 2D/3D 기반 분화 세포의 대사 조절 배양 조건을 이용한 심근세포 정제 효율 탐색,\n  제공된 세포주를 이용한 2D/3D 기반 새로운 프로토콜 효율 탐색, Low glucose/serum-free 배양 조건에서의 심근세포 정제 효율 분석, 세포주별 정제 효율\n  분석을 통한 정제 기법 최적화를 진행하였다.  용역 과제 발주부서에서 제공받은 3종의 세포주(CMC3, CMC9, CMC11)에 대하여 IRB 승인을 받았으며, CMC3\n  세포주를 이용한 심근세포 분화 기술 개발관련 SOP를 확립하고, 2017년 10월 26일 CMC3 세포주를 이용한 인간전분화능줄기세포 유래 심근세포 분화기술 SOP 문서를\n  제공하여 기술이전을 완료하였다. 교육 및 기술이전 내용으로는 CMC3 세포주를 이용하여 미분화배양, 계대배양, 심근세포 분화, 분화된 심근세포(응수축 심근세포)확인, 심근세포\n  정제 관련하여 심근세포 분화에 대한 모든 기술을 전반적으로 교육하고, 응수축 심근세포를 확인 후, 심근세포 정제와 특성 분석까지 진행하였다. SOP 내용 이외에 심근세포의 특성\n  분석 관련된 정보를 제공하고, CMC3 세포주를 passage 별로 샘플을 제공하였다.  안전성 확보를 위해 Xeno-free 배양을 위한 matrigel 대체\n  vitronectin, laminin 등의 세포외기질 활용이 검토되어야 하며, 미분화 세포에 의한 테라토마 혹은 비정제 심근세포에 의한 암 발생을 억제할 수 있는 분화와 정제\n  기법 개발이 요구된다. 부정맥 발생에 대한 문제점 극복을 위해서는 ventricle 타입의 심근세포만을 분화할 수 있는 기법과 고순도 정제를 위한 표면표지인자의 개발이\n  필요하다. 또한, 미성숙 심근세포의 기능은 체내 심근세포의 약물 반응과 응수축(contracting)기능을 충족할 수 없으므로, 분호된 심근세포의 성숙화 기법의 개발이\n  중요하다.\n===========================\n\n===== 그룹 ID: 14 (LLM: models/gemini-2.0-flash) =====\n--- 원본 청크 (3개) ---\n  1. [노트:S0000957/청크:1] (의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     예방프로그램의 주요 목표를 표 1에 정리하였다. 매일 유치도뇨관 장착 여부와 필요성을 확인하고, 유치도뇨관보다는 다른 대체방법을 강구하고, 유치...\n  2. [노트:S0000957/청크:2] (의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     중환자실에서 유치도뇨관 삽입률을 줄이기 어렵다는 것이 주요원인이었다. 이와 관련하여 프로그램을 보완하는 것이 필요해 보인다. ◦ 국내에서는 유치...\n  3. [노트:S0000957/청크:3] (의료관련 요로감염 예방지침 연장실행 증진 및 효과평가)\n     유치도뇨관 삽입의 대체 방법 강구 3. 소변량 측정을 위한 방광 스캐너 사용 4. 폐쇄된 요관 시스템 사용 5. 정기적인 유치도뇨관 교체 6. ...\n\n--- LLM 생성 통합 글 ---\n  국내 의료기관의 요로감염 예방을 위한 노력은 주로 대형병원 감염관리실 주관으로 이루어지고 있으며, 중소병원을 대상으로 한 체계적인 감시 및 가이드라인은 미흡한 실정이다.\n  KONIS를 통한 요로감염률 감시는 대형병원 중심으로 운영되고 있으며, 중소병원의 현실을 반영한 맞춤형 감시체계 및 가이드라인 개발이 필요한 상황이다. 이에\n  대한의료관련감염관리학회는 질병관리본부의 지원을 받아 중소병원 감염관리 자문시스템(ICCON)을 운영하고 있으며, 소규모 프로젝트를 통해 중소병원 요로감염 감시체계의 가능성을\n  확인한 바 있다.  ICCON 프로젝트 결과, 중소병원은 대형병원에 비해 유치도뇨관 삽입 비율은 낮지만 요로감염 발생률은 비슷하거나 높아, 중소병원 맞춤형 요로감염 관리\n  가이드라인의 필요성이 제기되었다. 프로젝트 참여자들의 건의사항을 분석한 결과, 일회성 교육보다는 미국의 \"CUSP: Stop CAUTI\" 프로그램처럼 반복적인 교육과 충분한\n  교육 시간 확보가 중요하며, 정기적인 피드백과 온라인 학습 자료 제공, 현장 방문을 통한 지원이 필요하다는 의견이 제시되었다. 또한, 중소병원의 미생물 검사 역량 부족과 인력\n  부족 문제 해결을 위해 외부 미생물검사실 연계 지원 및 간단한 toolkit 개발이 요구된다. 병원 전체의 적극적인 참여를 유도하는 리더십의 중요성과 함께, 국내 현실에 맞는\n  정책적 지원 방안 모색도 필요하다.  이러한 문제점을 해결하고자 의료관련 요로감염 예방지침 중재연구위원회를 구성하여 의료기관 네트워크 구축, 감시 및 중재 프로그램 수행,\n  감염감시 및 모니터링 지표 기반 효과 평가를 목표로 연구를 수행하였다. 연구는 1) 의료관련 요로감염 예방지침 중재프로그램과 실행 도구 개발, 2) 병원 특성에 따른 요로감염\n  발생 실태 파악, 3) 병원 특성에 따른 감염관리 중재 방법별 효과 평가 후 현실 적용 가능한 모델 개발, 4) 의료기관 네트워크 구축, 5) 현장 실행에 따른 장애와 극복\n  요인 분석을 주요 내용으로 한다. 중재 방법으로는 유치도뇨관 삽입의 적응증 재검토, 대체 방법 강구, 방광 스캐너 사용, 폐쇄형 요관 시스템 사용, 손위생 수행률 조사,\n  CAUTI 감시체계 운용 및 피드백 등이 고려되었다.  의료기관 네트워크는 중소병원 감염관리 네트워크 홈페이지의 감염감시체계 프로그램을 활용하여 구축하고, 온라인 커뮤니티와\n  게시판을 통해 정보 공유 및 소통을 활성화할 계획이다. 네트워크 운용팀은 온라인 게시판 관리, 감시시스템 문제 해결, Q&A 게시판 운영, 자료실 운용, 오프라인 회의 준비 등\n  커뮤니케이션 역할을 담당하고, 위원회는 교육팀, 중재팀, 평가팀으로 역할을 분담하여 운영된다.  실제 32개 의료기관(대학병원급 21개, 종합병원/중소병원 11개)이 참여한\n  결과, 교육프로그램과 감시, 피드백을 혼용하는 방식으로 진행되었다. 교육은 프로그램 도입 전, 중간, 마지막에 총 3회 시행되었으며, 매달 프로그램 적용 상황 점검 및 데이터\n  수집, 애로사항 해결 방안 모색이 이루어졌다. 분석 결과, 일반병실에서의 유치도뇨관 삽입률과 감염률은 유의하게 감소했으나, 중환자실에서는 변화가 없었다.\n===========================\n\n총 15개의 그룹에 대한 통합 글 출력을 완료했습니다.\n\n--- 통합 글 확인 완료 ---\n","output_type":"stream"}],"execution_count":7}]}